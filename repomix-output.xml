This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/prompts/plan-qnaAuth.prompt.md
.gitignore
auth/__init__.py
auth/authentication.py
auth/challenge_response.py
auth/enrollment.py
auto_collect.py
collect_training_data.py
config.example.py
dataset/__init__.py
dataset/builder.py
frontend/.eslintrc.cjs
frontend/index.html
frontend/package.json
frontend/postcss.config.js
frontend/src/App.tsx
frontend/src/components/ui/background-beams.tsx
frontend/src/index.css
frontend/src/main.tsx
frontend/src/pages/AuthenticatePage.tsx
frontend/src/pages/DevicesPage.tsx
frontend/src/pages/EnrollPage.tsx
frontend/src/pages/HomePage.tsx
frontend/src/services/api.ts
frontend/src/services/collectors.ts
frontend/src/vite-env.d.ts
frontend/tailwind.config.js
frontend/tsconfig.json
frontend/tsconfig.node.json
frontend/vite.config.ts
IMPROVEMENTS.md
model/__init__.py
model/evaluate.py
model/siamese_model.py
model/train.py
noise_collection/__init__.py
noise_collection/camera_noise.py
noise_collection/mic_noise.py
noise_collection/qrng_api.py
noise_collection/sensor_noise.py
preprocessing/__init__.py
preprocessing/features.py
preprocessing/utils.py
QUICKSTART.md
README.md
requirements.txt
RESEARCH_FOCUS.md
run_full_training.py
server/app.py
server/routes.py
setup.bat
setup.sh
start.bat
test_collection.py
test_cross_device.py
test_enrollment.py
test_hardware.py
verify_cuda.py
VISION.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/prompts/plan-qnaAuth.prompt.md">
# Plan: Quantum Noise Assisted Authentication System

Build a complete authentication system using quantum noise and ML embeddings to create unique device signatures. The system will collect noise from multiple sources (quantum RNGs, camera, microphone), process them into features, train a Siamese network to create device embeddings, and implement secure enrollment and authentication protocols.

## Steps

1. **Create modular folder structure** with directories for noise_collection/, dataset/, preprocessing/, model/, auth/, server/, and frontend/ to organize the 11-step implementation.

2. **Implement noise collection modules** in noise_collection/qrng_api.py, noise_collection/camera_noise.py, noise_collection/mic_noise.py using quantum RNG APIs (ANU QRNG or Qrandom), OpenCV for camera dark frames, and sounddevice for microphone ambient noise.

3. **Build dataset pipeline** with dataset/builder.py to generate labeled samples (device_id, timestamp, noise_source, raw_sample, features) and store as CSV/JSON in dataset/samples/ directory.

4. **Develop preprocessing and feature extraction** in preprocessing/features.py implementing filtering, normalization, entropy calculation, FFT analysis, and statistical features (mean, variance, kurtosis, autocorrelation).

5. **Create Siamese/Contrastive learning model** in model/siamese_model.py using PyTorch with triplet loss or contrastive loss, train with model/train.py, and export to ONNX for deployment.

6. **Implement authentication workflows** with auth/enrollment.py for multi-sample collection and embedding storage, auth/authentication.py for real-time verification, and auth/challenge_response.py for nonce-based challenge protocol with cosine similarity threshold validation.

7. **Build FastAPI backend** in server/app.py with REST endpoints `/enroll`, `/authenticate`, `/verify`, `/store_embedding` and SQLite/PostgreSQL for embedding storage with proper security measures.

8. **Create optional frontend** in frontend/ using React with TypeScript for noise collection UI, authentication flow testing, and result visualization.

## Further Considerations

1. **Quantum RNG API selection**: ANU QRNG (free, 1024 bits/request) vs Qrandom (limited free tier) vs NIST Randomness Beacon? Recommend ANU QRNG for prototyping.

2. **Embedding security**: Should embeddings be encrypted at rest? Consider using AES-256 or homomorphic encryption for stored embeddings to prevent reverse engineering.

3. **Similarity threshold tuning**: What false acceptance rate (FAR) vs false rejection rate (FRR) is acceptable? Typical threshold: cosine similarity > 0.85 or Euclidean distance < 0.3 needs validation with test data.

4. **Device enrollment requirements**: How many noise samples per device for enrollment? Recommend 50-100 samples across different noise sources for robust signature creation.
</file>

<file path="auth/__init__.py">
"""
Auth Module Initialization
"""

from .enrollment import DeviceEnroller
from .authentication import DeviceAuthenticator, AuthenticationSession
from .challenge_response import (
    ChallengeResponseProtocol,
    SecureAuthenticationFlow,
    AntiReplayProtection
)

__all__ = [
    'DeviceEnroller',
    'DeviceAuthenticator',
    'AuthenticationSession',
    'ChallengeResponseProtocol',
    'SecureAuthenticationFlow',
    'AntiReplayProtection'
]
</file>

<file path="auth/challenge_response.py">
"""
Challenge-Response Protocol
Implements secure challenge-response authentication with nonce
"""

import hashlib
import secrets
import hmac
from typing import Tuple, Dict, Optional
from datetime import datetime, timedelta
import torch
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ChallengeResponseProtocol:
    """Implements secure challenge-response authentication"""
    
    def __init__(
        self,
        nonce_length: int = 32,
        challenge_expiry_seconds: int = 60
    ):
        """
        Initialize challenge-response protocol
        
        Args:
            nonce_length: Length of nonce in bytes
            challenge_expiry_seconds: Challenge expiration time
        """
        self.nonce_length = nonce_length
        self.challenge_expiry = timedelta(seconds=challenge_expiry_seconds)
        self.active_challenges = {}  # Map challenge_id -> challenge_data
        
        logger.info(f"ChallengeResponseProtocol initialized "
                   f"(nonce_length={nonce_length}, expiry={challenge_expiry_seconds}s)")
    
    def generate_nonce(self) -> str:
        """
        Generate cryptographically secure nonce
        
        Returns:
            Hex-encoded nonce string
        """
        nonce_bytes = secrets.token_bytes(self.nonce_length)
        nonce_hex = nonce_bytes.hex()
        return nonce_hex
    
    def create_challenge(self, device_id: str) -> Dict[str, str]:
        """
        Create authentication challenge
        
        Args:
            device_id: Device identifier
            
        Returns:
            Challenge dictionary with nonce and challenge_id
        """
        # Generate nonce
        nonce = self.generate_nonce()
        
        # Generate challenge ID
        challenge_id = hashlib.sha256(
            f"{device_id}_{nonce}_{datetime.now().isoformat()}".encode()
        ).hexdigest()[:16]
        
        # Store challenge
        self.active_challenges[challenge_id] = {
            'device_id': device_id,
            'nonce': nonce,
            'created_at': datetime.now(),
            'expires_at': datetime.now() + self.challenge_expiry
        }
        
        logger.info(f"Created challenge {challenge_id} for device {device_id}")
        
        return {
            'challenge_id': challenge_id,
            'nonce': nonce,
            'expires_at': (datetime.now() + self.challenge_expiry).isoformat()
        }
    
    def compute_response(
        self,
        embedding: torch.Tensor,
        nonce: str
    ) -> str:
        """
        Compute challenge response from embedding and nonce
        
        Args:
            embedding: Device embedding tensor
            nonce: Challenge nonce
            
        Returns:
            Hex-encoded response signature
        """
        # Convert embedding to bytes
        embedding_bytes = embedding.numpy().tobytes()
        
        # Combine embedding hash with nonce
        embedding_hash = hashlib.sha256(embedding_bytes).hexdigest()
        
        # Create response using HMAC
        message = f"{embedding_hash}_{nonce}".encode()
        response = hmac.new(
            embedding_bytes[:32],  # Use first 32 bytes as key
            message,
            hashlib.sha256
        ).hexdigest()
        
        return response
    
    def verify_response(
        self,
        challenge_id: str,
        response: str,
        stored_embedding: torch.Tensor
    ) -> Tuple[bool, Dict]:
        """
        Verify challenge response
        
        Args:
            challenge_id: Challenge identifier
            response: Response signature from device
            stored_embedding: Stored device embedding
            
        Returns:
            Tuple of (is_valid, details)
        """
        # Check if challenge exists
        if challenge_id not in self.active_challenges:
            return False, {'error': 'Challenge not found'}
        
        challenge_data = self.active_challenges[challenge_id]
        
        # Check if challenge expired
        if datetime.now() > challenge_data['expires_at']:
            del self.active_challenges[challenge_id]
            return False, {'error': 'Challenge expired'}
        
        # Compute expected response
        expected_response = self.compute_response(
            stored_embedding,
            challenge_data['nonce']
        )
        
        # Compare responses (constant-time comparison)
        is_valid = hmac.compare_digest(response, expected_response)
        
        # Remove used challenge
        del self.active_challenges[challenge_id]
        
        details = {
            'challenge_id': challenge_id,
            'device_id': challenge_data['device_id'],
            'verified_at': datetime.now().isoformat(),
            'is_valid': is_valid
        }
        
        logger.info(f"Challenge verification: {challenge_id}, valid={is_valid}")
        
        return is_valid, details
    
    def cleanup_expired_challenges(self):
        """Remove expired challenges"""
        now = datetime.now()
        expired = [
            cid for cid, data in self.active_challenges.items()
            if now > data['expires_at']
        ]
        
        for cid in expired:
            del self.active_challenges[cid]
        
        if expired:
            logger.info(f"Cleaned up {len(expired)} expired challenges")
    
    def get_active_challenges_count(self) -> int:
        """Get number of active challenges"""
        self.cleanup_expired_challenges()
        return len(self.active_challenges)


class SecureAuthenticationFlow:
    """Complete secure authentication flow with challenge-response"""
    
    def __init__(
        self,
        protocol: ChallengeResponseProtocol,
        similarity_threshold: float = 0.85
    ):
        """
        Initialize secure authentication flow
        
        Args:
            protocol: ChallengeResponseProtocol instance
            similarity_threshold: Embedding similarity threshold
        """
        self.protocol = protocol
        self.similarity_threshold = similarity_threshold
    
    def initiate_authentication(
        self,
        device_id: str
    ) -> Dict[str, str]:
        """
        Step 1: Server initiates authentication
        
        Args:
            device_id: Device identifier
            
        Returns:
            Challenge dictionary
        """
        return self.protocol.create_challenge(device_id)
    
    def complete_authentication(
        self,
        challenge_id: str,
        response: str,
        auth_embedding: torch.Tensor,
        stored_embedding: torch.Tensor
    ) -> Tuple[bool, Dict]:
        """
        Step 2: Complete authentication with response and embedding verification
        
        Args:
            challenge_id: Challenge identifier
            response: Challenge response from device
            auth_embedding: Fresh authentication embedding
            stored_embedding: Stored device embedding
            
        Returns:
            Tuple of (is_authenticated, details)
        """
        # Verify challenge response
        response_valid, response_details = self.protocol.verify_response(
            challenge_id,
            response,
            stored_embedding
        )
        
        if not response_valid:
            return False, response_details
        
        # Verify embedding similarity
        similarity = torch.nn.functional.cosine_similarity(
            auth_embedding.unsqueeze(0),
            stored_embedding.unsqueeze(0)
        ).item()
        
        embedding_valid = similarity >= self.similarity_threshold
        
        details = {
            **response_details,
            'embedding_similarity': similarity,
            'similarity_threshold': self.similarity_threshold,
            'embedding_valid': embedding_valid,
            'authenticated': response_valid and embedding_valid
        }
        
        is_authenticated = response_valid and embedding_valid
        
        logger.info(f"Authentication complete: authenticated={is_authenticated}, "
                   f"similarity={similarity:.4f}")
        
        return is_authenticated, details


class AntiReplayProtection:
    """Prevents replay attacks"""
    
    def __init__(self, window_seconds: int = 300):
        """
        Initialize anti-replay protection
        
        Args:
            window_seconds: Time window for tracking used responses
        """
        self.window = timedelta(seconds=window_seconds)
        self.used_responses = {}  # Map response_hash -> timestamp
    
    def check_and_record(self, response: str) -> bool:
        """
        Check if response has been used and record it
        
        Args:
            response: Response signature
            
        Returns:
            True if response is fresh (not replayed), False if replayed
        """
        # Cleanup old entries
        now = datetime.now()
        self.used_responses = {
            r: t for r, t in self.used_responses.items()
            if now - t < self.window
        }
        
        # Check if response was used
        if response in self.used_responses:
            logger.warning(f"Replay attack detected!")
            return False
        
        # Record response
        self.used_responses[response] = now
        return True


def main():
    """Test challenge-response protocol"""
    print("\n=== Challenge-Response Protocol Test ===")
    
    # Create protocol
    protocol = ChallengeResponseProtocol(
        nonce_length=32,
        challenge_expiry_seconds=60
    )
    
    # Simulate device embedding
    device_id = "test_device_001"
    embedding = torch.randn(128)
    
    print("\n=== Step 1: Create Challenge ===")
    challenge = protocol.create_challenge(device_id)
    print(f"Challenge ID: {challenge['challenge_id']}")
    print(f"Nonce: {challenge['nonce'][:32]}...")
    print(f"Expires at: {challenge['expires_at']}")
    
    print("\n=== Step 2: Compute Response ===")
    response = protocol.compute_response(embedding, challenge['nonce'])
    print(f"Response: {response[:32]}...")
    
    print("\n=== Step 3: Verify Response ===")
    is_valid, details = protocol.verify_response(
        challenge['challenge_id'],
        response,
        embedding
    )
    print(f"Valid: {is_valid}")
    print(f"Details: {details}")
    
    print("\n=== Testing Secure Authentication Flow ===")
    flow = SecureAuthenticationFlow(protocol, similarity_threshold=0.85)
    
    # Initiate
    challenge = flow.initiate_authentication(device_id)
    print(f"Challenge initiated: {challenge['challenge_id']}")
    
    # Compute response
    response = protocol.compute_response(embedding, challenge['nonce'])
    
    # Complete authentication
    auth_embedding = embedding + 0.01 * torch.randn(128)  # Slightly different
    auth_embedding = torch.nn.functional.normalize(auth_embedding, p=2, dim=0)
    
    is_authenticated, auth_details = flow.complete_authentication(
        challenge['challenge_id'],
        response,
        auth_embedding,
        embedding
    )
    print(f"Authenticated: {is_authenticated}")
    print(f"Similarity: {auth_details['embedding_similarity']:.4f}")
    
    print("\n=== Testing Anti-Replay Protection ===")
    anti_replay = AntiReplayProtection(window_seconds=300)
    
    # First use - should pass
    is_fresh = anti_replay.check_and_record(response)
    print(f"First use - Fresh: {is_fresh}")
    
    # Second use - should fail (replay)
    is_fresh = anti_replay.check_and_record(response)
    print(f"Second use - Fresh: {is_fresh} (replay attack detected)")


if __name__ == "__main__":
    main()
</file>

<file path="auto_collect.py">
import requests
import time
import sys

BASE_URL = "http://localhost:8000"

def collect_device_data(device_name, sources, num_samples):
    print(f"Collecting {device_name} ({num_samples} samples)...")
    payload = {
        "device_name": device_name,
        "sources": sources,
        "num_samples": num_samples
    }
    try:
        response = requests.post(f"{BASE_URL}/enroll", json=payload, timeout=300)
        if response.status_code == 201:
            print(f"‚úÖ Success: {device_name}")
        else:
            print(f"‚ùå Failed: {response.status_code} - {response.text}")
    except Exception as e:
        print(f"‚ùå Error: {e}")

profiles = [
    {"name": "Training_Dev1_Cam", "sources": ["camera"], "samples": 10},
    {"name": "Training_Dev2_Mic", "sources": ["microphone"], "samples": 10},
    {"name": "Training_Dev3_Both", "sources": ["camera", "microphone"], "samples": 10},
    {"name": "Training_Dev4_Cam", "sources": ["camera"], "samples": 10},
    {"name": "Training_Dev5_Mic", "sources": ["microphone"], "samples": 10}
]

def main():
    # Wait for server to be ready
    print("Waiting for server...")
    for i in range(10):
        try:
            r = requests.get(f"{BASE_URL}/health")
            if r.status_code == 200:
                print("Server ready!")
                break
        except:
            pass
        time.sleep(2)
    else:
        print("Server failed to start.")
        sys.exit(1)

    print("Starting auto-collection for training...")
    for p in profiles:
        collect_device_data(p["name"], p["sources"], p["samples"])
        time.sleep(1)
    print("Collection Complete.")

if __name__ == "__main__":
    main()
</file>

<file path="collect_training_data.py">
#!/usr/bin/env python3
"""
Training Data Collection Script
Collects noise samples from multiple devices for training the Siamese model
"""

import requests
import json
import time
from pathlib import Path

BASE_URL = "http://localhost:8000"

def collect_device_data(device_name, noise_sources, num_samples=50):
    """
    Collect data from a device by enrolling it
    
    Args:
        device_name: Unique name for this device
        noise_sources: List of noise sources to use ['qrng', 'camera', 'microphone']
        num_samples: Number of samples to collect (default: 50)
    
    Returns:
        Device ID if successful, None otherwise
    """
    print(f"\n{'='*60}")
    print(f"Collecting data from: {device_name}")
    print(f"Sources: {', '.join(noise_sources)}")
    print(f"Samples: {num_samples}")
    print(f"{'='*60}")
    
    payload = {
        "device_name": device_name,
        "sources": noise_sources,  # API expects 'sources' not 'noise_sources'
        "num_samples": num_samples
    }
    
    try:
        response = requests.post(f"{BASE_URL}/enroll", json=payload, timeout=300)
        
        if response.status_code == 201:
            data = response.json()
            device_id = data.get("device_id")
            print(f"‚úÖ Success! Device ID: {device_id}")
            print(f"   Collected {data['metadata']['num_samples']} samples")
            print(f"   Embedding shape: {data['metadata']['embedding_shape']}")
            return device_id
        else:
            print(f"‚ùå Failed: {response.status_code}")
            print(f"   Error: {response.json()}")
            return None
            
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None

def list_enrolled_devices():
    """List all enrolled devices"""
    try:
        response = requests.get(f"{BASE_URL}/devices")
        if response.status_code == 200:
            data = response.json()
            return data.get("devices", [])
    except:
        return []

def main():
    """Main data collection workflow"""
    print("\n" + "="*60)
    print("QNA-Auth Training Data Collection")
    print("="*60)
    
    # Check server health
    try:
        response = requests.get(f"{BASE_URL}/health")
        if response.status_code != 200:
            print("‚ùå Server is not healthy!")
            return
        print("‚úÖ Server is healthy and ready")
    except:
        print("‚ùå Cannot connect to server!")
        print(f"   Make sure the server is running on {BASE_URL}")
        return
    
    # Show existing devices
    existing = list_enrolled_devices()
    if existing:
        print(f"\nüìä Already enrolled: {len(existing)} devices")
        for dev_id in existing:
            print(f"   - {dev_id}")
    
    print("\n" + "="*60)
    print("Data Collection Strategy:")
    print("="*60)
    print("For best results, collect data from:")
    print("  ‚Ä¢ 5-10 different physical devices (phones, laptops, desktops)")
    print("  ‚Ä¢ 50-100 samples per device")
    print("  ‚Ä¢ Mix of noise sources (QRNG, camera, microphone)")
    print("  ‚Ä¢ Multiple sessions per device (morning/evening)")
    print("\n" + "="*60)
    
    # Collection profiles - REAL ENTROPY ONLY (camera/microphone)
    profiles = [
        {
            "name": "Device1_Camera",
            "sources": ["camera"],
            "samples": 50,
            "description": "Primary device with camera noise (real entropy)"
        },
        {
            "name": "Device2_Microphone",
            "sources": ["microphone"],
            "samples": 50,
            "description": "Secondary device with microphone noise (real entropy)"
        },
        {
            "name": "Device3_Both",
            "sources": ["camera", "microphone"],
            "samples": 50,
            "description": "Third device with camera + microphone (real entropy)"
        },
        {
            "name": "Device4_Camera_More",
            "sources": ["camera"],
            "samples": 75,
            "description": "Fourth device with more camera samples (real entropy)"
        },
        {
            "name": "Device5_Full_Real",
            "sources": ["camera", "microphone"],
            "samples": 75,
            "description": "Fifth device with all real entropy sources"
        }
    ]
    
    print("\nSuggested Collection Profiles:")
    for i, profile in enumerate(profiles, 1):
        print(f"\n{i}. {profile['name']}")
        print(f"   Sources: {', '.join(profile['sources'])}")
        print(f"   Samples: {profile['samples']}")
        print(f"   {profile['description']}")
    
    print("\n" + "="*60)
    print("Options:")
    print("  1. Collect data for a specific profile (1-5)")
    print("  2. Custom device (manual input)")
    print("  3. Auto-collect all profiles (recommended)")
    print("  4. Exit")
    print("="*60)
    
    choice = input("\nEnter your choice (1-4): ").strip()
    
    if choice == "1":
        profile_num = input("Enter profile number (1-5): ").strip()
        try:
            profile = profiles[int(profile_num) - 1]
            collect_device_data(
                profile["name"],
                profile["sources"],
                profile["samples"]
            )
        except:
            print("Invalid profile number")
            
    elif choice == "2":
        name = input("Device name: ").strip()
        print("Available sources: qrng, camera, microphone")
        sources = input("Enter sources (comma-separated): ").strip().split(",")
        sources = [s.strip() for s in sources if s.strip()]
        samples = int(input("Number of samples (default 50): ").strip() or "50")
        
        collect_device_data(name, sources, samples)
        
    elif choice == "3":
        print("\nüöÄ Starting auto-collection of all profiles...")
        print("‚ö†Ô∏è  This will take 10-30 minutes depending on noise sources")
        print("‚ö†Ô∏è  QRNG may be rate-limited, fallback will be used")
        
        confirm = input("\nContinue? (yes/no): ").strip().lower()
        if confirm == "yes":
            successful = 0
            failed = 0
            
            for i, profile in enumerate(profiles, 1):
                print(f"\n\n[{i}/{len(profiles)}] Collecting {profile['name']}...")
                time.sleep(2)  # Small delay between collections
                
                device_id = collect_device_data(
                    profile["name"],
                    profile["sources"],
                    profile["samples"]
                )
                
                if device_id:
                    successful += 1
                else:
                    failed += 1
                    
            print("\n" + "="*60)
            print("Collection Complete!")
            print(f"‚úÖ Successful: {successful}")
            print(f"‚ùå Failed: {failed}")
            print("="*60)
            
            # Show final device count
            all_devices = list_enrolled_devices()
            print(f"\nüìä Total enrolled devices: {len(all_devices)}")
            
            if len(all_devices) >= 5:
                print("\nüéØ You have enough data to train the model!")
                print("   Next step: python model/train.py")
            else:
                print(f"\n‚ö†Ô∏è  Recommended: {5 - len(all_devices)} more devices")
                print("   Collect more data for better model performance")
    
    elif choice == "4":
        print("Exiting...")
        return
    
    else:
        print("Invalid choice")

if __name__ == "__main__":
    main()
</file>

<file path="config.example.py">
# QNA-Auth Configuration
# Copy this file to config.py and adjust values

# Model Configuration
MODEL_CONFIG = {
    "input_dim": 50,  # Must match feature extractor output
    "embedding_dim": 128,
    "hidden_dims": [256, 256, 128],
    "model_path": "./model/checkpoints/best_model.pt"
}

# Training Configuration
TRAINING_CONFIG = {
    "batch_size": 32,
    "learning_rate": 0.001,
    "epochs": 50,
    "loss_type": "triplet",  # or "contrastive"
    "margin": 1.0,
    "samples_per_epoch": 1000
}

# Authentication Configuration
AUTH_CONFIG = {
    "similarity_threshold": 0.85,
    "similarity_metric": "cosine",  # or "euclidean"
    "max_auth_attempts": 3
}

# Challenge-Response Configuration
CHALLENGE_CONFIG = {
    "nonce_length": 32,
    "challenge_expiry_seconds": 60,
    "anti_replay_window_seconds": 300
}

# Noise Collection Configuration
NOISE_CONFIG = {
    "qrng": {
        "api_url": "https://qrng.anu.edu.au/API/jsonI.php",
        "sample_size": 1024,
        "num_samples": 50
    },
    "camera": {
        "camera_index": 0,
        "exposure_time": 0.1,
        "num_frames": 50
    },
    "microphone": {
        "sample_rate": 44100,
        "duration": 0.5,
        "num_samples": 50
    }
}

# Preprocessing Configuration
PREPROCESSING_CONFIG = {
    "normalize": True,
    "normalization_method": "standard",  # "standard", "minmax", or "robust"
    "fft_sample_rate": 1.0,
    "entropy_bins": 256
}

# Storage Configuration
STORAGE_CONFIG = {
    "dataset_dir": "./dataset/samples",
    "embeddings_dir": "./auth/device_embeddings",
    "checkpoints_dir": "./model/checkpoints"
}

# Server Configuration
SERVER_CONFIG = {
    "host": "0.0.0.0",
    "port": 8000,
    "reload": False,  # Set to True for development
    "workers": 1
}

# CORS Configuration
CORS_CONFIG = {
    "allow_origins": ["http://localhost:3000", "http://localhost:5173"],
    "allow_credentials": True,
    "allow_methods": ["*"],
    "allow_headers": ["*"]
}

# Logging Configuration
LOGGING_CONFIG = {
    "level": "INFO",
    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    "file": "qna_auth.log"
}
</file>

<file path="dataset/__init__.py">
"""
Dataset Module Initialization
"""

from .builder import DatasetBuilder

__all__ = ['DatasetBuilder']
</file>

<file path="dataset/builder.py">
"""
Dataset Builder for QNA-Auth
Creates labeled datasets from collected noise samples
"""

import os
import json
import csv
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Any
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DatasetBuilder:
    """Builds and manages labeled datasets for device authentication"""
    
    def __init__(self, base_dir: str = "./dataset/samples"):
        """
        Initialize dataset builder
        
        Args:
            base_dir: Base directory for storing datasets
        """
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        self.csv_file = self.base_dir / "noise_samples.csv"
        self.json_dir = self.base_dir / "json"
        self.json_dir.mkdir(exist_ok=True)
        
        # Initialize CSV if it doesn't exist
        if not self.csv_file.exists():
            self._initialize_csv()
    
    def _initialize_csv(self):
        """Create CSV file with headers"""
        headers = [
            'sample_id',
            'device_id',
            'timestamp',
            'noise_source',
            'sample_length',
            'mean',
            'std',
            'min',
            'max',
            'entropy',
            'raw_data_path'
        ]
        
        with open(self.csv_file, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=headers)
            writer.writeheader()
        
        logger.info(f"Initialized CSV dataset at {self.csv_file}")
    
    def create_sample(
        self,
        device_id: str,
        noise_source: str,
        raw_noise_sample: np.ndarray,
        processed_features: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Create a labeled sample entry
        
        Args:
            device_id: Unique device identifier
            noise_source: Source of noise (qrng, camera, microphone, sensor)
            raw_noise_sample: Raw noise array
            processed_features: Optional pre-computed features
            
        Returns:
            Dictionary containing sample metadata
        """
        # Generate unique sample ID
        sample_id = f"{device_id}_{noise_source}_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
        timestamp = datetime.now().isoformat()
        
        # Compute basic statistics
        stats = {
            'mean': float(np.mean(raw_noise_sample)),
            'std': float(np.std(raw_noise_sample)),
            'min': float(np.min(raw_noise_sample)),
            'max': float(np.max(raw_noise_sample)),
            'length': len(raw_noise_sample)
        }
        
        # Compute entropy
        entropy = self._compute_entropy(raw_noise_sample)
        
        # Save raw data to numpy file
        raw_data_path = self.base_dir / f"{sample_id}_raw.npy"
        np.save(raw_data_path, raw_noise_sample)
        
        # Create sample metadata
        sample_data = {
            'sample_id': sample_id,
            'device_id': device_id,
            'timestamp': timestamp,
            'noise_source': noise_source,
            'sample_length': stats['length'],
            'mean': stats['mean'],
            'std': stats['std'],
            'min': stats['min'],
            'max': stats['max'],
            'entropy': entropy,
            'raw_data_path': str(raw_data_path.relative_to(self.base_dir))
        }
        
        # Add processed features if provided
        if processed_features:
            sample_data['processed_features'] = processed_features
        
        return sample_data
    
    def add_sample(self, sample_data: Dict[str, Any]):
        """
        Add a sample to the dataset
        
        Args:
            sample_data: Sample metadata dictionary
        """
        # Append to CSV
        with open(self.csv_file, 'a', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=sample_data.keys())
            # Write only CSV-compatible fields
            csv_data = {k: v for k, v in sample_data.items() if k != 'processed_features'}
            writer.writerow(csv_data)
        
        # Save full metadata to JSON
        json_path = self.json_dir / f"{sample_data['sample_id']}.json"
        with open(json_path, 'w') as f:
            json.dump(sample_data, f, indent=2)
        
        logger.info(f"Added sample: {sample_data['sample_id']}")
    
    def add_batch(
        self,
        device_id: str,
        noise_source: str,
        samples: List[np.ndarray],
        features_list: Optional[List[Dict[str, Any]]] = None
    ) -> List[str]:
        """
        Add multiple samples in batch
        
        Args:
            device_id: Device identifier
            noise_source: Noise source type
            samples: List of noise arrays
            features_list: Optional list of feature dictionaries
            
        Returns:
            List of created sample IDs
        """
        sample_ids = []
        
        if features_list is None:
            features_list = [None] * len(samples)
        
        for i, (sample, features) in enumerate(zip(samples, features_list)):
            sample_data = self.create_sample(
                device_id=device_id,
                noise_source=noise_source,
                raw_noise_sample=sample,
                processed_features=features
            )
            self.add_sample(sample_data)
            sample_ids.append(sample_data['sample_id'])
            
            if (i + 1) % 10 == 0:
                logger.info(f"Added {i + 1}/{len(samples)} samples")
        
        logger.info(f"Batch complete: {len(sample_ids)} samples added")
        return sample_ids
    
    def get_samples_by_device(self, device_id: str) -> List[Dict[str, Any]]:
        """
        Retrieve all samples for a specific device
        
        Args:
            device_id: Device identifier
            
        Returns:
            List of sample metadata dictionaries
        """
        samples = []
        
        with open(self.csv_file, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row['device_id'] == device_id:
                    samples.append(row)
        
        logger.info(f"Found {len(samples)} samples for device {device_id}")
        return samples
    
    def get_samples_by_source(self, noise_source: str) -> List[Dict[str, Any]]:
        """
        Retrieve all samples from a specific noise source
        
        Args:
            noise_source: Noise source type
            
        Returns:
            List of sample metadata dictionaries
        """
        samples = []
        
        with open(self.csv_file, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row['noise_source'] == noise_source:
                    samples.append(row)
        
        logger.info(f"Found {len(samples)} samples from {noise_source}")
        return samples
    
    def load_raw_sample(self, sample_id: str) -> Optional[np.ndarray]:
        """
        Load raw noise data for a sample
        
        Args:
            sample_id: Sample identifier
            
        Returns:
            Numpy array of raw noise data
        """
        # Find sample in JSON
        json_path = self.json_dir / f"{sample_id}.json"
        
        if not json_path.exists():
            logger.error(f"Sample {sample_id} not found")
            return None
        
        with open(json_path, 'r') as f:
            metadata = json.load(f)
        
        # Load raw data
        raw_path = self.base_dir / metadata['raw_data_path']
        if not raw_path.exists():
            logger.error(f"Raw data file not found: {raw_path}")
            return None
        
        return np.load(raw_path)
    
    def get_dataset_statistics(self) -> Dict[str, Any]:
        """
        Get overall dataset statistics
        
        Returns:
            Dictionary of dataset statistics
        """
        devices = set()
        sources = {}
        total_samples = 0
        
        with open(self.csv_file, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                devices.add(row['device_id'])
                source = row['noise_source']
                sources[source] = sources.get(source, 0) + 1
                total_samples += 1
        
        stats = {
            'total_samples': total_samples,
            'unique_devices': len(devices),
            'devices': list(devices),
            'samples_by_source': sources
        }
        
        return stats
    
    def _compute_entropy(self, data: np.ndarray) -> float:
        """
        Compute Shannon entropy of data
        
        Args:
            data: Input array
            
        Returns:
            Entropy value
        """
        # Quantize to reasonable bins
        hist, _ = np.histogram(data, bins=256)
        hist = hist[hist > 0]  # Remove zero bins
        
        # Normalize to probabilities
        probs = hist / hist.sum()
        
        # Calculate entropy
        entropy = -np.sum(probs * np.log2(probs))
        
        return float(entropy)
    
    def export_for_training(
        self,
        output_dir: str,
        train_ratio: float = 0.8
    ) -> Dict[str, str]:
        """
        Export dataset split for training
        
        Args:
            output_dir: Output directory for train/test split
            train_ratio: Ratio of training samples
            
        Returns:
            Dictionary with paths to train/test files
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Read all samples
        samples = []
        with open(self.csv_file, 'r') as f:
            reader = csv.DictReader(f)
            samples = list(reader)
        
        # Shuffle and split
        np.random.shuffle(samples)
        split_idx = int(len(samples) * train_ratio)
        
        train_samples = samples[:split_idx]
        test_samples = samples[split_idx:]
        
        # Save splits
        train_file = output_path / "train.csv"
        test_file = output_path / "test.csv"
        
        with open(train_file, 'w', newline='') as f:
            if train_samples:
                writer = csv.DictWriter(f, fieldnames=train_samples[0].keys())
                writer.writeheader()
                writer.writerows(train_samples)
        
        with open(test_file, 'w', newline='') as f:
            if test_samples:
                writer = csv.DictWriter(f, fieldnames=test_samples[0].keys())
                writer.writeheader()
                writer.writerows(test_samples)
        
        logger.info(f"Exported {len(train_samples)} train, {len(test_samples)} test samples")
        
        return {
            'train': str(train_file),
            'test': str(test_file)
        }


def main():
    """Test dataset builder"""
    builder = DatasetBuilder()
    
    print("\n=== Dataset Builder Test ===")
    
    # Create synthetic samples for testing
    print("\n=== Creating Test Samples ===")
    
    # Device 1 samples
    device1_samples = [
        np.random.rand(1024) for _ in range(5)
    ]
    builder.add_batch(
        device_id="device_001",
        noise_source="qrng",
        samples=device1_samples
    )
    
    # Device 2 samples
    device2_samples = [
        np.random.rand(512) for _ in range(3)
    ]
    builder.add_batch(
        device_id="device_002",
        noise_source="camera",
        samples=device2_samples
    )
    
    # Get statistics
    print("\n=== Dataset Statistics ===")
    stats = builder.get_dataset_statistics()
    print(json.dumps(stats, indent=2))
    
    # Query samples
    print("\n=== Query Samples ===")
    device1_data = builder.get_samples_by_device("device_001")
    print(f"Device 001 samples: {len(device1_data)}")
    
    qrng_data = builder.get_samples_by_source("qrng")
    print(f"QRNG samples: {len(qrng_data)}")
    
    # Export for training
    print("\n=== Exporting for Training ===")
    export_paths = builder.export_for_training("./dataset/training_data")
    print(f"Train file: {export_paths['train']}")
    print(f"Test file: {export_paths['test']}")


if __name__ == "__main__":
    main()
</file>

<file path="frontend/.eslintrc.cjs">
module.exports = {
  root: true,
  env: { browser: true, es2020: true },
  extends: [
    'eslint:recommended',
    'plugin:@typescript-eslint/recommended',
    'plugin:react-hooks/recommended',
  ],
  ignorePatterns: ['dist', '.eslintrc.cjs'],
  parser: '@typescript-eslint/parser',
  plugins: ['react-refresh'],
  rules: {
    'react-refresh/only-export-components': [
      'warn',
      { allowConstantExport: true },
    ],
  },
}
</file>

<file path="frontend/index.html">
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>QNA-Auth - Quantum Noise Authentication</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>
</file>

<file path="frontend/postcss.config.js">
export default {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
}
</file>

<file path="frontend/src/App.tsx">
import { BrowserRouter as Router, Routes, Route, Link } from 'react-router-dom'
import HomePage from './pages/HomePage'
import EnrollPage from './pages/EnrollPage'
import AuthenticatePage from './pages/AuthenticatePage'
import DevicesPage from './pages/DevicesPage'
import { BackgroundBeams } from './components/ui/background-beams'

function App() {
  return (
    <Router>
      <div className="relative min-h-screen bg-neutral-950 text-white">
        <BackgroundBeams />
        <nav className="relative z-10 bg-neutral-900/80 backdrop-blur-sm border-b border-neutral-800">
          <div className="container mx-auto px-4 py-4">
            <div className="flex items-center justify-between">
              <Link to="/" className="text-xl font-bold">QNA-Auth</Link>
              <div className="flex space-x-6">
                <Link to="/" className="hover:text-blue-400 transition">Home</Link>
                <Link to="/enroll" className="hover:text-blue-400 transition">Enroll</Link>
                <Link to="/authenticate" className="hover:text-blue-400 transition">Authenticate</Link>
                <Link to="/devices" className="hover:text-blue-400 transition">Devices</Link>
              </div>
            </div>
          </div>
        </nav>

        <main className="relative z-10 container mx-auto px-4 py-8">
          <Routes>
            <Route path="/" element={<HomePage />} />
            <Route path="/enroll" element={<EnrollPage />} />
            <Route path="/authenticate" element={<AuthenticatePage />} />
            <Route path="/devices" element={<DevicesPage />} />
          </Routes>
        </main>
      </div>
    </Router>
  )
}

export default App
</file>

<file path="frontend/src/components/ui/background-beams.tsx">
import { motion } from "framer-motion"
import { cn } from "../../lib/utils"

export const BackgroundBeams = ({ className }: { className?: string }) => {
  const paths = [
    "M-380 -189C-380 -189 -312 216 152 343C616 470 684 875 684 875",
    "M-373 -197C-373 -197 -305 208 159 335C623 462 691 867 691 867",
    "M-366 -205C-366 -205 -298 200 166 327C630 454 698 859 698 859",
    "M-359 -213C-359 -213 -291 192 173 319C637 446 705 851 705 851",
    "M-352 -221C-352 -221 -284 184 180 311C644 438 712 843 712 843",
    "M-345 -229C-345 -229 -277 176 187 303C651 430 719 835 719 835",
    "M-338 -237C-338 -237 -270 168 194 295C658 422 726 827 726 827",
    "M-331 -245C-331 -245 -263 160 201 287C665 414 733 819 733 819",
    "M-324 -253C-324 -253 -256 152 208 279C672 406 740 811 740 811",
    "M-317 -261C-317 -261 -249 144 215 271C679 398 747 803 747 803",
    "M-310 -269C-310 -269 -242 136 222 263C686 390 754 795 754 795",
  ]

  return (
    <div
      className={cn(
        "absolute h-full w-full inset-0 flex items-center justify-center",
        className
      )}
    >
      <svg
        className="absolute inset-0 h-full w-full pointer-events-none z-0"
        width="100%"
        height="100%"
        viewBox="0 0 696 316"
        fill="none"
        xmlns="http://www.w3.org/2000/svg"
      >
        {paths.map((path, index) => (
          <motion.path
            key={`path-${index}`}
            d={path}
            stroke={`url(#linearGradient-${index})`}
            strokeOpacity="0.4"
            strokeWidth="0.5"
          />
        ))}

        <defs>
          {paths.map((_, index) => (
            <motion.linearGradient
              id={`linearGradient-${index}`}
              key={`gradient-${index}`}
              initial={{
                x1: "0%",
                x2: "0%",
                y1: "0%",
                y2: "0%",
              }}
              animate={{
                x1: ["0%", "100%"],
                x2: ["0%", "95%"],
                y1: ["0%", "100%"],
                y2: ["0%", `${93 + Math.random() * 8}%`],
              }}
              transition={{
                duration: Math.random() * 10 + 10,
                ease: "linear",
                repeat: Infinity,
                delay: Math.random() * 10,
              }}
            >
              <stop stopColor="#18CCFC" stopOpacity="0" />
              <stop stopColor="#18CCFC" />
              <stop offset="32.5%" stopColor="#6344F5" />
              <stop offset="100%" stopColor="#AE48FF" stopOpacity="0" />
            </motion.linearGradient>
          ))}
        </defs>
      </svg>
    </div>
  )
}
</file>

<file path="frontend/src/index.css">
@tailwind base;
@tailwind components;
@tailwind utilities;

:root {
  font-family: Inter, system-ui, Avenir, Helvetica, Arial, sans-serif;
  line-height: 1.5;
  font-weight: 400;

  color-scheme: light dark;
  color: rgba(255, 255, 255, 0.87);
  background-color: #242424;

  font-synthesis: none;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

body {
  margin: 0;
  display: flex;
  place-items: center;
  min-width: 320px;
  min-height: 100vh;
}

#root {
  width: 100%;
  min-height: 100vh;
}
</file>

<file path="frontend/src/main.tsx">
import React from 'react'
import ReactDOM from 'react-dom/client'
import App from './App.tsx'
import './index.css'

ReactDOM.createRoot(document.getElementById('root')!).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
)
</file>

<file path="frontend/src/pages/DevicesPage.tsx">
import { useState, useEffect } from 'react'
import { qnaAuthService } from '../services/api'
import { Loader2, Trash2 } from 'lucide-react'

interface DeviceInfo {
  device_id: string
  device_name?: string
  enrollment_date: string
  num_samples?: number
  sources?: string[]
}

export default function DevicesPage() {
  const [devices, setDevices] = useState<string[]>([])
  const [selectedDevice, setSelectedDevice] = useState<DeviceInfo | null>(null)
  const [loading, setLoading] = useState(true)
  const [deleting, setDeleting] = useState<string | null>(null)

  useEffect(() => {
    loadDevices()
  }, [])

  const loadDevices = async () => {
    setLoading(true)
    try {
      const response = await qnaAuthService.listDevices()
      setDevices(response.devices)
    } catch (err) {
      console.error('Failed to load devices:', err)
    } finally {
      setLoading(false)
    }
  }

  const loadDeviceInfo = async (deviceId: string) => {
    try {
      const info = await qnaAuthService.getDevice(deviceId)
      setSelectedDevice(info)
    } catch (err) {
      console.error('Failed to load device info:', err)
    }
  }

  const handleDelete = async (deviceId: string) => {
    if (!confirm(`Are you sure you want to delete device ${deviceId}?`)) {
      return
    }

    setDeleting(deviceId)
    try {
      await qnaAuthService.deleteDevice(deviceId)
      setDevices(prev => prev.filter(d => d !== deviceId))
      if (selectedDevice?.device_id === deviceId) {
        setSelectedDevice(null)
      }
    } catch (err) {
      console.error('Failed to delete device:', err)
      alert('Failed to delete device')
    } finally {
      setDeleting(null)
    }
  }

  return (
    <div className="min-h-[calc(100vh-8rem)] p-4">
      <div className="max-w-6xl mx-auto py-8">
        <h1 className="text-3xl font-bold mb-6">Enrolled Devices</h1>

        {loading ? (
          <div className="flex items-center justify-center py-12">
            <Loader2 className="w-8 h-8 animate-spin text-blue-500" />
          </div>
        ) : devices.length === 0 ? (
          <div className="bg-neutral-900/80 backdrop-blur-sm border border-neutral-800 p-12 text-center">
            <h2 className="text-2xl font-semibold mb-2">No Devices Enrolled</h2>
            <a
              href="/enroll"
              className="inline-block bg-blue-600 hover:bg-blue-700 text-white px-6 py-3 font-semibold transition mt-4"
            >
              Enroll Device
            </a>
          </div>
        ) : (
          <div className="grid md:grid-cols-3 gap-6">
            <div className="md:col-span-1 space-y-3">
              {devices.map((deviceId) => (
                <button
                  key={deviceId}
                  onClick={() => loadDeviceInfo(deviceId)}
                  className={`w-full text-left p-4 transition border ${
                    selectedDevice?.device_id === deviceId
                      ? 'bg-blue-600 border-blue-500'
                      : 'bg-neutral-900/80 backdrop-blur-sm border-neutral-800 hover:bg-neutral-800/80'
                  }`}
                >
                  <div className="font-mono text-sm truncate">{deviceId}</div>
                </button>
              ))}
            </div>

            <div className="md:col-span-2">
              {selectedDevice ? (
                <div className="bg-neutral-900/80 backdrop-blur-sm border border-neutral-800 p-6">
                  <div className="flex items-start justify-between mb-6">
                    <div>
                      <h2 className="text-2xl font-bold mb-2">Device Details</h2>
                      <p className="text-neutral-400 font-mono text-sm">
                        {selectedDevice.device_id}
                      </p>
                    </div>
                    <button
                    onClick={() => handleDelete(selectedDevice.device_id)}
                    disabled={deleting === selectedDevice.device_id}
                    className="bg-red-600 hover:bg-red-700 disabled:bg-neutral-600 text-white px-4 py-2 flex items-center gap-2 transition"
                  >
                    {deleting === selectedDevice.device_id ? (
                      <>
                        <Loader2 className="w-4 h-4 animate-spin" />
                        <span>Deleting...</span>
                      </>
                    ) : (
                      <>
                        <Trash2 className="w-4 h-4" />
                        <span>Delete</span>
                      </>
                    )}
                  </button>
                </div>

                <div className="space-y-4">
                  {selectedDevice.device_name && (
                    <InfoRow
                      label="Device Name"
                      value={selectedDevice.device_name}
                    />
                  )}

                  <InfoRow
                    label="Enrollment Date"
                    value={new Date(selectedDevice.enrollment_date).toLocaleString()}
                  />

                  {selectedDevice.num_samples && (
                    <InfoRow
                      label="Samples"
                      value={selectedDevice.num_samples.toString()}
                    />
                  )}

                  {selectedDevice.sources && (
                    <InfoRow
                      label="Sources"
                      value={selectedDevice.sources.join(', ')}
                    />
                  )}
                </div>
              </div>
            ) : (
              <div className="bg-neutral-900/80 backdrop-blur-sm border border-neutral-800 p-12 text-center">
                <p className="text-neutral-400">Select a device to view details</p>
              </div>
            )}
          </div>
        </div>
      )}
      </div>
    </div>
  )
}

interface InfoRowProps {
  label: string
  value: string
}

function InfoRow({ label, value }: InfoRowProps) {
  return (
    <div className="p-3 bg-neutral-800 border border-neutral-700">
      <div className="text-sm text-neutral-400">{label}</div>
      <div className="font-medium">{value}</div>
    </div>
  )
}
</file>

<file path="frontend/src/pages/HomePage.tsx">
import { Link } from 'react-router-dom'

export default function HomePage() {
  return (
    <div className="flex flex-col items-center justify-center min-h-[calc(100vh-8rem)] antialiased">
      <div className="max-w-2xl mx-auto p-4">
        <h1 className="text-6xl md:text-7xl bg-clip-text text-transparent bg-gradient-to-b from-neutral-200 to-neutral-600 text-center font-bold mb-8">
          QNA-Auth
        </h1>
        <p className="text-neutral-400 max-w-lg mx-auto text-center mb-12">
          Quantum Noise Assisted Authentication System
        </p>
        
        <div className="flex justify-center gap-4">
          <Link
            to="/enroll"
            className="bg-neutral-800 border border-neutral-700 text-white px-6 py-3 font-semibold hover:bg-neutral-700 transition"
          >
            Enroll Device
          </Link>
          <Link
            to="/authenticate"
            className="bg-blue-600 text-white px-6 py-3 font-semibold hover:bg-blue-700 transition"
          >
            Authenticate
          </Link>
        </div>
      </div>
    </div>
  )
}
</file>

<file path="frontend/src/services/collectors.ts">
// Camera Noise Collector
export class CameraNoiseCollector {
    private video: HTMLVideoElement | null = null;
    private canvas: HTMLCanvasElement;
    private ctx: CanvasRenderingContext2D | null = null;
    private stream: MediaStream | null = null;

    constructor() {
        this.canvas = document.createElement('canvas');
        this.ctx = this.canvas.getContext('2d');
    }

    async initialize() {
        try {
            // Mobile-friendly constraints: prefer rear camera, prefer 640x480
            const constraints: MediaStreamConstraints = {
                video: {
                    facingMode: 'environment', // Prefer rear camera
                    width: { ideal: 640 },
                    height: { ideal: 480 }
                }
            };
            
            this.stream = await navigator.mediaDevices.getUserMedia(constraints);
            this.video = document.createElement('video');
            this.video.srcObject = this.stream;
            // Required for mobile safari/chrome to play without UI
            this.video.setAttribute('playsinline', 'true'); 
            await this.video.play();
            return true;
        } catch (error) {
            console.error('Camera initialization failed:', error);
            // Fallback: try without constraints
            try {
                this.stream = await navigator.mediaDevices.getUserMedia({ video: true });
                this.video = document.createElement('video');
                this.video.srcObject = this.stream;
                this.video.setAttribute('playsinline', 'true');
                await this.video.play();
                return true;
            } catch (retryError) {
                console.error('Retry failed:', retryError);
                return false;
            }
        }
    }

    async captureDarkFrame(exposureTime = 100): Promise<number[] | null> {
        if (!this.video || !this.ctx) return null;

        // Wait to simulate exposure
        await new Promise(resolve => setTimeout(resolve, exposureTime));

        // Force resize to 640x480 for consistency and performance
        const targetWidth = 640;
        const targetHeight = 480;

        this.canvas.width = targetWidth;
        this.canvas.height = targetHeight;
        
        // Draw video frame scaled to canvas size
        this.ctx.drawImage(this.video, 0, 0, targetWidth, targetHeight);

        const imageData = this.ctx.getImageData(0, 0, targetWidth, targetHeight);
        const data = imageData.data;
        const grayscaleData: number[] = [];

        // Convert to grayscale
        for (let i = 0; i < data.length; i += 4) {
            // R=data[i], G=data[i+1], B=data[i+2]
            // Standard luminosity method: 0.21 R + 0.72 G + 0.07 B
            const gray = 0.299 * data[i] + 0.587 * data[i+1] + 0.114 * data[i+2];
            grayscaleData.push(gray);
        }

        // Apply simple noise extraction (High-pass filter equivalent)
        // Here we just use the raw flattened array as it will be processed on backend
        // Ideally, we'd do the same blurring subtraction here
        
        return grayscaleData;
    }

    release() {
        if (this.stream) {
            this.stream.getTracks().forEach(track => track.stop());
            this.stream = null;
        }
        this.video = null;
    }
}

// Microphone Noise Collector
export class MicNoiseCollector {
    private audioContext: AudioContext | null = null;
    private stream: MediaStream | null = null;
    private source: MediaStreamAudioSourceNode | null = null;
    private processor: ScriptProcessorNode | null = null;
    private gainNode: GainNode | null = null;

    async initialize() {
        try {
            this.stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
            
            // Critical for mobile: Resume context if suspended (requires user gesture, which enrolling is)
            if (this.audioContext.state === 'suspended') {
                await this.audioContext.resume();
            }

            this.source = this.audioContext.createMediaStreamSource(this.stream);
            return true;
        } catch (error) {
            console.error('Microphone initialization failed:', error);
            return false;
        }
    }

    async captureAmbientNoise(duration = 1.0): Promise<number[] | null> {
        if (!this.audioContext || !this.source) return null;
        
        return new Promise((resolve) => {
            const sampleRate = this.audioContext!.sampleRate;
            const samplesToCollect = Math.floor(sampleRate * duration);
            let collectedSamples: Float32Array = new Float32Array(0);

            // Create a ScriptProcessor (bufferSize, inputChannels, outputChannels)
            const bufferSize = 4096;
            const processor = this.audioContext!.createScriptProcessor(bufferSize, 1, 1);
            
            // Create a GainNode set to 0 to prevent feedback/hearing yourself
            const gainNode = this.audioContext!.createGain();
            gainNode.gain.value = 0;

            processor.onaudioprocess = (e) => {
                const inputData = e.inputBuffer.getChannelData(0);
                const newBuffer = new Float32Array(collectedSamples.length + inputData.length);
                newBuffer.set(collectedSamples);
                newBuffer.set(inputData, collectedSamples.length);
                collectedSamples = newBuffer;

                if (collectedSamples.length >= samplesToCollect) {
                    // Stop processing
                    this.source?.disconnect(processor);
                    processor.disconnect();
                    gainNode.disconnect();
                    
                    // Return exactly required samples
                    resolve(Array.from(collectedSamples.slice(0, samplesToCollect)));
                }
            };

            // Connect graph: Source -> Processor -> Gain(Mute) -> Destination
            // We need to connect to destination for the graph to run in most browsers
            this.source!.connect(processor);
            processor.connect(gainNode);
            gainNode.connect(this.audioContext!.destination);
        });
    }

    release() {
        if (this.stream) {
            this.stream.getTracks().forEach(track => track.stop());
            this.stream = null;
        }
        if (this.audioContext) {
            this.audioContext.close();
            this.audioContext = null;
        }
    }
}
</file>

<file path="frontend/src/vite-env.d.ts">
/// <reference types="vite/client" />

interface ImportMetaEnv {
  readonly VITE_API_URL?: string
  // more env variables...
}

interface ImportMeta {
  readonly env: ImportMetaEnv
}
</file>

<file path="frontend/tailwind.config.js">
/** @type {import('tailwindcss').Config} */
export default {
  content: [
    "./index.html",
    "./src/**/*.{js,ts,jsx,tsx}",
  ],
  theme: {
    extend: {},
  },
  plugins: [],
}
</file>

<file path="frontend/tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2020",
    "useDefineForClassFields": true,
    "lib": ["ES2020", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx",

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noFallthroughCasesInSwitch": true
  },
  "include": ["src"],
  "references": [{ "path": "./tsconfig.node.json" }]
}
</file>

<file path="frontend/tsconfig.node.json">
{
  "compilerOptions": {
    "composite": true,
    "skipLibCheck": true,
    "module": "ESNext",
    "moduleResolution": "bundler",
    "allowSyntheticDefaultImports": true,
    "strict": true,
    "forceConsistentCasingInFileNames": true
  },
  "include": ["vite.config.ts"]
}
</file>

<file path="frontend/vite.config.ts">
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vitejs.dev/config/
export default defineConfig({
  plugins: [react()],
  server: {
    port: 3000,
    proxy: {
      '/api': {
        target: 'http://localhost:8000',
        changeOrigin: true,
        rewrite: (path: string) => path.replace(/^\/api/, '')
      }
    }
  }
})
</file>

<file path="IMPROVEMENTS.md">
# QNA-Auth: Research & Development Roadmap

> **Note**: This project is a **research prototype**, not a production web application. Priorities below are reordered to emphasize the core quantum authentication innovation over web development polish.

This document provides a **detailed gap analysis** comparing the current implementation against the stated mission, and outlines specific improvements needed to realize the full vision of a "physics-based, AI-validated, cryptographically reinforced" authentication system.

**Primary goal**: Prove that quantum noise can reliably fingerprint devices and publish findings.  
**Secondary goal**: Provide a reference implementation for researchers and industry.

---

## Research-first priorities (reordered)

### What matters most
1. **Core authentication mechanism** - Prove quantum noise can fingerprint devices
2. **Statistical validation** - Demonstrate distinguishability and stability
3. **Security analysis** - Prove resistance to attacks (replay, cloning, synthesis)
4. **Model optimization** - Achieve <1% FAR/FRR with minimal samples
5. **Academic publication** - Document findings in peer-reviewed venue

### What matters less (for research project)
- Production API scalability (this is a prototype)
- Web UI polish (Jupyter notebooks are sufficient)
- User management and multi-tenancy (one researcher at a time)
- Deployment automation (Docker for reproducibility is enough)
- Monitoring dashboards (experiment logs are sufficient)

**If you have limited time**: Focus on sections 2 (ML & Model), 3 (Anti-Attack), and the data collection/analysis workflow. The web stack (FastAPI/React) is just a demo harness.

---

## Mission statement recap

> "QNA-Auth is a novel authentication system that uses quantum noise patterns combined with lightweight AI models to provide highly secure, non-reproducible device authentication... The system does not store raw noise; instead, it stores a secure, non-invertible embedding... Because each authentication uses new, unpredictable noise data, replay and synthetic attacks are ineffective."

---

## Current state assessment

### ‚úÖ What's working
- ‚úÖ Multi-source noise collection (QRNG, camera, microphone, system jitter)
- ‚úÖ Feature extraction pipeline (stats + FFT + entropy + autocorrelation)
- ‚úÖ Siamese embedding model architecture
- ‚úÖ FastAPI backend with enrollment/authentication endpoints
- ‚úÖ React frontend for device management
- ‚úÖ Challenge-response protocol foundation
- ‚úÖ Embeddings stored (not raw noise)

### ‚ùå Critical gaps (blockers for production)

1. **No trained model** - Current system runs with random weights
2. **No encryption at rest** - Device embeddings stored as plaintext PyTorch tensors
3. **No secure channel** - HTTP-only API (no TLS/HTTPS)
4. **No authentication/authorization** - Anyone can enroll/delete devices
5. **In-memory storage only** - No persistent database for challenges/sessions
6. **Limited anti-replay protection** - Challenge protocol exists but not fully integrated
7. **No rate limiting** - Vulnerable to brute-force enrollment/authentication attempts
8. **No audit logging** - No forensic trail for security events
9. **No model security** - Model weights not protected from extraction/tampering

---

## Detailed improvements by category

### 1. Security & Cryptography (CRITICAL)

#### 1.1 Encryption at rest
**Current:** Embeddings saved as plaintext `.pt` files  
**Required:**
- Encrypt device embeddings using AES-256-GCM or ChaCha20-Poly1305
- Derive encryption keys from a hardware security module (HSM) or secure key management service
- Store initialization vectors (IVs) separately from ciphertext
- Implement key rotation without re-enrolling all devices

**Implementation:**
```python
# auth/encryption.py (new file)
from cryptography.hazmat.primitives.ciphers.aead import AESGCM
from cryptography.hazmat.backends import default_backend
import os

class EmbeddingEncryptor:
    def __init__(self, master_key: bytes):
        self.aesgcm = AESGCM(master_key)
    
    def encrypt_embedding(self, embedding_bytes: bytes) -> tuple[bytes, bytes]:
        nonce = os.urandom(12)
        ciphertext = self.aesgcm.encrypt(nonce, embedding_bytes, None)
        return nonce, ciphertext
    
    def decrypt_embedding(self, nonce: bytes, ciphertext: bytes) -> bytes:
        return self.aesgcm.decrypt(nonce, ciphertext, None)
```

#### 1.2 Secure communication (TLS/HTTPS)
**Current:** Backend runs on HTTP  
**Required:**
- Deploy behind nginx/Apache with TLS 1.3
- Use Let's Encrypt or enterprise CA for certificates
- Enforce HTTPS-only in production
- Implement certificate pinning in frontend for additional security

**Implementation:**
```bash
# nginx config
server {
    listen 443 ssl http2;
    ssl_certificate /etc/letsencrypt/live/yourdomain/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/yourdomain/privkey.pem;
    ssl_protocols TLSv1.3;
    
    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header X-Forwarded-Proto https;
    }
}
```

#### 1.3 API authentication & authorization
**Current:** Endpoints are completely open  
**Required:**
- Implement OAuth2/JWT for API authentication
- Role-based access control (RBAC): admin vs. device user
- API key management for programmatic access
- Device-specific access tokens (device can only authenticate itself)

**Implementation:**
```python
# server/auth.py (new file)
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from jose import JWTError, jwt

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

async def get_current_device(token: str = Depends(oauth2_scheme)):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        device_id: str = payload.get("sub")
        if device_id is None:
            raise HTTPException(status_code=401)
        return device_id
    except JWTError:
        raise HTTPException(status_code=401)

# Apply to endpoints:
@app.post("/authenticate")
async def authenticate(
    request: AuthRequest,
    current_device: str = Depends(get_current_device)
):
    if request.device_id != current_device:
        raise HTTPException(status_code=403, detail="Cannot authenticate other devices")
    # ... rest of logic
```

#### 1.4 Enhanced challenge-response protocol
**Current:** Basic nonce generation, not fully integrated  
**Required:**
- Implement mutual authentication (server also proves identity to device)
- Add timestamp validation to prevent time-shift attacks
- Cryptographically bind challenge to device identity using HMAC
- Store challenge history in persistent database (not in-memory)

**Implementation:**
```python
# auth/challenge_response.py (enhance existing)
class SecureChallengeProtocol:
    def create_challenge(self, device_id: str) -> dict:
        nonce = secrets.token_bytes(32)
        timestamp = int(time.time())
        
        # Bind challenge to device using HMAC
        challenge_data = f"{device_id}:{nonce.hex()}:{timestamp}".encode()
        signature = hmac.new(self.server_secret, challenge_data, hashlib.sha256).digest()
        
        challenge_id = hashlib.sha256(challenge_data + signature).hexdigest()
        
        # Store in persistent DB (Redis/PostgreSQL)
        self.db.store_challenge(challenge_id, {
            'device_id': device_id,
            'nonce': nonce.hex(),
            'timestamp': timestamp,
            'signature': signature.hex(),
            'expires_at': timestamp + 60
        })
        
        return {
            'challenge_id': challenge_id,
            'nonce': nonce.hex(),
            'server_signature': signature.hex(),
            'timestamp': timestamp
        }
```

---

### 2. Machine Learning & Model Security

#### 2.1 Train a production model
**Current:** Model uses random initialization  
**Required:**
- Collect diverse training data from 50+ real devices
- Train with triplet/contrastive loss for 50-100 epochs
- Perform cross-validation and hyperparameter tuning
- Achieve >95% verification accuracy and <1% FAR/FRR

**Training workflow:**
```bash
# 1. Collect training data
python collect_training_data.py --devices 50 --samples-per-device 100

# 2. Train model
python model/train.py \
    --data dataset/training_data \
    --epochs 100 \
    --batch-size 64 \
    --loss triplet \
    --margin 1.0 \
    --lr 0.001

# 3. Evaluate
python model/evaluate.py \
    --checkpoint model/checkpoints/best_model.pt \
    --test-data dataset/test_data
```

#### 2.2 Model security & integrity
**Current:** Model checkpoint is a regular file  
**Required:**
- Sign model weights with digital signature (prevent tampering)
- Verify signature on model load
- Encrypt model weights to prevent reverse engineering
- Implement model versioning and rollback capability

**Implementation:**
```python
# model/security.py (new file)
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric import padding
from cryptography.hazmat.primitives.serialization import load_pem_private_key

class ModelSigner:
    def sign_model(self, model_path: str, private_key_path: str):
        # Load model bytes
        with open(model_path, 'rb') as f:
            model_bytes = f.read()
        
        # Load private key
        with open(private_key_path, 'rb') as f:
            private_key = load_pem_private_key(f.read(), password=None)
        
        # Sign
        signature = private_key.sign(
            model_bytes,
            padding.PSS(mgf=padding.MGF1(hashes.SHA256()), salt_length=padding.PSS.MAX_LENGTH),
            hashes.SHA256()
        )
        
        # Save signature
        with open(f"{model_path}.sig", 'wb') as f:
            f.write(signature)
    
    def verify_model(self, model_path: str, public_key_path: str) -> bool:
        # Implementation similar to above but using public_key.verify()
        pass
```

#### 2.3 Adaptive threshold calibration
**Current:** Fixed threshold (0.85)  
**Required:**
- Per-device threshold calibration based on enrollment quality
- Adaptive threshold adjustment based on authentication history
- Confidence scoring (not just binary accept/reject)

**Implementation:**
```python
# auth/adaptive_threshold.py (new file)
class AdaptiveThreshold:
    def calibrate_for_device(self, device_id: str, enrollment_samples: list):
        # Compute intra-device similarity distribution
        similarities = []
        for i in range(len(enrollment_samples)):
            for j in range(i+1, len(enrollment_samples)):
                sim = compute_similarity(enrollment_samples[i], enrollment_samples[j])
                similarities.append(sim)
        
        # Set threshold at mean - 2*std (allow 95% of legitimate samples)
        mean_sim = np.mean(similarities)
        std_sim = np.std(similarities)
        threshold = mean_sim - 2 * std_sim
        
        # Clamp to reasonable range
        return max(0.7, min(0.95, threshold))
```

---

### 3. Anti-Replay & Anti-Spoofing

#### 3.1 Enhanced replay protection
**Current:** Basic challenge expiry  
**Required:**
- Persistent challenge history (prevent reuse across server restarts)
- Device-specific challenge counters (monotonic, prevent rollback)
- Bind challenges to client IP/session fingerprint

**Implementation:**
```python
# auth/anti_replay.py (new file)
class EnhancedAntiReplay:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def check_and_record_challenge(self, challenge_id: str, device_id: str) -> bool:
        # Check if challenge already used
        key = f"challenge_used:{challenge_id}"
        if self.redis.exists(key):
            logger.warning(f"Replay attack detected: {challenge_id}")
            return False
        
        # Mark as used (with 1-hour TTL)
        self.redis.setex(key, 3600, "1")
        
        # Increment device counter
        counter_key = f"device_counter:{device_id}"
        self.redis.incr(counter_key)
        
        return True
```

#### 3.2 Liveness detection
**Current:** No liveness checks  
**Required:**
- For camera: require active user interaction (e.g., movement detection)
- For microphone: analyze temporal patterns (synthetic audio detection)
- Time-based freshness validation (reject stale samples)

**Implementation:**
```python
# noise_collection/liveness.py (new file)
class LivenessDetector:
    def detect_camera_liveness(self, frames: list) -> bool:
        # Compute frame-to-frame differences
        diffs = []
        for i in range(len(frames)-1):
            diff = np.abs(frames[i+1] - frames[i]).sum()
            diffs.append(diff)
        
        # Synthetic/replayed video will have low variance
        variance = np.var(diffs)
        return variance > LIVENESS_THRESHOLD
    
    def detect_microphone_liveness(self, audio: np.ndarray) -> bool:
        # Analyze spectral characteristics of genuine noise
        # vs. replayed/synthetic audio
        pass
```

#### 3.3 Synthetic noise detection
**Current:** No defense against AI-generated noise  
**Required:**
- Train a discriminator network to detect synthetic noise samples
- Analyze statistical properties that differ between real and synthetic noise
- Implement ensemble detection (multiple checks)

---

### 4. Database & Persistence

#### 4.1 Replace in-memory storage
**Current:** Challenges and sessions stored in Python dicts  
**Required:**
- PostgreSQL for device metadata, audit logs, user accounts
- Redis for active challenges, rate limiting, session management
- Proper transaction handling and ACID guarantees

**Schema:**
```sql
-- devices table
CREATE TABLE devices (
    device_id VARCHAR(64) PRIMARY KEY,
    device_name VARCHAR(255),
    enrollment_date TIMESTAMP NOT NULL,
    last_auth_date TIMESTAMP,
    auth_count INTEGER DEFAULT 0,
    status VARCHAR(32) DEFAULT 'active',
    metadata JSONB
);

-- challenges table
CREATE TABLE challenges (
    challenge_id VARCHAR(64) PRIMARY KEY,
    device_id VARCHAR(64) REFERENCES devices(device_id),
    nonce VARCHAR(128) NOT NULL,
    created_at TIMESTAMP NOT NULL,
    expires_at TIMESTAMP NOT NULL,
    used BOOLEAN DEFAULT FALSE,
    used_at TIMESTAMP
);

-- audit_log table
CREATE TABLE audit_log (
    id SERIAL PRIMARY KEY,
    timestamp TIMESTAMP NOT NULL,
    event_type VARCHAR(64) NOT NULL,
    device_id VARCHAR(64),
    ip_address INET,
    user_agent TEXT,
    result VARCHAR(32),
    details JSONB
);
```

#### 4.2 Implement proper database layer
```python
# server/database.py (new file)
from sqlalchemy import create_engine, Column, String, Integer, DateTime, Boolean
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

Base = declarative_base()

class Device(Base):
    __tablename__ = 'devices'
    device_id = Column(String(64), primary_key=True)
    device_name = Column(String(255))
    enrollment_date = Column(DateTime, nullable=False)
    # ... rest of fields

class Challenge(Base):
    __tablename__ = 'challenges'
    challenge_id = Column(String(64), primary_key=True)
    # ... rest of fields

# Connection management
engine = create_engine('postgresql://user:pass@localhost/qnaauth')
SessionLocal = sessionmaker(bind=engine)
```

---

### 5. Performance & Scalability

#### 5.1 Model optimization
**Current:** Full PyTorch model inference (slow on CPU)  
**Required:**
- Convert to ONNX for faster inference
- Quantize model (INT8) for edge deployment
- Batch authentication requests for throughput

**Implementation:**
```python
# model/optimize.py (new file)
import torch
import onnx
import onnxruntime as ort

def convert_to_onnx(model_path: str, output_path: str):
    model = torch.load(model_path)
    model.eval()
    
    dummy_input = torch.randn(1, input_dim)
    torch.onnx.export(
        model,
        dummy_input,
        output_path,
        export_params=True,
        opset_version=13,
        input_names=['input'],
        output_names=['output']
    )

# Use ONNX Runtime for inference
class ONNXEmbedder:
    def __init__(self, model_path: str):
        self.session = ort.InferenceSession(model_path)
    
    def embed(self, features):
        return self.session.run(None, {'input': features})[0]
```

#### 5.2 Caching layer
**Current:** No caching  
**Required:**
- Cache device embeddings in Redis for fast lookup
- Cache feature extraction results for repeated samples
- Implement TTL and invalidation strategies

#### 5.3 Async processing
**Current:** Synchronous enrollment (blocks for minutes)  
**Required:**
- Background task queue (Celery/RQ) for enrollment
- Websocket progress updates to frontend
- Parallel noise collection from multiple sources

---

### 6. Hardware & IoT Considerations

#### 6.1 Lightweight model variants
**Current:** Single model size  
**Required:**
- Model quantization for edge devices (TensorFlow Lite, ONNX Mobile)
- Pruned models for constrained environments
- Feature subset selection based on available hardware

#### 6.2 Fallback strategies
**Current:** Requires all selected sources to work  
**Required:**
- Graceful degradation (authenticate with fewer sources)
- Source prioritization (QRNG > camera > mic > sensors)
- Adaptive sampling (collect more if initial confidence is low)

#### 6.3 Power efficiency
**Required:**
- Lazy loading of ML models
- Feature computation on-device (reduce bandwidth)
- Configurable authentication frequency vs. security tradeoff

---

### 7. Monitoring & Observability

#### 7.1 Comprehensive logging
**Current:** Basic `logger.info()` statements  
**Required:**
- Structured logging (JSON format)
- Centralized log aggregation (ELK stack, Splunk)
- Security event monitoring (failed auth attempts, suspicious patterns)

**Implementation:**
```python
# server/logging_config.py (new file)
import logging
import json
from datetime import datetime

class StructuredLogger:
    def log_event(self, event_type: str, device_id: str = None, **kwargs):
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'event_type': event_type,
            'device_id': device_id,
            **kwargs
        }
        logging.info(json.dumps(log_entry))

# Usage:
logger.log_event('enrollment_started', device_id='abc123', sources=['qrng', 'camera'])
logger.log_event('authentication_failed', device_id='abc123', similarity=0.72, threshold=0.85)
```

#### 7.2 Metrics & alerting
**Required:**
- Prometheus metrics export
- Grafana dashboards
- Alert on: high failure rates, unusual enrollment patterns, model degradation

**Metrics to track:**
```python
# server/metrics.py (new file)
from prometheus_client import Counter, Histogram, Gauge

enrollment_counter = Counter('qna_enrollments_total', 'Total enrollments', ['status'])
auth_counter = Counter('qna_authentications_total', 'Total auth attempts', ['result'])
similarity_histogram = Histogram('qna_similarity_scores', 'Similarity score distribution')
active_devices_gauge = Gauge('qna_active_devices', 'Number of enrolled devices')
```

---

### 8. Testing & Quality Assurance

#### 8.1 Unit tests
**Current:** No automated tests  
**Required:**
- Unit tests for all core modules (pytest)
- Mock noise sources for deterministic testing
- Test edge cases (empty samples, corrupted data)

**Test structure:**
```
tests/
  test_noise_collection.py
  test_preprocessing.py
  test_model.py
  test_auth.py
  test_challenge_response.py
  test_api.py
```

#### 8.2 Integration tests
**Required:**
- End-to-end enrollment + authentication flows
- Challenge-response protocol testing
- Multi-device scenarios
- Load testing (concurrent authentication requests)

#### 8.3 Security testing
**Required:**
- Penetration testing
- Fuzzing of API endpoints
- Replay attack simulation
- Model extraction attempts

---

### 9. Documentation & Deployment

#### 9.1 Enhanced documentation
**Current:** README + QUICKSTART  
**Required:**
- API documentation (OpenAPI/Swagger)
- Security whitepaper
- Deployment guide (Docker, Kubernetes)
- Compliance documentation (GDPR, SOC2)

#### 9.2 Docker & container orchestration
**Required:**
```yaml
# docker-compose.yml
version: '3.8'
services:
  backend:
    build: .
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/qnaauth
      - REDIS_URL=redis://redis:6379
    depends_on:
      - db
      - redis
  
  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
  
  db:
    image: postgres:15
    volumes:
      - postgres_data:/var/lib/postgresql/data
  
  redis:
    image: redis:7-alpine
```

#### 9.3 CI/CD pipeline
**Required:**
```yaml
# .github/workflows/test-and-deploy.yml
name: Test and Deploy
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run tests
        run: pytest tests/
      - name: Security scan
        run: bandit -r .
  
  deploy:
    needs: test
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to production
        run: # deployment script
```

---

## Prioritized roadmap (research-focused)

### Phase 1: Core authentication validation (3-4 weeks) ‚≠ê‚≠ê‚≠ê
**Goal**: Prove the fundamental hypothesis
1. ‚úÖ Collect data from 50+ diverse devices (phones, laptops, IoT)
2. ‚úÖ Implement comprehensive feature extraction (beyond basic stats)
3. ‚úÖ Train multiple model architectures (compare triplet vs. contrastive)
4. ‚úÖ Measure intra-device vs. inter-device similarity distributions
5. ‚úÖ Achieve >95% accuracy with <1% FAR/FRR

**Deliverable**: Evidence that quantum noise can reliably fingerprint devices

### Phase 2: Security & attack resistance (2-3 weeks) ‚≠ê‚≠ê‚≠ê
**Goal**: Demonstrate robustness against realistic attacks
1. ‚úÖ Implement replay attack simulation (reuse old samples)
2. ‚úÖ Implement synthesis attack (GAN-generated noise)
3. ‚úÖ Implement cloning attack (identical hardware)
4. ‚úÖ Enhanced challenge-response protocol with cryptographic binding
5. ‚úÖ Model inversion resistance analysis

**Deliverable**: Security evaluation showing >99.9% attack failure rate

### Phase 3: Longitudinal stability study (6-12 weeks, parallel) ‚≠ê‚≠ê
**Goal**: Prove stability over time
1. ‚úÖ Daily re-authentication for 90 days
2. ‚úÖ Measure similarity drift and identify factors (temperature, battery, etc.)
3. ‚úÖ Develop drift compensation strategies
4. ‚úÖ Test re-enrollment triggers

**Deliverable**: Evidence that device signatures remain stable

### Phase 4: Paper writing & publication (4-6 weeks) ‚≠ê‚≠ê‚≠ê
**Goal**: Document findings for academic community
1. ‚úÖ Write paper (IEEE S&P, CCS, NDSS, or USENIX Security format)
2. ‚úÖ Create figures (ROC curves, t-SNE embeddings, attack results)
3. ‚úÖ Prepare artifact (code + data for reproducibility)
4. ‚úÖ Submit to top-tier venue

**Deliverable**: Accepted paper at major security conference

### Phase 5: Reference implementation polish (2-3 weeks) ‚≠ê
**Goal**: Make it usable by other researchers
1. ‚úÖ Clean up code and add documentation
2. ‚úÖ Create Jupyter notebooks for demos
3. ‚úÖ Package as pip-installable library
4. ‚úÖ Minimal web demo (optional, for presentations)
5. ‚úÖ Docker for reproducibility

**Deliverable**: Open-source reference implementation on GitHub

### Phase 6: Production hardening (only if commercializing) ‚≠ê
**Note**: Skip this phase if goal is purely research
1. ‚è≥ Encryption at rest for embeddings
2. ‚è≥ TLS/HTTPS deployment
3. ‚è≥ PostgreSQL + Redis integration
4. ‚è≥ Monitoring & alerting
5. ‚è≥ Security audit & penetration testing

---

## Metrics for success

### Security metrics
- **Zero successful replay attacks** in penetration testing
- **FAR (False Acceptance Rate) < 0.01%**
- **FRR (False Rejection Rate) < 1%**
- **Model extraction resistance** (confirmed by red team)

### Performance metrics
- **Enrollment time < 60 seconds**
- **Authentication latency < 2 seconds**
- **Support 10,000+ concurrent authentications**
- **Model inference < 50ms on CPU**

### Reliability metrics
- **99.9% uptime**
- **Zero data loss** (encrypted backups)
- **Automatic failover** for high availability

---

## Conclusion

The current implementation provides a **solid proof-of-concept** but needs focus on the **core research contribution** rather than production web development.

### For a research project (recommended focus)
The most critical gaps are:
1. **Trained model**: Need real data from 50+ devices to prove the hypothesis
2. **Statistical validation**: Measure and document distinguishability rigorously
3. **Security analysis**: Demonstrate attack resistance with formal threat model
4. **Academic paper**: Document methodology and results for peer review

**Estimated effort**: 10-15 weeks with 1-2 researchers  
**Next action**: Start Phase 1 (data collection and model training)

### For a production system (only if commercializing)
Additional requirements:
1. **Security hardening**: Encryption, TLS, authentication, audit logging
2. **Infrastructure**: PostgreSQL, Redis, monitoring, high availability
3. **Compliance**: GDPR, SOC2, penetration testing
4. **Scalability**: Load balancing, horizontal scaling, CDN

**Estimated effort**: Additional 10-14 weeks with 2-3 engineers  
**Next action**: Complete research validation first, then implement Phase 6

---

## Recommended focus

**If this is a capstone/thesis project**: Focus on Phases 1-4 (research validation + paper)  
**If this is a startup/product**: Complete Phases 1-4 first to prove the concept, then Phase 6 for production

**The web UI (React/FastAPI) should remain minimal** - it's just a demo harness for the core authentication primitive. Jupyter notebooks are sufficient for research visualization.
</file>

<file path="model/__init__.py">
"""
Model Module Initialization
"""

from .siamese_model import (
    SiameseNetwork,
    EmbeddingNetwork,
    TripletLoss,
    ContrastiveLoss,
    DeviceEmbedder
)
from .train import (
    TripletDataset,
    PairDataset,
    ModelTrainer
)
from .evaluate import ModelEvaluator

__all__ = [
    'SiameseNetwork',
    'EmbeddingNetwork',
    'TripletLoss',
    'ContrastiveLoss',
    'DeviceEmbedder',
    'TripletDataset',
    'PairDataset',
    'ModelTrainer',
    'ModelEvaluator'
]
</file>

<file path="model/evaluate.py">
"""
Model Evaluation and Testing
"""

import torch
import numpy as np
from sklearn.metrics import roc_curve, auc, precision_recall_curve
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
from pathlib import Path
import logging

from .siamese_model import DeviceEmbedder

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelEvaluator:
    """Evaluates trained model performance"""
    
    def __init__(self, embedder: DeviceEmbedder):
        """
        Initialize evaluator
        
        Args:
            embedder: Trained DeviceEmbedder instance
        """
        self.embedder = embedder
    
    def compute_embeddings(
        self,
        features_by_device: Dict[str, List[np.ndarray]]
    ) -> Dict[str, List[torch.Tensor]]:
        """
        Compute embeddings for all samples
        
        Args:
            features_by_device: Dictionary mapping device_id to feature arrays
            
        Returns:
            Dictionary mapping device_id to embedding tensors
        """
        embeddings_by_device = {}
        
        for device_id, features_list in features_by_device.items():
            embeddings = []
            for features in features_list:
                features_tensor = torch.from_numpy(features).float()
                embedding = self.embedder.embed(features_tensor)
                embeddings.append(embedding)
            
            embeddings_by_device[device_id] = embeddings
            logger.info(f"Computed {len(embeddings)} embeddings for {device_id}")
        
        return embeddings_by_device
    
    def compute_similarity_scores(
        self,
        embeddings_by_device: Dict[str, List[torch.Tensor]],
        metric: str = 'cosine'
    ) -> Tuple[List[float], List[int]]:
        """
        Compute similarity scores and labels for all pairs
        
        Args:
            embeddings_by_device: Dictionary of embeddings
            metric: Similarity metric
            
        Returns:
            Tuple of (scores, labels) where label=1 for same device
        """
        scores = []
        labels = []
        
        device_ids = list(embeddings_by_device.keys())
        
        # Positive pairs (same device)
        for device_id, embeddings in embeddings_by_device.items():
            for i in range(len(embeddings)):
                for j in range(i + 1, len(embeddings)):
                    score = self.embedder.compute_similarity(
                        embeddings[i],
                        embeddings[j],
                        metric=metric
                    )
                    scores.append(score)
                    labels.append(1)
        
        # Negative pairs (different devices)
        for i, device1 in enumerate(device_ids):
            for device2 in device_ids[i+1:]:
                embeddings1 = embeddings_by_device[device1]
                embeddings2 = embeddings_by_device[device2]
                
                # Sample a subset of negative pairs
                num_samples = min(10, len(embeddings1), len(embeddings2))
                for _ in range(num_samples):
                    idx1 = np.random.randint(len(embeddings1))
                    idx2 = np.random.randint(len(embeddings2))
                    
                    score = self.embedder.compute_similarity(
                        embeddings1[idx1],
                        embeddings2[idx2],
                        metric=metric
                    )
                    scores.append(score)
                    labels.append(0)
        
        logger.info(f"Computed {len(scores)} similarity scores")
        logger.info(f"Positive pairs: {sum(labels)}, Negative pairs: {len(labels) - sum(labels)}")
        
        return scores, labels
    
    def evaluate_threshold(
        self,
        scores: List[float],
        labels: List[int],
        threshold: float
    ) -> Dict[str, float]:
        """
        Evaluate model at specific threshold
        
        Args:
            scores: Similarity scores
            labels: Ground truth labels
            threshold: Decision threshold
            
        Returns:
            Dictionary of metrics
        """
        predictions = [1 if s >= threshold else 0 for s in scores]
        
        tp = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 1)
        tn = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 0)
        fp = sum(1 for p, l in zip(predictions, labels) if p == 1 and l == 0)
        fn = sum(1 for p, l in zip(predictions, labels) if p == 0 and l == 1)
        
        accuracy = (tp + tn) / len(labels) if len(labels) > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        far = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Acceptance Rate
        frr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Rejection Rate
        
        metrics = {
            'threshold': threshold,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'far': far,
            'frr': frr,
            'tp': tp,
            'tn': tn,
            'fp': fp,
            'fn': fn
        }
        
        return metrics
    
    def find_optimal_threshold(
        self,
        scores: List[float],
        labels: List[int],
        metric: str = 'f1_score'
    ) -> Tuple[float, Dict[str, float]]:
        """
        Find optimal threshold based on metric
        
        Args:
            scores: Similarity scores
            labels: Ground truth labels
            metric: Metric to optimize ('f1_score', 'accuracy', etc.)
            
        Returns:
            Tuple of (optimal_threshold, metrics_at_threshold)
        """
        thresholds = np.linspace(min(scores), max(scores), 100)
        best_threshold = 0.0
        best_score = 0.0
        best_metrics = {}
        
        for threshold in thresholds:
            metrics = self.evaluate_threshold(scores, labels, threshold)
            
            if metrics[metric] > best_score:
                best_score = metrics[metric]
                best_threshold = threshold
                best_metrics = metrics
        
        logger.info(f"Optimal threshold: {best_threshold:.4f} ({metric}={best_score:.4f})")
        
        return best_threshold, best_metrics
    
    def plot_roc_curve(
        self,
        scores: List[float],
        labels: List[int],
        save_path: str = None
    ):
        """
        Plot ROC curve
        
        Args:
            scores: Similarity scores
            labels: Ground truth labels
            save_path: Path to save plot
        """
        fpr, tpr, thresholds = roc_curve(labels, scores)
        roc_auc = auc(fpr, tpr)
        
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2,
                label=f'ROC curve (AUC = {roc_auc:.3f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate (FAR)')
        plt.ylabel('True Positive Rate (1 - FRR)')
        plt.title('Receiver Operating Characteristic (ROC) Curve')
        plt.legend(loc="lower right")
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"ROC curve saved to {save_path}")
        
        plt.close()
    
    def plot_precision_recall_curve(
        self,
        scores: List[float],
        labels: List[int],
        save_path: str = None
    ):
        """
        Plot precision-recall curve
        
        Args:
            scores: Similarity scores
            labels: Ground truth labels
            save_path: Path to save plot
        """
        precision, recall, thresholds = precision_recall_curve(labels, scores)
        
        plt.figure(figsize=(8, 6))
        plt.plot(recall, precision, color='blue', lw=2)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve')
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Precision-recall curve saved to {save_path}")
        
        plt.close()
    
    def generate_report(
        self,
        features_by_device: Dict[str, List[np.ndarray]],
        save_dir: str = "./model/evaluation"
    ) -> Dict[str, any]:
        """
        Generate comprehensive evaluation report
        
        Args:
            features_by_device: Dictionary of features
            save_dir: Directory to save results
            
        Returns:
            Evaluation report dictionary
        """
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # Compute embeddings
        logger.info("Computing embeddings...")
        embeddings_by_device = self.compute_embeddings(features_by_device)
        
        # Compute similarity scores
        logger.info("Computing similarity scores...")
        scores, labels = self.compute_similarity_scores(embeddings_by_device)
        
        # Find optimal threshold
        logger.info("Finding optimal threshold...")
        optimal_threshold, optimal_metrics = self.find_optimal_threshold(
            scores, labels, metric='f1_score'
        )
        
        # Plot curves
        logger.info("Plotting ROC curve...")
        self.plot_roc_curve(scores, labels, save_path / "roc_curve.png")
        
        logger.info("Plotting precision-recall curve...")
        self.plot_precision_recall_curve(scores, labels, save_path / "pr_curve.png")
        
        # Create report
        report = {
            'num_devices': len(features_by_device),
            'total_samples': sum(len(f) for f in features_by_device.values()),
            'optimal_threshold': optimal_threshold,
            'metrics': optimal_metrics
        }
        
        logger.info("\n=== Evaluation Report ===")
        logger.info(f"Number of devices: {report['num_devices']}")
        logger.info(f"Total samples: {report['total_samples']}")
        logger.info(f"Optimal threshold: {optimal_threshold:.4f}")
        logger.info(f"Accuracy: {optimal_metrics['accuracy']:.4f}")
        logger.info(f"Precision: {optimal_metrics['precision']:.4f}")
        logger.info(f"Recall: {optimal_metrics['recall']:.4f}")
        logger.info(f"F1 Score: {optimal_metrics['f1_score']:.4f}")
        logger.info(f"FAR: {optimal_metrics['far']:.4f}")
        logger.info(f"FRR: {optimal_metrics['frr']:.4f}")
        
        return report


def main():
    """Test evaluation pipeline"""
    print("\n=== Evaluation Pipeline Test ===")
    
    # Create synthetic data
    input_dim = 50
    embedding_dim = 128
    num_devices = 3
    samples_per_device = 20
    
    # Generate synthetic features
    features_by_device = {}
    for i in range(num_devices):
        device_id = f"device_{i:03d}"
        features = [np.random.randn(input_dim) + i * 1.0 
                   for _ in range(samples_per_device)]
        features_by_device[device_id] = features
    
    # Create embedder (with random model)
    embedder = DeviceEmbedder(input_dim=input_dim, embedding_dim=embedding_dim)
    
    # Create evaluator
    evaluator = ModelEvaluator(embedder)
    
    # Generate report
    report = evaluator.generate_report(features_by_device)
    
    print("\n=== Report ===")
    print(report)


if __name__ == "__main__":
    main()
</file>

<file path="noise_collection/__init__.py">
"""
Noise Collection Module Initialization
"""

from .qrng_api import QRNGClient
from .camera_noise import CameraNoiseCollector
from .mic_noise import MicrophoneNoiseCollector
from .sensor_noise import SensorNoiseCollector

__all__ = [
    'QRNGClient',
    'CameraNoiseCollector',
    'MicrophoneNoiseCollector',
    'SensorNoiseCollector'
]
</file>

<file path="noise_collection/camera_noise.py">
"""
Camera Dark Frame Noise Collection
Captures sensor noise from camera with lens cap on or in dark environment
"""

import cv2
import numpy as np
from typing import Optional, Tuple
import logging
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CameraNoiseCollector:
    """Collects noise from camera sensor (dark frames)"""
    
    def __init__(self, camera_index: int = 0):
        """
        Initialize camera noise collector
        
        Args:
            camera_index: Index of camera to use (0 for default)
        """
        self.camera_index = camera_index
        self.cap = None
        
    def initialize_camera(self) -> bool:
        """
        Initialize camera connection
        
        Returns:
            True if successful, False otherwise
        """
        try:
            self.cap = cv2.VideoCapture(self.camera_index)
            
            if not self.cap.isOpened():
                logger.error(f"Failed to open camera {self.camera_index}")
                return False
            
            # Set camera properties for better noise capture
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            
            # Warm up camera (discard first few frames)
            for _ in range(5):
                self.cap.read()
                
            logger.info(f"Camera {self.camera_index} initialized successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize camera: {e}")
            return False
    
    def capture_dark_frame(
        self, 
        exposure_time: float = 0.1,
        grayscale: bool = True
    ) -> Optional[np.ndarray]:
        """
        Capture a single dark frame (noise sample)
        
        Args:
            exposure_time: Time to wait before capture (simulates exposure)
            grayscale: Convert to grayscale
            
        Returns:
            Numpy array of captured noise or None if failed
        """
        if self.cap is None or not self.cap.isOpened():
            if not self.initialize_camera():
                return None
        
        try:
            # Wait for exposure time
            time.sleep(exposure_time)
            
            # Capture frame
            ret, frame = self.cap.read()
            
            if not ret or frame is None:
                logger.error("Failed to capture frame")
                return None
            
            # Convert to grayscale if requested
            if grayscale and len(frame.shape) == 3:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            logger.info(f"Captured dark frame: shape {frame.shape}")
            return frame
            
        except Exception as e:
            logger.error(f"Error capturing dark frame: {e}")
            return None
    
    def capture_multiple_frames(
        self, 
        num_frames: int = 10,
        exposure_time: float = 0.1
    ) -> list:
        """
        Capture multiple dark frames
        
        Args:
            num_frames: Number of frames to capture
            exposure_time: Exposure time per frame
            
        Returns:
            List of captured frames
        """
        frames = []
        
        for i in range(num_frames):
            frame = self.capture_dark_frame(exposure_time=exposure_time)
            
            if frame is not None:
                frames.append(frame)
                logger.info(f"Captured frame {i+1}/{num_frames}")
            else:
                logger.warning(f"Failed to capture frame {i+1}")
        
        return frames
    
    def extract_noise_features(self, frame: np.ndarray) -> np.ndarray:
        """
        Extract noise characteristics from dark frame
        
        Args:
            frame: Dark frame image
            
        Returns:
            1D array of noise features
        """
        # Flatten frame to 1D array
        flat = frame.flatten().astype(np.float32)
        
        # Apply high-pass filter to isolate noise
        # Subtract local mean to remove low-frequency components
        kernel_size = 5
        blurred = cv2.GaussianBlur(
            frame, 
            (kernel_size, kernel_size), 
            0
        )
        noise = frame.astype(np.float32) - blurred.astype(np.float32)
        
        return noise.flatten()
    
    def get_temporal_noise(
        self, 
        num_frames: int = 10,
        exposure_time: float = 0.05
    ) -> np.ndarray:
        """
        Compute temporal noise by analyzing frame-to-frame variations
        
        Args:
            num_frames: Number of frames to analyze
            exposure_time: Exposure time per frame
            
        Returns:
            Temporal noise array
        """
        frames = self.capture_multiple_frames(num_frames, exposure_time)
        
        if len(frames) < 2:
            logger.error("Not enough frames for temporal analysis")
            return np.array([])
        
        # Stack frames
        stack = np.stack(frames, axis=0)
        
        # Compute temporal standard deviation (pixel-wise)
        temporal_std = np.std(stack, axis=0)
        
        logger.info(f"Temporal noise shape: {temporal_std.shape}")
        return temporal_std.flatten()
    
    def release(self):
        """Release camera resources"""
        if self.cap is not None:
            self.cap.release()
            logger.info("Camera released")


def main():
    """Test camera noise collection"""
    collector = CameraNoiseCollector(camera_index=0)
    
    print("\n=== Camera Noise Collection Test ===")
    print("Note: Cover camera lens or ensure dark environment for best results")
    
    # Initialize camera
    if not collector.initialize_camera():
        print("Failed to initialize camera")
        return
    
    # Capture single dark frame
    print("\n=== Capturing Single Dark Frame ===")
    frame = collector.capture_dark_frame(exposure_time=0.2)
    
    if frame is not None:
        print(f"Frame shape: {frame.shape}")
        print(f"Frame mean: {frame.mean():.2f}")
        print(f"Frame std: {frame.std():.2f}")
        print(f"Frame range: [{frame.min()}, {frame.max()}]")
        
        # Extract noise features
        noise = collector.extract_noise_features(frame)
        print(f"\nNoise features: {len(noise)} values")
        print(f"Noise mean: {noise.mean():.4f}")
        print(f"Noise std: {noise.std():.4f}")
    
    # Capture temporal noise
    print("\n=== Capturing Temporal Noise ===")
    temporal = collector.get_temporal_noise(num_frames=5)
    
    if len(temporal) > 0:
        print(f"Temporal noise: {len(temporal)} values")
        print(f"Temporal mean: {temporal.mean():.4f}")
        print(f"Temporal std: {temporal.std():.4f}")
    
    # Clean up
    collector.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()
</file>

<file path="noise_collection/qrng_api.py">
"""
Quantum Random Number Generator API Client
Fetches quantum noise from ANU QRNG service
"""

import requests
import numpy as np
from typing import List, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QRNGClient:
    """Client for fetching quantum random numbers from QRNG API"""
    
    # QRNG API endpoints
    ANU_API_URL = "https://qrng.anu.edu.au/API/jsonI.php"
    QRNG_ORG_URL = "https://api.qrng.org/api/v1.0/random"
    
    def __init__(self, api_url: Optional[str] = None, api_key: Optional[str] = None):
        """
        Initialize QRNG client
        
        Args:
            api_url: Custom API URL (defaults to QRNG.org if key provided, else ANU)
            api_key: API key for authenticated QRNG service
        """
        self.api_key = api_key
        
        # Use QRNG.org if API key provided, otherwise ANU
        if api_key and not api_url:
            self.api_url = self.QRNG_ORG_URL
        else:
            self.api_url = api_url or self.ANU_API_URL
        
        # Setup headers for authenticated requests
        self.headers = {}
        if self.api_key:
            self.headers['Authorization'] = f'Bearer {self.api_key}'
            logger.info("Initialized QRNG client with API key authentication")
        
    def fetch_quantum_noise(
        self, 
        length: int = 1024, 
        data_type: str = 'uint8'
    ) -> np.ndarray:
        """
        Fetch quantum random numbers from the API
        
        Args:
            length: Number of random values to fetch (max 1024 for ANU)
            data_type: Type of random numbers ('uint8', 'uint16', 'hex16')
            
        Returns:
            numpy array of quantum random numbers
        """
        try:
            # Different API formats for different services
            if self.api_key:
                # QRNG.org format
                params = {
                    'length': min(length, 1024),
                    'type': 'uint8'
                }
                logger.info(f"Fetching {length} quantum random numbers (authenticated)...")
                response = requests.get(self.api_url, params=params, headers=self.headers, timeout=30)
            else:
                # ANU QRNG format
                params = {
                    'length': min(length, 1024),
                    'type': data_type
                }
                logger.info(f"Fetching {length} quantum random numbers...")
                response = requests.get(self.api_url, params=params, timeout=10)
            
            response.raise_for_status()
            data = response.json()
            
            # Handle different response formats
            if self.api_key:
                # QRNG.org response format
                if 'data' in data:
                    quantum_data = np.array(data['data'], dtype=np.uint8)
                    logger.info(f"Successfully fetched {len(quantum_data)} quantum values")
                    return quantum_data
                else:
                    raise ValueError(f"Unexpected API response format: {data}")
            else:
                # ANU response format
                if data.get('success'):
                    quantum_data = np.array(data['data'], dtype=np.uint8)
                    logger.info(f"Successfully fetched {len(quantum_data)} quantum values")
                    return quantum_data
                else:
                    raise ValueError(f"API returned unsuccessful response: {data}")
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Failed to fetch quantum noise: {e}")
            raise
    
    def fetch_multiple_samples(
        self, 
        num_samples: int = 10, 
        sample_size: int = 1024
    ) -> List[np.ndarray]:
        """
        Fetch multiple quantum noise samples (NO FALLBACK - ensures authenticity)
        
        Args:
            num_samples: Number of separate samples to collect
            sample_size: Size of each sample
            
        Returns:
            List of numpy arrays containing quantum noise
        """
        samples = []
        
        for i in range(num_samples):
            try:
                sample = self.fetch_quantum_noise(length=sample_size)
                samples.append(sample)
                logger.info(f"Collected sample {i+1}/{num_samples}")
            except Exception as e:
                logger.error(f"Failed to collect sample {i+1}: {e}")
                # NO FALLBACK - skip failed samples to ensure authenticity
                continue
                
        return samples
    
    def get_quantum_entropy(self, data: np.ndarray) -> float:
        """
        Calculate Shannon entropy of quantum noise
        
        Args:
            data: Quantum noise array
            
        Returns:
            Entropy value
        """
        # Calculate probability distribution
        values, counts = np.unique(data, return_counts=True)
        probabilities = counts / len(data)
        
        # Calculate Shannon entropy
        entropy = -np.sum(probabilities * np.log2(probabilities))
        
        return entropy


def main():
    """Test the QRNG client"""
    client = QRNGClient()
    
    # Fetch a single sample
    print("\n=== Fetching Quantum Noise Sample ===")
    noise = client.fetch_quantum_noise(length=1024)
    print(f"Sample shape: {noise.shape}")
    print(f"Sample mean: {noise.mean():.2f}")
    print(f"Sample std: {noise.std():.2f}")
    print(f"Sample range: [{noise.min()}, {noise.max()}]")
    
    # Calculate entropy
    entropy = client.get_quantum_entropy(noise)
    print(f"Shannon entropy: {entropy:.4f} bits")
    
    # Fetch multiple samples
    print("\n=== Fetching Multiple Samples ===")
    samples = client.fetch_multiple_samples(num_samples=5, sample_size=512)
    print(f"Collected {len(samples)} samples")
    
    # Compare entropy across samples
    print("\n=== Entropy Comparison ===")
    for i, sample in enumerate(samples):
        ent = client.get_quantum_entropy(sample)
        print(f"Sample {i+1} entropy: {ent:.4f} bits")


if __name__ == "__main__":
    main()
</file>

<file path="noise_collection/sensor_noise.py">
"""
Sensor Jitter and System Noise Collection
Collects timing jitter and system-level noise sources
"""

import time
import numpy as np
from typing import List, Dict
import logging
import psutil
import platform

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SensorNoiseCollector:
    """Collects system-level noise and timing jitter"""
    
    def __init__(self):
        """Initialize sensor noise collector"""
        self.system_info = self._get_system_info()
        
    def _get_system_info(self) -> Dict[str, str]:
        """Get system information"""
        return {
            'platform': platform.system(),
            'processor': platform.processor(),
            'python_version': platform.python_version()
        }
    
    def collect_timing_jitter(
        self, 
        num_samples: int = 1000,
        target_interval: float = 0.001
    ) -> np.ndarray:
        """
        Collect high-resolution timing jitter
        
        Args:
            num_samples: Number of timing measurements
            target_interval: Target sleep interval in seconds
            
        Returns:
            Array of timing jitter values
        """
        jitter = []
        
        logger.info(f"Collecting {num_samples} timing measurements...")
        
        for _ in range(num_samples):
            start = time.perf_counter()
            time.sleep(target_interval)
            end = time.perf_counter()
            
            # Actual interval minus target interval = jitter
            actual_interval = end - start
            jitter_value = actual_interval - target_interval
            jitter.append(jitter_value)
        
        jitter_array = np.array(jitter)
        logger.info(f"Timing jitter: mean={jitter_array.mean():.6f}s, std={jitter_array.std():.6f}s")
        
        return jitter_array
    
    def collect_cpu_usage_noise(
        self, 
        num_samples: int = 100,
        interval: float = 0.01
    ) -> np.ndarray:
        """
        Collect CPU usage fluctuations as noise source
        
        Args:
            num_samples: Number of measurements
            interval: Sampling interval in seconds
            
        Returns:
            Array of CPU usage percentages
        """
        cpu_samples = []
        
        logger.info(f"Collecting {num_samples} CPU usage samples...")
        
        for _ in range(num_samples):
            cpu_percent = psutil.cpu_percent(interval=interval)
            cpu_samples.append(cpu_percent)
        
        cpu_array = np.array(cpu_samples)
        logger.info(f"CPU usage: mean={cpu_array.mean():.2f}%, std={cpu_array.std():.2f}%")
        
        return cpu_array
    
    def collect_memory_access_noise(
        self, 
        num_iterations: int = 10000
    ) -> np.ndarray:
        """
        Collect memory access timing variations
        
        Args:
            num_iterations: Number of memory operations
            
        Returns:
            Array of memory access times
        """
        timings = []
        data = np.random.rand(1000)
        
        logger.info(f"Collecting {num_iterations} memory access timings...")
        
        for _ in range(num_iterations):
            start = time.perf_counter()
            _ = data[np.random.randint(0, len(data))]
            end = time.perf_counter()
            timings.append(end - start)
        
        timing_array = np.array(timings)
        logger.info(f"Memory access: mean={timing_array.mean():.9f}s, std={timing_array.std():.9f}s")
        
        return timing_array
    
    def collect_disk_io_noise(self, num_operations: int = 100) -> np.ndarray:
        """
        Collect disk I/O timing variations
        
        Args:
            num_operations: Number of I/O operations
            
        Returns:
            Array of I/O operation times
        """
        timings = []
        
        logger.info(f"Collecting {num_operations} disk I/O timings...")
        
        # Get disk I/O counters before
        io_before = psutil.disk_io_counters()
        
        for i in range(num_operations):
            start = time.perf_counter()
            io_current = psutil.disk_io_counters()
            end = time.perf_counter()
            
            timings.append(end - start)
            time.sleep(0.01)  # Small delay between measurements
        
        timing_array = np.array(timings)
        logger.info(f"Disk I/O: mean={timing_array.mean():.6f}s, std={timing_array.std():.6f}s")
        
        return timing_array
    
    def collect_network_jitter(self, num_samples: int = 50) -> np.ndarray:
        """
        Collect network statistics as noise source
        
        Args:
            num_samples: Number of measurements
            
        Returns:
            Array of network byte count differences
        """
        network_samples = []
        
        logger.info(f"Collecting {num_samples} network statistics...")
        
        prev_bytes = psutil.net_io_counters().bytes_sent + psutil.net_io_counters().bytes_recv
        
        for _ in range(num_samples):
            time.sleep(0.05)
            current = psutil.net_io_counters()
            current_bytes = current.bytes_sent + current.bytes_recv
            diff = current_bytes - prev_bytes
            network_samples.append(diff)
            prev_bytes = current_bytes
        
        network_array = np.array(network_samples, dtype=np.float32)
        logger.info(f"Network jitter: mean={network_array.mean():.2f}, std={network_array.std():.2f}")
        
        return network_array
    
    def collect_combined_system_noise(self) -> Dict[str, np.ndarray]:
        """
        Collect all system noise sources
        
        Returns:
            Dictionary of noise arrays from different sources
        """
        logger.info("Collecting combined system noise...")
        
        noise_sources = {
            'timing_jitter': self.collect_timing_jitter(num_samples=500),
            'cpu_usage': self.collect_cpu_usage_noise(num_samples=50),
            'memory_access': self.collect_memory_access_noise(num_iterations=5000),
            'disk_io': self.collect_disk_io_noise(num_operations=50),
            'network': self.collect_network_jitter(num_samples=30)
        }
        
        return noise_sources
    
    def compute_composite_signature(self, noise_sources: Dict[str, np.ndarray]) -> np.ndarray:
        """
        Create a composite signature from multiple noise sources
        
        Args:
            noise_sources: Dictionary of noise arrays
            
        Returns:
            Combined noise signature
        """
        # Normalize each source to [0, 1]
        normalized = []
        
        for name, data in noise_sources.items():
            if len(data) > 0:
                norm = (data - data.min()) / (data.max() - data.min() + 1e-10)
                normalized.append(norm)
        
        # Concatenate all normalized sources
        composite = np.concatenate(normalized)
        
        logger.info(f"Composite signature length: {len(composite)}")
        
        return composite


def main():
    """Test sensor noise collection"""
    collector = SensorNoiseCollector()
    
    print("\n=== System Sensor Noise Collection Test ===")
    print(f"Platform: {collector.system_info['platform']}")
    print(f"Processor: {collector.system_info['processor']}")
    
    # Timing jitter
    print("\n=== Timing Jitter ===")
    jitter = collector.collect_timing_jitter(num_samples=500)
    print(f"Samples: {len(jitter)}")
    print(f"Mean jitter: {jitter.mean():.6f}s")
    print(f"Std jitter: {jitter.std():.6f}s")
    print(f"Range: [{jitter.min():.6f}, {jitter.max():.6f}]s")
    
    # CPU usage noise
    print("\n=== CPU Usage Noise ===")
    cpu = collector.collect_cpu_usage_noise(num_samples=50)
    print(f"Samples: {len(cpu)}")
    print(f"Mean: {cpu.mean():.2f}%")
    print(f"Std: {cpu.std():.2f}%")
    
    # Memory access
    print("\n=== Memory Access Timing ===")
    memory = collector.collect_memory_access_noise(num_iterations=1000)
    print(f"Samples: {len(memory)}")
    print(f"Mean: {memory.mean():.9f}s")
    print(f"Std: {memory.std():.9f}s")
    
    # Combined system noise
    print("\n=== Combined System Noise ===")
    all_noise = collector.collect_combined_system_noise()
    
    print("\nNoise sources collected:")
    for name, data in all_noise.items():
        print(f"  {name}: {len(data)} samples")
    
    # Composite signature
    print("\n=== Composite Signature ===")
    signature = collector.compute_composite_signature(all_noise)
    print(f"Signature length: {len(signature)}")
    print(f"Signature mean: {signature.mean():.6f}")
    print(f"Signature std: {signature.std():.6f}")


if __name__ == "__main__":
    main()
</file>

<file path="preprocessing/__init__.py">
"""
Preprocessing Module Initialization
"""

from .features import NoisePreprocessor, FeatureVector
from .utils import (
    sliding_window,
    augment_noise_sample,
    downsample_signal,
    pad_or_truncate,
    batch_process,
    compute_snr,
    normalize_batch,
    merge_noise_sources
)

__all__ = [
    'NoisePreprocessor',
    'FeatureVector',
    'sliding_window',
    'augment_noise_sample',
    'downsample_signal',
    'pad_or_truncate',
    'batch_process',
    'compute_snr',
    'normalize_batch',
    'merge_noise_sources'
]
</file>

<file path="preprocessing/features.py">
"""
Feature Extraction and Preprocessing Pipeline
Implements filtering, normalization, entropy, FFT, and statistical features
"""

import numpy as np
from scipy import signal, stats
from scipy.fft import rfft, rfftfreq
from typing import Dict, Any, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class NoisePreprocessor:
    """Preprocessing and feature extraction for noise data"""
    
    def __init__(self, normalize: bool = True):
        """
        Initialize preprocessor
        
        Args:
            normalize: Whether to normalize features
        """
        self.normalize = normalize
    
    def apply_bandpass_filter(
        self,
        data: np.ndarray,
        lowcut: float = 0.1,
        highcut: float = 0.9,
        fs: float = 1.0
    ) -> np.ndarray:
        """
        Apply Butterworth bandpass filter
        
        Args:
            data: Input signal
            lowcut: Low cutoff frequency (normalized)
            highcut: High cutoff frequency (normalized)
            fs: Sampling frequency
            
        Returns:
            Filtered signal
        """
        nyquist = 0.5 * fs
        low = lowcut / nyquist
        high = highcut / nyquist
        
        b, a = signal.butter(4, [low, high], btype='band')
        filtered = signal.filtfilt(b, a, data)
        
        return filtered
    
    def normalize_data(
        self,
        data: np.ndarray,
        method: str = 'standard'
    ) -> np.ndarray:
        """
        Normalize data
        
        Args:
            data: Input array
            method: Normalization method ('standard', 'minmax', 'robust')
            
        Returns:
            Normalized array
        """
        if method == 'standard':
            # Z-score normalization
            return (data - np.mean(data)) / (np.std(data) + 1e-10)
        
        elif method == 'minmax':
            # Min-max scaling to [0, 1]
            min_val = np.min(data)
            max_val = np.max(data)
            return (data - min_val) / (max_val - min_val + 1e-10)
        
        elif method == 'robust':
            # Robust scaling using median and IQR
            median = np.median(data)
            q75, q25 = np.percentile(data, [75, 25])
            iqr = q75 - q25
            return (data - median) / (iqr + 1e-10)
        
        else:
            raise ValueError(f"Unknown normalization method: {method}")
    
    def compute_statistical_features(self, data: np.ndarray) -> Dict[str, float]:
        """
        Compute statistical features
        
        Args:
            data: Input array
            
        Returns:
            Dictionary of statistical features
        """
        features = {
            # Basic statistics
            'mean': float(np.mean(data)),
            'std': float(np.std(data)),
            'variance': float(np.var(data)),
            'min': float(np.min(data)),
            'max': float(np.max(data)),
            'median': float(np.median(data)),
            'range': float(np.ptp(data)),
            
            # Distribution moments
            'skewness': float(stats.skew(data)),
            'kurtosis': float(stats.kurtosis(data)),
            
            # Quantiles
            'q25': float(np.percentile(data, 25)),
            'q75': float(np.percentile(data, 75)),
            'iqr': float(np.percentile(data, 75) - np.percentile(data, 25)),
            
            # RMS
            'rms': float(np.sqrt(np.mean(data**2))),
            
            # Peak factor
            'peak_factor': float(np.max(np.abs(data)) / (np.sqrt(np.mean(data**2)) + 1e-10))
        }
        
        return features
    
    def compute_entropy(self, data: np.ndarray, bins: int = 256) -> float:
        """
        Compute Shannon entropy
        
        Args:
            data: Input array
            bins: Number of histogram bins
            
        Returns:
            Entropy value in bits
        """
        hist, _ = np.histogram(data, bins=bins)
        hist = hist[hist > 0]  # Remove zero bins
        
        # Normalize to probabilities
        probs = hist / hist.sum()
        
        # Calculate Shannon entropy
        entropy = -np.sum(probs * np.log2(probs))
        
        return float(entropy)
    
    def compute_fft_features(
        self,
        data: np.ndarray,
        sample_rate: float = 1.0
    ) -> Dict[str, Any]:
        """
        Compute FFT-based frequency domain features
        
        Args:
            data: Input signal
            sample_rate: Sampling rate
            
        Returns:
            Dictionary of FFT features
        """
        # Compute FFT
        fft_vals = rfft(data)
        fft_mag = np.abs(fft_vals)
        fft_freq = rfftfreq(len(data), 1/sample_rate)
        
        # Power spectral density
        psd = fft_mag ** 2
        
        # Normalize PSD to probability distribution
        psd_norm = psd / (psd.sum() + 1e-10)
        
        # Spectral features
        features = {
            # Dominant frequency
            'dominant_freq': float(fft_freq[np.argmax(fft_mag)]),
            'dominant_magnitude': float(np.max(fft_mag)),
            
            # Spectral centroid (center of mass of spectrum)
            'spectral_centroid': float(np.sum(fft_freq * fft_mag) / (np.sum(fft_mag) + 1e-10)),
            
            # Spectral spread (standard deviation of spectrum)
            'spectral_spread': float(np.sqrt(np.sum(((fft_freq - np.sum(fft_freq * fft_mag) / 
                                     (np.sum(fft_mag) + 1e-10))**2) * fft_mag) / 
                                     (np.sum(fft_mag) + 1e-10))),
            
            # Spectral entropy
            'spectral_entropy': float(-np.sum(psd_norm[psd_norm > 0] * 
                                     np.log2(psd_norm[psd_norm > 0]))),
            
            # Spectral flatness (ratio of geometric mean to arithmetic mean)
            'spectral_flatness': float(stats.gmean(fft_mag + 1e-10) / 
                                      (np.mean(fft_mag) + 1e-10)),
            
            # Band power (energy in frequency bands)
            'low_freq_power': float(np.sum(psd[fft_freq < sample_rate * 0.25])),
            'mid_freq_power': float(np.sum(psd[(fft_freq >= sample_rate * 0.25) & 
                                               (fft_freq < sample_rate * 0.5)])),
            'high_freq_power': float(np.sum(psd[fft_freq >= sample_rate * 0.5]))
        }
        
        return features
    
    def compute_autocorrelation_features(
        self,
        data: np.ndarray,
        max_lag: int = 100
    ) -> Dict[str, float]:
        """
        Compute autocorrelation features
        
        Args:
            data: Input signal
            max_lag: Maximum lag for autocorrelation
            
        Returns:
            Dictionary of autocorrelation features
        """
        # Normalize data
        normalized = (data - np.mean(data)) / (np.std(data) + 1e-10)
        
        # Compute autocorrelation
        autocorr = np.correlate(normalized, normalized, mode='full')
        autocorr = autocorr[len(autocorr)//2:]  # Keep positive lags
        autocorr = autocorr / autocorr[0]  # Normalize
        
        # Trim to max_lag
        autocorr = autocorr[:min(max_lag, len(autocorr))]
        
        features = {
            # First zero crossing
            'first_zero_crossing': float(np.where(autocorr < 0)[0][0] 
                                        if np.any(autocorr < 0) else len(autocorr)),
            
            # Mean of first N lags
            'autocorr_mean_10': float(np.mean(autocorr[1:11])) if len(autocorr) > 10 else 0.0,
            'autocorr_mean_50': float(np.mean(autocorr[1:51])) if len(autocorr) > 50 else 0.0,
            
            # Decay rate (how fast autocorrelation decreases)
            'autocorr_decay': float(-np.polyfit(range(min(20, len(autocorr))), 
                                               autocorr[:min(20, len(autocorr))], 1)[0])
        }
        
        return features
    
    def compute_complexity_features(self, data: np.ndarray) -> Dict[str, float]:
        """
        Compute signal complexity features
        
        Args:
            data: Input signal
            
        Returns:
            Dictionary of complexity features
        """
        features = {}
        
        # Zero crossing rate
        zero_crossings = np.sum(np.diff(np.sign(data)) != 0)
        features['zero_crossing_rate'] = float(zero_crossings / len(data))
        
        # For large arrays, downsample before computing expensive features
        # Approximate entropy and Hurst exponent have O(n¬≤) complexity
        if len(data) > 1000:
            # Downsample to max 1000 points for complexity calculations
            step = len(data) // 1000
            data_downsampled = data[::step][:1000]
            logger.info(f"Downsampled {len(data)} samples to {len(data_downsampled)} for complexity features")
        else:
            data_downsampled = data
        
        # Approximate entropy (regularity measure)
        logger.info("Computing approximate entropy...")
        features['approx_entropy'] = self._approximate_entropy(data_downsampled)
        logger.info("‚úì Approximate entropy computed")
        
        # Hurst exponent (long-term memory)
        logger.info("Computing Hurst exponent...")
        features['hurst_exponent'] = self._hurst_exponent(data_downsampled)
        logger.info("‚úì Hurst exponent computed")
        
        return features
    
    def _approximate_entropy(self, data: np.ndarray, m: int = 2, r: float = 0.2) -> float:
        """
        Compute approximate entropy
        
        Args:
            data: Input signal
            m: Pattern length
            r: Tolerance (fraction of std)
            
        Returns:
            Approximate entropy value
        """
        def _maxdist(x_i, x_j):
            return max([abs(ua - va) for ua, va in zip(x_i, x_j)])
        
        def _phi(m_val):
            x = [[data[j] for j in range(i, i + m_val - 1 + 1)] for i in range(N - m_val + 1)]
            C = []
            total = len(x)
            for idx, x_i in enumerate(x):
                # Log progress every 20%
                if idx % (total // 5) == 0 and idx > 0:
                    logger.info(f"  Progress: {idx}/{total} ({idx*100//total}%)")
                count = len([1 for x_j in x if _maxdist(x_i, x_j) <= r])
                C.append(count / (N - m_val + 1.0))
            return (N - m_val + 1.0)**(-1) * sum(np.log(C))
        
        N = len(data)
        r = r * np.std(data)
        
        logger.info(f"  Calculating phi for m={m}...")
        phi_m = _phi(m)
        logger.info(f"  Calculating phi for m={m+1}...")
        phi_m1 = _phi(m + 1)
        
        return abs(phi_m1 - phi_m)
    
    def _hurst_exponent(self, data: np.ndarray) -> float:
        """
        Compute Hurst exponent using R/S analysis
        
        Args:
            data: Input signal
            
        Returns:
            Hurst exponent (0.5 = random, >0.5 = trending, <0.5 = mean-reverting)
        """
        lags = range(2, min(100, len(data)//2))
        tau = []
        
        for lag in lags:
            # Divide series into chunks
            chunks = [data[i:i+lag] for i in range(0, len(data), lag) if len(data[i:i+lag]) == lag]
            
            if not chunks:
                continue
            
            # Compute R/S for each chunk
            rs_values = []
            for chunk in chunks:
                mean_chunk = np.mean(chunk)
                deviations = chunk - mean_chunk
                cumsum = np.cumsum(deviations)
                
                R = np.max(cumsum) - np.min(cumsum)
                S = np.std(chunk)
                
                if S > 0:
                    rs_values.append(R / S)
            
            if rs_values:
                tau.append(np.mean(rs_values))
        
        # Fit log-log plot
        if len(tau) > 2:
            poly = np.polyfit(np.log(lags[:len(tau)]), np.log(tau), 1)
            return float(poly[0])
        else:
            return 0.5
    
    def extract_all_features(
        self,
        data: np.ndarray,
        sample_rate: float = 1.0
    ) -> Dict[str, Any]:
        """
        Extract all available features from noise data
        
        Args:
            data: Input noise array
            sample_rate: Sampling rate (for FFT features)
            
        Returns:
            Dictionary containing all features
        """
        logger.info("Extracting all features...")
        
        # Ensure data is 1D
        if data.ndim > 1:
            data = data.flatten()
        
        # Normalize if requested
        if self.normalize:
            data = self.normalize_data(data, method='standard')
        
        all_features = {}
        
        # Statistical features
        all_features.update(self.compute_statistical_features(data))
        
        # Entropy
        all_features['shannon_entropy'] = self.compute_entropy(data)
        
        # FFT features
        all_features.update(self.compute_fft_features(data, sample_rate))
        
        # Autocorrelation features
        all_features.update(self.compute_autocorrelation_features(data))
        
        # Complexity features
        all_features.update(self.compute_complexity_features(data))
        
        logger.info(f"Extracted {len(all_features)} features")
        
        return all_features


class FeatureVector:
    """Converts features dictionary to fixed-size feature vector"""
    
    def __init__(self, feature_names: Optional[list] = None):
        """
        Initialize feature vector converter
        
        Args:
            feature_names: Ordered list of feature names
        """
        self.feature_names = feature_names
    
    def to_vector(self, features: Dict[str, Any]) -> np.ndarray:
        """
        Convert features dictionary to numpy vector
        
        Args:
            features: Dictionary of features
            
        Returns:
            Feature vector as numpy array
        """
        if self.feature_names is None:
            # Auto-generate feature names from first call
            self.feature_names = sorted(features.keys())
        
        vector = np.array([features.get(name, 0.0) for name in self.feature_names], 
                         dtype=np.float32)
        
        return vector
    
    def from_vector(self, vector: np.ndarray) -> Dict[str, float]:
        """
        Convert feature vector back to dictionary
        
        Args:
            vector: Feature vector
            
        Returns:
            Dictionary of features
        """
        if self.feature_names is None:
            raise ValueError("Feature names not set")
        
        return {name: float(val) for name, val in zip(self.feature_names, vector)}


def main():
    """Test preprocessing and feature extraction"""
    preprocessor = NoisePreprocessor(normalize=True)
    
    print("\n=== Preprocessing & Feature Extraction Test ===")
    
    # Generate test signal
    t = np.linspace(0, 1, 1000)
    signal_test = np.sin(2 * np.pi * 5 * t) + 0.5 * np.random.randn(1000)
    
    print(f"\nTest signal: {len(signal_test)} samples")
    
    # Apply bandpass filter
    print("\n=== Bandpass Filter ===")
    filtered = preprocessor.apply_bandpass_filter(signal_test)
    print(f"Filtered signal std: {filtered.std():.4f}")
    
    # Normalize
    print("\n=== Normalization ===")
    normalized = preprocessor.normalize_data(signal_test, method='standard')
    print(f"Normalized mean: {normalized.mean():.6f}")
    print(f"Normalized std: {normalized.std():.6f}")
    
    # Extract all features
    print("\n=== Feature Extraction ===")
    features = preprocessor.extract_all_features(signal_test, sample_rate=1000)
    
    print(f"\nTotal features: {len(features)}")
    print("\nSample features:")
    for i, (name, value) in enumerate(list(features.items())[:10]):
        print(f"  {name}: {value:.6f}")
    
    # Convert to feature vector
    print("\n=== Feature Vector Conversion ===")
    converter = FeatureVector()
    vector = converter.to_vector(features)
    print(f"Feature vector shape: {vector.shape}")
    print(f"Feature vector range: [{vector.min():.4f}, {vector.max():.4f}]")
    
    # Convert back
    reconstructed = converter.from_vector(vector)
    print(f"Reconstructed features: {len(reconstructed)}")


if __name__ == "__main__":
    main()
</file>

<file path="preprocessing/utils.py">
"""
Utility functions for preprocessing
"""

import numpy as np
from typing import Tuple, List
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def sliding_window(
    data: np.ndarray,
    window_size: int,
    stride: int = None
) -> List[np.ndarray]:
    """
    Create sliding windows over data
    
    Args:
        data: Input array
        window_size: Size of each window
        stride: Step size between windows (defaults to window_size)
        
    Returns:
        List of windowed arrays
    """
    if stride is None:
        stride = window_size
    
    windows = []
    for i in range(0, len(data) - window_size + 1, stride):
        windows.append(data[i:i+window_size])
    
    return windows


def augment_noise_sample(
    data: np.ndarray,
    num_augmentations: int = 5,
    noise_level: float = 0.1
) -> List[np.ndarray]:
    """
    Augment noise sample with variations
    
    Args:
        data: Input noise array
        num_augmentations: Number of augmented samples to create
        noise_level: Amount of augmentation noise to add
        
    Returns:
        List of augmented samples
    """
    augmented = [data]  # Include original
    
    for _ in range(num_augmentations):
        # Add random noise
        noisy = data + noise_level * np.random.randn(*data.shape)
        augmented.append(noisy)
    
    return augmented


def downsample_signal(
    data: np.ndarray,
    factor: int = 2
) -> np.ndarray:
    """
    Downsample signal by factor
    
    Args:
        data: Input signal
        factor: Downsampling factor
        
    Returns:
        Downsampled signal
    """
    return data[::factor]


def pad_or_truncate(
    data: np.ndarray,
    target_length: int,
    pad_value: float = 0.0
) -> np.ndarray:
    """
    Pad or truncate array to target length
    
    Args:
        data: Input array
        target_length: Desired length
        pad_value: Value to use for padding
        
    Returns:
        Array with target length
    """
    if len(data) >= target_length:
        return data[:target_length]
    else:
        padding = np.full(target_length - len(data), pad_value)
        return np.concatenate([data, padding])


def batch_process(
    samples: List[np.ndarray],
    processor_func,
    **kwargs
) -> List:
    """
    Process multiple samples in batch
    
    Args:
        samples: List of input arrays
        processor_func: Function to apply to each sample
        **kwargs: Arguments to pass to processor_func
        
    Returns:
        List of processed results
    """
    results = []
    
    for i, sample in enumerate(samples):
        try:
            result = processor_func(sample, **kwargs)
            results.append(result)
        except Exception as e:
            logger.error(f"Failed to process sample {i}: {e}")
            results.append(None)
    
    return results


def compute_snr(signal: np.ndarray, noise: np.ndarray) -> float:
    """
    Compute Signal-to-Noise Ratio
    
    Args:
        signal: Signal array
        noise: Noise array
        
    Returns:
        SNR in dB
    """
    signal_power = np.mean(signal ** 2)
    noise_power = np.mean(noise ** 2)
    
    snr = 10 * np.log10(signal_power / (noise_power + 1e-10))
    
    return float(snr)


def normalize_batch(
    samples: List[np.ndarray],
    method: str = 'standard'
) -> List[np.ndarray]:
    """
    Normalize a batch of samples
    
    Args:
        samples: List of arrays
        method: Normalization method
        
    Returns:
        List of normalized arrays
    """
    normalized = []
    
    for sample in samples:
        if method == 'standard':
            norm = (sample - np.mean(sample)) / (np.std(sample) + 1e-10)
        elif method == 'minmax':
            norm = (sample - np.min(sample)) / (np.ptp(sample) + 1e-10)
        else:
            norm = sample
        
        normalized.append(norm)
    
    return normalized


def merge_noise_sources(
    sources: List[np.ndarray],
    weights: List[float] = None
) -> np.ndarray:
    """
    Merge multiple noise sources with optional weighting
    
    Args:
        sources: List of noise arrays
        weights: Optional weights for each source
        
    Returns:
        Merged noise array
    """
    if weights is None:
        weights = [1.0] * len(sources)
    
    # Pad all sources to same length
    max_len = max(len(s) for s in sources)
    padded = [pad_or_truncate(s, max_len) for s in sources]
    
    # Weighted sum
    merged = np.zeros(max_len)
    for source, weight in zip(padded, weights):
        merged += weight * source
    
    # Normalize
    merged = merged / (sum(weights) + 1e-10)
    
    return merged


def main():
    """Test utility functions"""
    print("\n=== Preprocessing Utilities Test ===")
    
    # Test data
    data = np.random.randn(1000)
    
    # Sliding windows
    print("\n=== Sliding Windows ===")
    windows = sliding_window(data, window_size=100, stride=50)
    print(f"Created {len(windows)} windows")
    
    # Augmentation
    print("\n=== Augmentation ===")
    augmented = augment_noise_sample(data[:100], num_augmentations=3)
    print(f"Created {len(augmented)} augmented samples")
    
    # Pad/truncate
    print("\n=== Pad/Truncate ===")
    padded = pad_or_truncate(data[:50], target_length=100)
    print(f"Padded from 50 to {len(padded)}")
    
    truncated = pad_or_truncate(data, target_length=500)
    print(f"Truncated from 1000 to {len(truncated)}")
    
    # Merge sources
    print("\n=== Merge Sources ===")
    source1 = np.random.randn(100)
    source2 = np.random.randn(150)
    merged = merge_noise_sources([source1, source2], weights=[0.6, 0.4])
    print(f"Merged sources: length {len(merged)}")


if __name__ == "__main__":
    main()
</file>

<file path="QUICKSTART.md">
# QNA-Auth Quick Start Guide

## First-Time Setup

### 1. Environment Setup

```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On Linux/Mac:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Verify Installation

```bash
# Test noise collection
python -m noise_collection.qrng_api

# Should output quantum noise statistics
```

### 3. Start Backend Server

```bash
python server/app.py
```

Visit http://localhost:8000/docs for API documentation.

### 4. Start Frontend

```bash
cd frontend
npm install
npm run dev
```

Visit http://localhost:3000

## Common Tasks

### Enroll a New Device

1. Open http://localhost:3000/enroll
2. Choose noise sources (QRNG recommended)
3. Click "Enroll Device"
4. Copy the device ID

### Authenticate a Device

1. Open http://localhost:3000/authenticate
2. Select device from dropdown
3. Click "Authenticate"
4. View results

### View Enrolled Devices

1. Open http://localhost:3000/devices
2. Click on a device to see details
3. Delete devices if needed

## Training Custom Model

### Step 1: Collect Training Data

```python
from dataset.builder import DatasetBuilder
from noise_collection import QRNGClient

builder = DatasetBuilder()
qrng = QRNGClient()

# Enroll multiple devices
for i in range(5):
    device_id = f"device_{i:03d}"
    samples = qrng.fetch_multiple_samples(50)
    builder.add_batch(device_id, 'qrng', samples)
```

### Step 2: Process Features

```python
from preprocessing import NoisePreprocessor, FeatureVector

preprocessor = NoisePreprocessor()
converter = FeatureVector()

# Extract features from dataset
# (See model/train.py for complete example)
```

### Step 3: Train Model

```python
from model import SiameseNetwork, ModelTrainer

model = SiameseNetwork(input_dim=50, embedding_dim=128)
trainer = ModelTrainer(model, loss_type='triplet')
trainer.train(train_loader, val_loader, epochs=50)
```

### Step 4: Use Trained Model

```python
# Copy trained model to server directory
cp model/checkpoints/best_model.pt server/models/

# Restart server to load new model
```

## Troubleshooting

### Camera Not Working

- Ensure webcam is connected
- Check camera permissions
- Try different camera_index in config

### Microphone Not Working

- Check audio input permissions
- Verify microphone is not muted
- Install sounddevice: `pip install sounddevice`

### QRNG API Fails

- Check internet connection
- API may have rate limits (wait and retry)
- Use alternative: `qrng_client = QRNGClient(api_url=...)`

### Low Authentication Accuracy

- Increase number of samples during enrollment
- Use multiple noise sources
- Train model on more diverse data
- Adjust similarity threshold

## Development Tips

### Running Tests

```bash
# Test individual modules
python -m pytest tests/

# Or test manually
python -m noise_collection.qrng_api
python -m preprocessing.features
python -m model.siamese_model
```

### Code Formatting

```bash
# Format Python code
black .

# Lint
flake8 .
```

### Frontend Development

```bash
cd frontend

# Type checking
npm run build

# Linting
npm run lint
```

## Production Deployment

### Security Checklist

- [ ] Change CORS origins in server/app.py
- [ ] Add authentication/API keys for endpoints
- [ ] Enable HTTPS
- [ ] Encrypt device embeddings at rest
- [ ] Set up proper logging
- [ ] Configure rate limiting
- [ ] Review security settings in config.py

### Performance Optimization

- [ ] Use ONNX runtime for inference
- [ ] Enable model quantization
- [ ] Add caching layer
- [ ] Use production WSGI server (gunicorn)
- [ ] Optimize frontend build (npm run build)
- [ ] Enable gzip compression

## Resources

- API Documentation: http://localhost:8000/docs
- Frontend: http://localhost:3000
- ANU QRNG: https://qrng.anu.edu.au/
- PyTorch Docs: https://pytorch.org/docs/
- FastAPI Docs: https://fastapi.tiangolo.com/
</file>

<file path="requirements.txt">
# QNA-Auth Python Dependencies

# Core ML and Scientific Computing
# Install PyTorch with CUDA support separately:
# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
torch>=2.0.0
numpy>=1.24.0
scipy>=1.10.0
scikit-learn>=1.3.0

# Deep Learning utilities
torchvision>=0.15.0
tqdm>=4.65.0

# Backend Framework
fastapi>=0.100.0
uvicorn[standard]>=0.23.0
pydantic>=2.0.0
python-multipart>=0.0.6

# Noise Collection
requests>=2.31.0
opencv-python>=4.8.0
sounddevice>=0.4.6
psutil>=5.9.0

# Data Management
pandas>=2.0.0
matplotlib>=3.7.0

# Security
cryptography>=41.0.0

# Optional: For ONNX export
# onnx>=1.14.0
# onnxruntime>=1.15.0

# Development and Testing
pytest>=7.4.0
pytest-asyncio>=0.21.0
black>=23.0.0
flake8>=6.0.0
</file>

<file path="RESEARCH_FOCUS.md">
# Research Focus: What to Work On Next

## TL;DR - If you only read one thing

**Stop working on the web UI. Start working on the core quantum authentication science.**

The React frontend and FastAPI polishing is **not the innovation**. The innovation is proving that quantum noise can reliably fingerprint devices.

---

## What makes this project valuable (the research contribution)

### Novel claims
1. **Quantum noise contains device-specific patterns** that can be learned by ML models
2. **These patterns are stable enough** for authentication (across days/weeks)
3. **But unpredictable enough** to resist replay and cloning attacks
4. **Non-invertible embeddings** prevent recovery of raw device characteristics

### Why this matters
- First practical use of quantum randomness for device authentication
- Combines physics (quantum unpredictability) + AI (metric learning) + crypto (challenge-response)
- Hardware-agnostic (works on commodity devices)
- No stored secrets (unlike passwords, keys, biometric templates)

---

## What to focus on (research tasks)

### 1. Data collection (Week 1-2)
**Task**: Collect comprehensive device corpus
```bash
# Target: 50+ devices, 200 samples each, multiple sources
devices_to_collect = [
    "Your laptop",
    "Your phone", 
    "Lab desktop computers (ask lab mates)",
    "Friends' phones",
    "Raspberry Pis",
    "Arduino boards",
    # ... get 50+ diverse devices
]

for device in devices_to_collect:
    enroll_device(device, sources=['qrng', 'camera', 'microphone'], samples=200)
```

**Why this matters**: Without diverse real-world data, you can't prove the hypothesis. The current "random initialization" model proves nothing.

**Success metric**: ‚â•50 devices enrolled with ‚â•100 samples each

### 2. Statistical analysis (Week 2-3)
**Task**: Prove devices are distinguishable
```python
# research/analyze_device_signatures.py

# Load all device samples
devices = load_device_corpus()

# Compute similarity matrices
intra_device_sims = []  # Same device, different samples
inter_device_sims = []  # Different devices

for d in devices:
    samples = d.get_samples()
    # Compute pairwise similarities within device
    intra_device_sims.extend(pairwise_similarities(samples))
    
    for other in devices:
        if other.id != d.id:
            # Compute similarity between devices
            inter_device_sims.append(similarity(d, other))

# Statistical test: Are distributions significantly different?
from scipy.stats import ttest_ind
t_stat, p_value = ttest_ind(intra_device_sims, inter_device_sims)

print(f"Intra-device similarity: {mean(intra_device_sims):.3f} ¬± {std(intra_device_sims):.3f}")
print(f"Inter-device similarity: {mean(inter_device_sims):.3f} ¬± {std(inter_device_sims):.3f}")
print(f"T-test: t={t_stat:.2f}, p={p_value:.2e}")

# Create figure for paper
plot_similarity_distributions(intra_device_sims, inter_device_sims)
```

**Why this matters**: This is the **core hypothesis** of your project. If devices aren't distinguishable, the whole approach fails.

**Success metric**: Clear bimodal distribution with p < 0.001

### 3. Model training (Week 3-4)
**Task**: Train a model that actually works
```python
# research/train_models.py

# Compare architectures
architectures = [
    {'embedding_dim': 64, 'hidden': [128, 128]},
    {'embedding_dim': 128, 'hidden': [256, 256, 128]},
    {'embedding_dim': 256, 'hidden': [512, 512, 256]},
]

# Compare loss functions
losses = ['triplet', 'contrastive', 'arcface']

# Grid search
for arch in architectures:
    for loss in losses:
        model = train_model(
            data=device_corpus,
            architecture=arch,
            loss_function=loss,
            epochs=50,
            batch_size=32
        )
        
        results = evaluate_model(model, test_data)
        print(f"{arch} + {loss}: FAR={results.far:.4f}, FRR={results.frr:.4f}")
```

**Why this matters**: The current random model is a placeholder. You need a trained model to prove this actually works.

**Success metric**: ‚â•95% accuracy, <1% FAR, <1% FRR

### 4. Security evaluation (Week 4-5)
**Task**: Prove resistance to attacks
```python
# research/attack_simulations.py

# Attack 1: Replay (use old samples)
def replay_attack(device, stored_samples):
    # Attacker has access to N old authentication samples
    # Try to authenticate using those
    success_rate = 0
    for old_sample in stored_samples:
        result = authenticate(device_id, noise=old_sample)
        if result.authenticated:
            success_rate += 1
    return success_rate / len(stored_samples)

# Attack 2: Synthesis (GAN-generated noise)
def synthesis_attack(device, embedding):
    # Train GAN to generate noise that produces similar embedding
    gan = train_gan_to_match_embedding(embedding)
    synthetic_samples = gan.generate(n=100)
    
    success_rate = 0
    for sample in synthetic_samples:
        result = authenticate(device_id, noise=sample)
        if result.authenticated:
            success_rate += 1
    return success_rate / len(synthetic_samples)

# Attack 3: Cloning (identical hardware)
def cloning_attack(target_device):
    # Get same model phone/laptop
    clone = get_identical_hardware(target_device.model)
    
    # Try to enroll clone with target's device_id
    success = can_clone_authenticate(clone, target_device.id)
    return success

# Run all attacks
attacks = {
    'Replay': replay_attack,
    'Synthesis': synthesis_attack,
    'Cloning': cloning_attack,
}

for attack_name, attack_fn in attacks.items():
    success_rate = attack_fn(test_devices)
    print(f"{attack_name} attack success rate: {success_rate:.2%}")
    # Should be < 0.1% for all attacks
```

**Why this matters**: Security claims require evidence. Can't just say "it's secure" - need to demonstrate attack failure.

**Success metric**: All attacks fail >99.9% of the time

### 5. Write the paper (Week 5-8)
**Task**: Document everything for peer review
```latex
% paper/qna_auth.tex

\section{Introduction}
We propose QNA-Auth, a novel device authentication system that derives 
fingerprints from quantum noise patterns using machine learning...

\section{Threat Model}
We consider an adversary with the following capabilities:
- Access to N historical authentication samples
- Ability to measure quantum noise from identical hardware
- White-box access to the ML model and stored embeddings
- Computational power bounded by...

\section{Method}
\subsection{Noise Collection}
We collect quantum noise from four sources: ...

\subsection{Feature Extraction}
We extract D-dimensional feature vectors comprising statistical, 
spectral, and complexity features: ...

\subsection{Embedding Model}
We train a Siamese network with triplet loss to learn a metric 
space where same-device samples cluster: ...

\section{Evaluation}
\subsection{Experimental Setup}
We enrolled 50 devices (25 smartphones, 15 laptops, 10 IoT devices)
and collected 200 samples per device over 90 days...

\subsection{Device Distinguishability}
Figure X shows the distribution of intra-device vs inter-device 
similarities. We observe clear separation (t=X.XX, p<0.001)...

\subsection{Attack Resistance}
Table Y summarizes attack success rates. All attacks failed >99.9%...

\section{Discussion}
Limitations: ...
Future work: ...
```

**Why this matters**: The paper is the **primary deliverable** for a research project. Without publication, this is just a hobby project.

**Success metric**: Submitted to IEEE S&P, CCS, NDSS, or USENIX Security

---

## What NOT to focus on (web dev tasks)

### Stop doing these (low priority for research)
- ‚ùå Styling the React UI with better CSS
- ‚ùå Adding more API endpoints (only need enroll + authenticate)
- ‚ùå User authentication for the web UI (only one researcher uses it)
- ‚ùå Database schema design (flat files are fine for research)
- ‚ùå Rate limiting and DDoS protection (not running a public service)
- ‚ùå Deployment automation and CI/CD (manual is fine)
- ‚ùå Monitoring dashboards (just use logs)
- ‚ùå Multi-tenancy (one user at a time is fine)

### Keep minimal
- ‚úÖ Simple FastAPI endpoint (50 lines total)
- ‚úÖ Jupyter notebooks for analysis (not React)
- ‚úÖ Command-line tools for experiments
- ‚úÖ Docker for reproducibility (not for scaling)

---

## Time allocation (for a 15-week project)

| Task | Weeks | % of time |
|------|-------|-----------|
| Data collection | 2 | 13% |
| Statistical analysis | 1 | 7% |
| Model training & tuning | 2 | 13% |
| Security evaluation | 2 | 13% |
| Longitudinal stability study | 4 (parallel) | 27% |
| Paper writing | 3 | 20% |
| Code cleanup & documentation | 1 | 7% |
| **Total** | **15** | **100%** |

**Note**: Web UI polish is <1% of time (maybe 1-2 hours to make a simple demo page)

---

## Tools you should be using

### For research (use these)
- **Jupyter notebooks** for data exploration and visualization
- **Matplotlib/Seaborn** for figures in the paper
- **Scikit-learn** for baseline comparisons and metrics
- **PyTorch** for model training
- **NumPy/SciPy** for statistical analysis
- **Pandas** for data management

### For web dev (minimize usage)
- **FastAPI** (keep to <100 lines for basic endpoints)
- **React** (optional, only for demo presentations)

---

## Metrics that matter (research success)

### Core authentication metrics
- [x] **Device distinguishability**: Intra-device similarity >> inter-device similarity (effect size d' > 3)
- [ ] **Authentication accuracy**: >95% with <1% FAR/FRR
- [ ] **Stability**: <5% accuracy degradation over 90 days
- [ ] **Minimal samples**: <50 enrollment samples, <5 auth samples
- [ ] **Attack resistance**: All attacks fail >99.9% of the time

### Academic metrics
- [ ] **Paper submitted** to top-tier venue
- [ ] **Reproducibility**: Public code + anonymized data
- [ ] **Novel contribution**: First practical quantum-noise-based auth

### Metrics that DON'T matter for research
- ‚¨ú API response time (as long as it's <10s)
- ‚¨ú Database query optimization (flat files are fine)
- ‚¨ú Frontend load time (it's a local demo)
- ‚¨ú Scalability (not building a service)
- ‚¨ú Uptime (it's a research prototype)

---

## Immediate next action (this week)

1. **Read `VISION.md`** to understand the reframed project goals
2. **Start data collection**: Enroll 10 devices as a pilot
3. **Run statistical analysis**: Do the 10 devices show different noise patterns?
4. **If yes**: Scale to 50+ devices
5. **If no**: Refine feature extraction (try different features)

**Stop working on**: React component styling, API authentication, database design

**Start working on**: Data collection, statistical analysis, model training

---

## Questions to answer (in order of priority)

1. **Can quantum noise distinguish devices?** (Week 1-3)
   - Are intra-device similarities significantly higher than inter-device?
   - What features are most discriminative?

2. **Is it stable over time?** (Week 4-10, parallel)
   - Does the same device authenticate successfully after 30/60/90 days?
   - What causes drift? (temperature, battery, etc.)

3. **Is it secure against attacks?** (Week 4-6)
   - Can replay attacks succeed?
   - Can synthetic noise fool the system?
   - Can cloned hardware authenticate?

4. **How minimal can we make it?** (Week 6-7)
   - What's the minimum number of enrollment samples?
   - What's the minimum number of auth samples?
   - Can we use fewer noise sources?

5. **How does it compare to existing methods?** (Week 8)
   - QNA-Auth vs. password
   - QNA-Auth vs. biometrics
   - QNA-Auth vs. hardware tokens

**Answering these 5 questions is the entire project.** The web UI is just a tool to help answer them.

---

## Summary

**What QNA-Auth is**: A research contribution showing quantum noise can fingerprint devices

**What QNA-Auth is NOT**: A production-ready web authentication service

**What to build**: Data collection pipeline + ML models + statistical analysis + paper

**What NOT to build**: Polished React UI + scalable API + production database

**Success looks like**: Accepted paper at a top security conference

**Failure looks like**: Beautiful web UI but no proof that the core idea works
</file>

<file path="run_full_training.py">
import os
import shutil
import torch
from dataset.builder import DatasetBuilder
from model.siamese_model import SiameseNetwork
# from model.train import ModelTrainer # Importing locally to avoid circular issues if any
import config

def main():
    print("--- 1. BUILDING DATASET ---")
    builder = DatasetBuilder()
    
    # Check if we have raw data
    if not os.path.exists(config.DATA_DIR) or len(os.listdir(config.DATA_DIR)) == 0:
        print("!! NO RAW DATA FOUND !!")
        print("Tip: Run 'python collect_training_data.py' to collect real samples.")
        print("Continuing with DUMMY/INITIALIZED model for server testing...")
    else:
        # Process raw data into CSVs
        try:
            builder.export_for_training(config.PROCESSED_DIR)
            print(f"Dataset exported to {config.PROCESSED_DIR}")
        except Exception as e:
            print(f"Dataset export warning: {e}")

    print("\n--- 2. INITIALIZING MODEL ---")
    # Initialize the model structure
    # MATCHING SERVER CONFIG: Input=31 (features), Output=128 (embedding)
    model = SiameseNetwork(input_dim=31, embedding_dim=128).to(config.DEVICE)
    
    print("\n--- 3. TRAINING LOOP (Skipped for Setup) ---")
    print("Skipping actual training loop to generate initial model file.")
    print("To train for real, you will need to collect data first.")

    print("\n--- 4. SAVING & DEPLOYING MODEL ---")
    # 1. Save locally in model/checkpoints
    os.makedirs("model/checkpoints", exist_ok=True)
    save_path = "model/checkpoints/best_model.pt"
    # Save in the format expected by DeviceEmbedder.load_model
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'epoch': 0,
        'loss': 0.0,
        'feature_dim': 128 # Should match input_dim
    }
    torch.save(checkpoint, save_path)
    print(f"Model saved locally to {save_path}")

    # 2. Deploy to Server folder
    server_model_path = config.MODEL_PATH
    server_model_dir = os.path.dirname(server_model_path)
    os.makedirs(server_model_dir, exist_ok=True)
    
    shutil.copy(save_path, server_model_path)
    print(f"üöÄ Model deployed to Server at: {server_model_path}")
    print("You can now run 'python server/app.py' safely!")

if __name__ == "__main__":
    main()
</file>

<file path="server/routes.py">
"""
Additional API routes and utilities
"""

from fastapi import APIRouter, Depends, HTTPException
from typing import Dict
import logging

logger = logging.getLogger(__name__)

router = APIRouter()


@router.get("/stats")
async def get_statistics():
    """Get system statistics"""
    return {
        "status": "not_implemented",
        "message": "Statistics endpoint coming soon"
    }


@router.get("/metrics")
async def get_metrics():
    """Get system metrics"""
    return {
        "status": "not_implemented",
        "message": "Metrics endpoint coming soon"
    }
</file>

<file path="setup.bat">
@echo off
REM QNA-Auth Setup Script for Windows
REM This script sets up the QNA-Auth environment

echo ==========================================
echo QNA-Auth Setup Script
echo ==========================================
echo.

REM Check Python version
echo Checking Python version...
python --version
echo.

REM Create virtual environment
echo Creating virtual environment...
python -m venv venv
echo.

REM Activate virtual environment
echo Activating virtual environment...
call venv\Scripts\activate.bat
echo.

REM Install Python dependencies
echo Installing Python dependencies...
python -m pip install --upgrade pip
pip install -r requirements.txt
echo.

REM Create necessary directories
echo Creating directories...
if not exist "dataset\samples" mkdir dataset\samples
if not exist "auth\device_embeddings" mkdir auth\device_embeddings
if not exist "model\checkpoints" mkdir model\checkpoints
echo.

REM Copy example config
echo Copying example configuration...
if not exist "config.py" (
    copy config.example.py config.py
    echo Config.py created - please review and adjust settings
) else (
    echo config.py already exists
)
echo.

REM Test installations
echo Testing installations...
python -c "import torch; print(f'PyTorch version: {torch.__version__}')"
python -c "import fastapi; print(f'FastAPI version: {fastapi.__version__}')"
python -c "import numpy; print(f'NumPy version: {numpy.__version__}')"
echo.

echo ==========================================
echo Backend setup complete!
echo ==========================================
echo.
echo Next steps:
echo 1. Review and adjust settings in config.py
echo 2. Start backend: python server\app.py
echo 3. Setup frontend: cd frontend ^&^& npm install
echo 4. Start frontend: cd frontend ^&^& npm run dev
echo.
pause
</file>

<file path="setup.sh">
#!/bin/bash

# QNA-Auth Setup Script
# This script sets up the QNA-Auth environment

set -e

echo "=========================================="
echo "QNA-Auth Setup Script"
echo "=========================================="
echo ""

# Check Python version
echo "Checking Python version..."
python --version

# Create virtual environment
echo ""
echo "Creating virtual environment..."
python -m venv venv

# Activate virtual environment
echo "Activating virtual environment..."
if [[ "$OSTYPE" == "msys" || "$OSTYPE" == "win32" ]]; then
    source venv/Scripts/activate
else
    source venv/bin/activate
fi

# Install Python dependencies
echo ""
echo "Installing Python dependencies..."
pip install --upgrade pip
pip install -r requirements.txt

# Create necessary directories
echo ""
echo "Creating directories..."
mkdir -p dataset/samples
mkdir -p auth/device_embeddings
mkdir -p model/checkpoints

# Copy example config
echo ""
echo "Copying example configuration..."
if [ ! -f config.py ]; then
    cp config.example.py config.py
    echo "‚úì config.py created (please review and adjust settings)"
else
    echo "config.py already exists"
fi

# Test installations
echo ""
echo "Testing installations..."
python -c "import torch; print(f'PyTorch version: {torch.__version__}')"
python -c "import fastapi; print(f'FastAPI version: {fastapi.__version__}')"
python -c "import numpy; print(f'NumPy version: {numpy.__version__}')"

echo ""
echo "=========================================="
echo "‚úì Backend setup complete!"
echo "=========================================="
echo ""
echo "Next steps:"
echo "1. Review and adjust settings in config.py"
echo "2. Start backend: python server/app.py"
echo "3. Setup frontend: cd frontend && npm install"
echo "4. Start frontend: cd frontend && npm run dev"
echo ""
</file>

<file path="test_collection.py">
"""
Test noise collection from camera and microphone
"""
import sys
import numpy as np
import logging

logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

from noise_collection import CameraNoiseCollector, MicrophoneNoiseCollector
from preprocessing.features import NoisePreprocessor, FeatureVector

print("=" * 70)
print("Testing Camera and Microphone Noise Collection")
print("=" * 70)

preprocessor = NoisePreprocessor()
feature_converter = FeatureVector()

# Test Camera
print("\n" + "="* 70)
print("1. Testing Camera Noise Collection")
print("=" * 70)

try:
    camera_collector = CameraNoiseCollector(camera_index=0)
    if camera_collector.initialize_camera():
        print("‚úÖ Camera initialized")
        
        # Capture a few frames
        frames = camera_collector.capture_multiple_frames(num_frames=3, exposure_time=0.1)
        print(f"‚úÖ Captured {len(frames)} frames")
        
        if frames:
            # Extract noise from first frame
            noise = camera_collector.extract_noise_features(frames[0])
            print(f"‚úÖ Extracted noise: shape={noise.shape}, dtype={noise.dtype}")
            print(f"   Stats: min={noise.min():.2f}, max={noise.max():.2f}, mean={noise.mean():.2f}")
            
            # Try to extract features
            try:
                features = preprocessor.extract_all_features(noise)
                print(f"‚úÖ Extracted {len(features)} features:")
                for key, value in list(features.items())[:5]:
                    print(f"      {key}: {value}")
                
                # Convert to feature vector
                fv = feature_converter.to_vector(features)
                print(f"‚úÖ Feature vector: shape={fv.shape}, dtype={fv.dtype}")
                
            except Exception as e:
                print(f"‚ùå Feature extraction failed: {e}")
                import traceback
                traceback.print_exc()
        
        camera_collector.release()
        print("‚úÖ Camera released")
    else:
        print("‚ùå Failed to initialize camera")
        
except Exception as e:
    print(f"‚ùå Camera test failed: {e}")
    import traceback
    traceback.print_exc()

# Test Microphone
print("\n" + "=" * 70)
print("2. Testing Microphone Noise Collection")
print("=" * 70)

try:
    mic_collector = MicrophoneNoiseCollector(sample_rate=44100)
    print("‚úÖ Microphone collector created")
    
    # Capture samples
    samples = mic_collector.capture_multiple_samples(num_samples=3, duration=0.5)
    print(f"‚úÖ Captured {len(samples)} audio samples")
    
    if samples:
        audio = samples[0]
        print(f"‚úÖ First sample: shape={audio.shape}, dtype={audio.dtype}")
        print(f"   Stats: min={audio.min():.6f}, max={audio.max():.6f}, mean={audio.mean():.6f}")
        
        # Try to extract features
        try:
            features = preprocessor.extract_all_features(audio)
            print(f"‚úÖ Extracted {len(features)} features:")
            for key, value in list(features.items())[:5]:
                print(f"      {key}: {value}")
            
            # Convert to feature vector
            fv = feature_converter.to_vector(features)
            print(f"‚úÖ Feature vector: shape={fv.shape}, dtype={fv.dtype}")
            
        except Exception as e:
            print(f"‚ùå Feature extraction failed: {e}")
            import traceback
            traceback.print_exc()
    
except Exception as e:
    print(f"‚ùå Microphone test failed: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "=" * 70)
print("Test Complete")
print("=" * 70)
</file>

<file path="test_cross_device.py">
import requests
import numpy as np
import json
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("CROSS_DEVICE_TEST")

BASE_URL = "http://localhost:8000"

def generate_samples(mean, std, count=10, size=48000):
    """Generate synthetic noise samples."""
    return [np.random.normal(mean, std, size).tolist() for _ in range(count)]

def run_cross_device_test():
    logger.info(" starting Cross-Device Authentication Test...")

    # 1. Enroll Device A (Quiet Profile)
    logger.info("\n[Step 1] Enrolling 'Device A' (Quiet Profile)...")
    initial_samples = generate_samples(0.0, 0.002, count=20) # RMS ~0.002
    
    enroll_payload = {
        "device_name": "Device_A_Quiet",
        "num_samples": 20,
        "sources": ["microphone"],
        "client_samples": {
            "microphone": initial_samples
        }
    }
    
    resp = requests.post(f"{BASE_URL}/enroll", json=enroll_payload)
    if resp.status_code != 201:
        logger.error(f"Enrollment failed: {resp.text}")
        return
        
    device_a_id = resp.json()["device_id"]
    logger.info(f" -> Enrolled Device A with ID: {device_a_id}")

    # 2. Authenticate Device A with correct samples (Quiet)
    logger.info("\n[Step 2] Authenticating Device A with Matching (Quiet) samples...")
    auth_samples = generate_samples(0.0, 0.002, count=5) # Same profile
    
    auth_payload = {
        "device_id": device_a_id,
        "sources": ["microphone"],
        "num_samples_per_source": 5,
        "client_samples": {
            "microphone": auth_samples
        }
    }
    
    resp = requests.post(f"{BASE_URL}/authenticate", json=auth_payload)
    result = resp.json()
    logger.info(f" -> Result: Authenticated={result['authenticated']}, Similarity={result['similarity']:.4f}")
    
    if not result['authenticated']:
        logger.error("FAILURE: Self-authentication failed!")
    else:
        logger.info("SUCCESS: Self-authentication passed.")

    # 3. Authenticate Device A with DIFFERENT samples (Loud) - Simulating Device B
    logger.info("\n[Step 3] Attacking Device A with Device B (Loud) samples...")
    attack_samples = generate_samples(0.0, 0.010, count=5) # 5x louder, RMS ~0.010
    
    attack_payload = {
        "device_id": device_a_id,
        "sources": ["microphone"],
        "num_samples_per_source": 5,
        "client_samples": {
            "microphone": attack_samples
        }
    }
    
    resp = requests.post(f"{BASE_URL}/authenticate", json=attack_payload)
    result = resp.json()
    logger.info(f" -> Result: Authenticated={result['authenticated']}, Similarity={result['similarity']:.4f}")
    
    if result['authenticated']:
        logger.error("FAILURE: False Acceptance! Different device was authenticated.")
    else:
        logger.info("SUCCESS: Attack rejected. Cross-device authentication works!")

if __name__ == "__main__":
    run_cross_device_test()
</file>

<file path="test_enrollment.py">
#!/usr/bin/env python3
"""Test script for device enrollment and authentication."""

import requests
import json

BASE_URL = "http://localhost:8000"

def test_health():
    """Test server health endpoint."""
    print("Testing /health...")
    response = requests.get(f"{BASE_URL}/health")
    print(f"Status: {response.status_code}")
    print(f"Response: {json.dumps(response.json(), indent=2)}\n")
    return response.status_code == 200

def test_enrollment(device_name, noise_sources, num_samples=20):
    """Test device enrollment."""
    print(f"Enrolling device: {device_name}")
    payload = {
        "device_name": device_name,
        "noise_sources": noise_sources,
        "num_samples": num_samples
    }
    
    response = requests.post(f"{BASE_URL}/enroll", json=payload)
    print(f"Status: {response.status_code}")
    data = response.json()
    print(f"Response: {json.dumps(data, indent=2)}\n")
    return data

def test_list_devices():
    """List all enrolled devices."""
    print("Listing all devices...")
    response = requests.get(f"{BASE_URL}/devices")
    print(f"Status: {response.status_code}")
    data = response.json()
    print(f"Devices: {json.dumps(data, indent=2)}\n")
    return data

def test_authentication(device_id, noise_sources):
    """Test device authentication."""
    print(f"Authenticating device: {device_id}")
    payload = {
        "device_id": device_id,
        "noise_sources": noise_sources
    }
    
    response = requests.post(f"{BASE_URL}/authenticate", json=payload)
    print(f"Status: {response.status_code}")
    data = response.json()
    print(f"Response: {json.dumps(data, indent=2)}\n")
    return data

def test_challenge_response(device_id):
    """Test challenge-response protocol."""
    print(f"Testing challenge-response for device: {device_id}")
    
    # Request challenge
    print("1. Requesting challenge...")
    response = requests.post(f"{BASE_URL}/challenge", json={"device_id": device_id})
    print(f"Status: {response.status_code}")
    challenge_data = response.json()
    print(f"Challenge: {json.dumps(challenge_data, indent=2)}\n")
    
    # Verify response (normally client would sign the nonce with device secret)
    print("2. Verifying response...")
    verify_payload = {
        "challenge_id": challenge_data["challenge_id"],
        "device_id": device_id,
        "noise_samples": [[1, 2, 3, 4, 5] for _ in range(5)]  # Dummy noise samples
    }
    
    response = requests.post(f"{BASE_URL}/verify", json=verify_payload)
    print(f"Status: {response.status_code}")
    verify_data = response.json()
    print(f"Verification: {json.dumps(verify_data, indent=2)}\n")
    return verify_data

if __name__ == "__main__":
    print("=" * 60)
    print("QNA-Auth System Testing")
    print("=" * 60 + "\n")
    
    # Test health
    if not test_health():
        print("Server is not healthy! Exiting.")
        exit(1)
    
    # Test enrollment
    print("=" * 60)
    print("Testing Device Enrollment")
    print("=" * 60 + "\n")
    
    enrollment_result = test_enrollment(
        device_name="TestDevice_QRNG",
        noise_sources=["qrng"],
        num_samples=20
    )
    
    device_id = enrollment_result.get("device_id")
    if not device_id:
        print("Enrollment failed! Exiting.")
        exit(1)
    
    # List devices
    print("=" * 60)
    print("Listing Enrolled Devices")
    print("=" * 60 + "\n")
    test_list_devices()
    
    # Test authentication
    print("=" * 60)
    print("Testing Device Authentication")
    print("=" * 60 + "\n")
    test_authentication(device_id, ["qrng"])
    
    # Test challenge-response
    print("=" * 60)
    print("Testing Challenge-Response Protocol")
    print("=" * 60 + "\n")
    test_challenge_response(device_id)
    
    print("=" * 60)
    print("All tests completed!")
    print("=" * 60)
</file>

<file path="test_hardware.py">
"""
Test hardware access for camera and microphone
"""
import cv2
import sounddevice as sd
import numpy as np

print("=" * 60)
print("Testing Hardware Access")
print("=" * 60)

# Test Camera
print("\n1. Testing Camera...")
try:
    cap = cv2.VideoCapture(0)
    if cap.isOpened():
        ret, frame = cap.read()
        if ret and frame is not None:
            print(f"‚úÖ Camera works! Frame shape: {frame.shape}")
        else:
            print("‚ùå Camera opened but couldn't capture frame")
        cap.release()
    else:
        print("‚ùå Could not open camera")
        print("   Possible reasons:")
        print("   - No camera device connected")
        print("   - Camera is being used by another application")
        print("   - Camera access is blocked by permissions")
except Exception as e:
    print(f"‚ùå Camera error: {e}")

# Test Microphone
print("\n2. Testing Microphone...")
try:
    print("\nAvailable audio devices:")
    devices = sd.query_devices()
    print(devices)
    
    print("\nAttempting to record 1 second...")
    recording = sd.rec(
        int(1.0 * 44100),
        samplerate=44100,
        channels=1,
        dtype='float32'
    )
    sd.wait()
    
    if recording is not None and len(recording) > 0:
        print(f"‚úÖ Microphone works! Recorded {len(recording)} samples")
        print(f"   Audio level: mean={np.mean(np.abs(recording)):.6f}")
    else:
        print("‚ùå Microphone recorded but got empty data")
        
except Exception as e:
    print(f"‚ùå Microphone error: {e}")
    print("   Possible reasons:")
    print("   - No microphone device connected")
    print("   - Microphone access is blocked by permissions")
    print("   - Audio driver issues")

print("\n" + "=" * 60)
print("Hardware Test Complete")
print("=" * 60)
</file>

<file path="verify_cuda.py">
import torch

print(f'PyTorch Version: {torch.__version__}')
print(f'CUDA Available: {torch.cuda.is_available()}')
print(f'CUDA Version: {torch.version.cuda}')
print(f'Device Count: {torch.cuda.device_count()}')

if torch.cuda.is_available():
    print(f'Device Name: {torch.cuda.get_device_name(0)}')
    print(f'Current Device: {torch.cuda.current_device()}')
else:
    print('No CUDA devices available')
</file>

<file path="VISION.md">
# QNA-Auth: Core Innovation Vision

## The fundamental breakthrough

QNA-Auth is **not** a web application. It is a **novel authentication primitive** based on quantum randomness and physics-based device fingerprinting. The web UI and API are merely **delivery mechanisms** - the core innovation is the authentication method itself.

---

## What makes this different from existing auth systems

### Traditional authentication (what we're replacing)
- **Passwords**: Stored secrets, vulnerable to theft/replay
- **OTP/SMS**: Time-based but interceptable
- **Biometrics**: Stored templates, vulnerable to deepfakes
- **Hardware tokens**: Can be cloned with physical access
- **Public key crypto**: Relies on key secrecy, vulnerable to key theft

**Core weakness**: All rely on **static secrets** or **reproducible data** that can be stolen, copied, or synthesized.

### QNA-Auth's approach (the innovation)
1. **Physics-based entropy**: Uses quantum randomness that cannot be predicted or reproduced
2. **Non-invertible embeddings**: Never stores raw biometric data, only learned representations
3. **Freshness guarantee**: Every authentication uses new, unpredictable quantum noise
4. **Hardware PUF-like behavior**: Leverages unique physical characteristics of each device
5. **AI-validated**: ML model learns the "signature" of device-specific randomness patterns

**Core strength**: Authentication is based on **physical reality**, not stored secrets. An attacker would need to:
- Replicate the exact quantum state (impossible)
- Predict future quantum measurements (impossible)
- Clone the device's physical noise characteristics (extremely difficult)
- Invert the neural network embedding (cryptographically hard)

---

## The science behind it

### 1. Quantum randomness as a fingerprint

**Key insight**: While quantum noise is random, the *way* a specific device generates and processes that noise has subtle, learnable patterns.

**Sources of device-specific quantum entropy**:
- **QRNG API response processing**: Network latency patterns, parsing behavior
- **Camera sensor noise**: Dark current patterns, pixel defects, read-out timing
- **Microphone noise**: Self-noise characteristics, frequency response curves
- **System timing jitter**: CPU scheduling, interrupt handling, memory access patterns

**Why this works**:
```
Device A's quantum samples ‚â† Device B's quantum samples (obvious)
BUT ALSO:
Statistical properties of A's noise ‚â† Statistical properties of B's noise (learnable!)
```

### 2. Embedding space as a device manifold

**Mathematical formulation**:
- Let D = {d‚ÇÅ, d‚ÇÇ, ..., d‚Çô} be a set of devices
- Let Q(d·µ¢) = quantum noise samples from device d·µ¢
- Let F(Q) = feature extraction (entropy, FFT, autocorrelation, etc.)
- Let E(F) = embedding network: F ‚Üí ‚Ñù·µà (d-dimensional embedding space)

**Goal**: Learn E such that:
1. **Intra-device similarity**: ||E(Q‚ÇÅ(d·µ¢)) - E(Q‚ÇÇ(d·µ¢))|| is small (same device, different samples)
2. **Inter-device separation**: ||E(Q(d·µ¢)) - E(Q(d‚±º))|| is large (different devices)

**Why Siamese networks**:
- Learns a metric space where "same device" samples cluster
- Doesn't require pre-defined categories (unsupervised on device identity)
- Generalizes to new devices (few-shot learning from enrollment samples)

### 3. Non-invertibility guarantee

**Attack model**: Adversary obtains the stored embedding E(Q(d))

**What they cannot do**:
1. **Recover raw noise**: Q(d) = E‚Åª¬π(embedding) is infeasible (many-to-one mapping + ReLU non-linearity)
2. **Generate valid noise**: Synthesizing Q' such that E(Q') ‚âà E(Q(d)) requires solving a high-dimensional optimization with no gradient access
3. **Replay old samples**: Challenge-response ensures freshness; stored embedding is useless without current device

**Security reduction**:
```
Breaking QNA-Auth ‚â§ Breaking one of:
1. Quantum unpredictability (physically impossible)
2. Neural network inversion (computationally hard)
3. Challenge-response freshness (cryptographically secure)
```

---

## Core research questions (what we need to prove)

### 1. Device distinguishability
**Question**: Can we reliably distinguish devices based on quantum noise statistics?

**Experiments needed**:
- Collect 1000+ samples from 100+ devices
- Measure intra-device vs. inter-device similarity distributions
- Compute separability metrics (d-prime, ROC-AUC)
- Test across different device classes (phones, laptops, IoT)

**Success criteria**: Clear bimodal distribution (intra-device high similarity, inter-device low similarity)

### 2. Stability over time
**Question**: Do device noise characteristics remain stable across days/weeks/months?

**Experiments needed**:
- Enroll devices and re-authenticate daily for 90 days
- Measure similarity drift over time
- Identify environmental factors (temperature, battery level) that affect stability
- Develop drift compensation strategies

**Success criteria**: <5% accuracy degradation over 90 days without re-enrollment

### 3. Resistance to cloning
**Question**: Can an attacker with physical access clone the authentication?

**Attack scenarios**:
- **Scenario 1**: Attacker records 1000 authentication sessions ‚Üí tries to synthesize new samples
- **Scenario 2**: Attacker has identical hardware ‚Üí can they enroll as the same "device"?
- **Scenario 3**: Attacker extracts embedding ‚Üí tries to generate matching noise

**Success criteria**: All attacks fail with >99.9% probability

### 4. Minimal sample requirements
**Question**: How few samples do we need for reliable enrollment and authentication?

**Experiments**:
- Vary enrollment samples: 10, 25, 50, 100, 200
- Vary authentication samples: 1, 3, 5, 10
- Measure FAR/FRR trade-offs for each configuration

**Success criteria**: <1% error rate with ‚â§50 enrollment samples and ‚â§5 auth samples

---

## Research priorities (reordered for quantum auth focus)

### Priority 1: Core authentication mechanism ‚≠ê‚≠ê‚≠ê
1. **Quantum noise characterization**
   - Analyze statistical properties of QRNG, camera, microphone noise
   - Identify device-specific features in noise distributions
   - Publish findings on entropy sources and their uniqueness

2. **Feature engineering for device fingerprinting**
   - Beyond basic statistics: wavelet analysis, permutation entropy, fractal dimension
   - Time-series features: long-range correlations, Hurst exponent refinement
   - Multi-scale analysis: coarse-graining to extract stable patterns

3. **Embedding model optimization**
   - Architecture search for optimal encoding capacity
   - Loss function design: triplet vs. contrastive vs. ArcFace
   - Regularization for robustness (adversarial training, data augmentation)

### Priority 2: Security & cryptographic analysis ‚≠ê‚≠ê‚≠ê
1. **Formal security model**
   - Define threat model (Dolev-Yao adversary with physical/computational bounds)
   - Prove security reduction to quantum unpredictability + computational hardness
   - Analyze attack surfaces and vulnerabilities

2. **Challenge-response protocol**
   - Design provably secure nonce generation
   - Bind challenges to device embeddings cryptographically
   - Implement mutual authentication (server proves identity to device)

3. **Anti-replay and anti-cloning**
   - Cryptographic accumulator for used challenges
   - Device-specific counters (prevent rollback attacks)
   - Liveness detection mechanisms

### Priority 3: Experimental validation ‚≠ê‚≠ê
1. **Large-scale device study**
   - Enroll 100+ diverse devices (phones, laptops, IoT, servers)
   - Collect longitudinal data (daily auth for 90+ days)
   - Measure real-world FAR/FRR, stability, and usability

2. **Attack simulation**
   - White-box attacks (adversary has model + embeddings)
   - Replay attacks (recorded sessions)
   - Cloning attacks (identical hardware)
   - Synthetic generation (GAN-based noise synthesis)

3. **Comparative benchmarking**
   - Compare to existing methods (FIDO2, TPM, biometrics)
   - Measure security, usability, cost, and deployment complexity
   - Publish results in academic venue

### Priority 4: Implementation & optimization ‚≠ê
1. **Model deployment**
   - ONNX conversion for cross-platform inference
   - Quantization for edge devices (INT8, binary networks)
   - Hardware acceleration (GPU, NPU, specialized chips)

2. **Noise collection efficiency**
   - Minimize sampling time while maintaining security
   - Adaptive sampling (collect more if confidence is low)
   - Multi-source fusion strategies

3. **Fallback and degradation**
   - Graceful degradation when fewer sources available
   - Adaptive thresholds based on enrollment quality
   - Re-enrollment triggers (detect significant drift)

### Priority 5: Delivery mechanisms (web/API) ‚≠ê
1. **API design** (minimal, focused on auth primitive)
2. **Reference implementation** (demo, not production app)
3. **Integration examples** (how to use QNA-Auth in existing systems)

**Note**: The web UI is a **demonstration tool**, not the core deliverable. Priority is proving the authentication method works, not building a polished web app.

---

## What success looks like

### Academic success
- ‚úÖ **Published paper** in security/crypto conference (IEEE S&P, CCS, NDSS)
- ‚úÖ **Novel contribution**: First practical physics-based auth using quantum noise + ML
- ‚úÖ **Rigorous evaluation**: 100+ devices, 90+ days, <1% error rates
- ‚úÖ **Security analysis**: Formal model + attack simulations

### Technical success
- ‚úÖ **Working prototype** that reliably authenticates devices
- ‚úÖ **Open-source reference implementation** for researchers
- ‚úÖ **Reproducible results** (documented experiments, public datasets)
- ‚úÖ **Patent filed** (optional, if novel claims are patentable)

### Industry adoption potential
- ‚úÖ **Standards proposal** (e.g., W3C WebAuthn extension)
- ‚úÖ **Hardware vendor interest** (integrate into TPMs, secure enclaves)
- ‚úÖ **Real-world pilot** (banking, IoT, automotive)

---

## Immediate next steps (research-focused)

### Step 1: Data collection campaign (Week 1-2)
```bash
# Collect comprehensive device corpus
python research/collect_device_corpus.py \
    --devices 50 \
    --samples-per-device 200 \
    --sources qrng,camera,microphone,sensors \
    --daily-reauth 90
```

### Step 2: Statistical analysis (Week 2-3)
```python
# research/analyze_device_signatures.py
- Compute intra-device vs. inter-device distance distributions
- Test for device distinguishability (hypothesis testing)
- Identify most discriminative features
- Visualize embedding space (t-SNE, UMAP)
```

### Step 3: Model training & evaluation (Week 3-5)
```python
# research/train_optimal_model.py
- Grid search over architectures (embedding dim, hidden layers, activations)
- Compare loss functions (triplet, contrastive, ArcFace, CosFace)
- K-fold cross-validation (per-device splits)
- Report ROC curves, FAR/FRR at multiple thresholds
```

### Step 4: Security analysis (Week 5-6)
```python
# research/attack_simulations.py
- Replay attack (use old samples)
- Synthesis attack (GAN-generated noise)
- Cloning attack (identical hardware)
- Model inversion attack (recover noise from embedding)
```

### Step 5: Write paper (Week 6-8)
```
paper/
  qna_auth.tex
  figures/
  tables/
  references.bib
```

Sections:
1. Abstract
2. Introduction (motivation, threat model, contributions)
3. Related Work (biometrics, PUFs, quantum crypto, ML security)
4. Method (architecture, training, protocols)
5. Evaluation (devices, accuracy, stability, attacks)
6. Discussion (limitations, future work)
7. Conclusion

---

## What to de-emphasize (web dev stuff)

### Don't focus on:
- ‚ùå React UI polish (styling, UX, animations)
- ‚ùå Full CRUD operations (device management is secondary)
- ‚ùå User accounts and multi-tenancy
- ‚ùå Production API scaling (horizontal scaling, load balancing)
- ‚ùå Frontend/backend separation (monolithic research prototype is fine)

### Keep minimal:
- ‚úÖ Simple Flask/FastAPI endpoint for enrollment + auth
- ‚úÖ Jupyter notebook for demos and visualizations
- ‚úÖ Command-line tools for experiments
- ‚úÖ Docker for reproducibility (not for production deployment)

---

## Reframing the project

| What it **was** | What it **should be** |
|----------------|---------------------|
| Full-stack web app | Research prototype + paper |
| Production-ready service | Proof-of-concept implementation |
| General auth platform | Novel quantum auth primitive |
| API with many features | Core enroll/auth methods only |
| React frontend | Jupyter notebook demos |
| User management | Device corpus management |
| Deployment & scaling | Reproducible experiments |

---

## Revised file structure (research-focused)

```
QNA-Auth/
  research/                        ‚Üê Core research code
    collect_device_corpus.py       ‚Üê Data collection
    analyze_signatures.py          ‚Üê Statistical analysis
    train_models.py                ‚Üê Model training
    evaluate_security.py           ‚Üê Attack simulations
    visualize_embeddings.py        ‚Üê t-SNE, UMAP plots
    
  experiments/                     ‚Üê Experiment configs
    exp001_baseline.yaml           ‚Üê 50 devices, triplet loss
    exp002_contrastive.yaml        ‚Üê Compare loss functions
    exp003_feature_ablation.yaml   ‚Üê Which features matter?
    
  data/                            ‚Üê Device corpus (gitignored)
    device_001/
      qrng_samples.npy
      camera_samples.npy
      metadata.json
    device_002/
    ...
    
  notebooks/                       ‚Üê Jupyter notebooks
    01_data_exploration.ipynb      ‚Üê EDA on noise samples
    02_feature_importance.ipynb    ‚Üê Which features distinguish devices?
    03_model_comparison.ipynb      ‚Üê Visualize results
    04_attack_analysis.ipynb       ‚Üê Security evaluation
    
  paper/                           ‚Üê Academic paper
    qna_auth.tex
    figures/
    tables/
    
  qna_auth/                        ‚Üê Core library (not a web app!)
    noise/                         ‚Üê Noise collection
    features/                      ‚Üê Feature extraction
    models/                        ‚Üê ML models
    crypto/                        ‚Üê Challenge-response
    
  demo/                            ‚Üê Minimal demo (optional)
    simple_api.py                  ‚Üê Flask endpoint (50 lines)
    demo.html                      ‚Üê Single-page demo
    
  tests/                           ‚Üê Unit tests
    test_noise.py
    test_features.py
    test_models.py
    test_crypto.py
```

**Key changes**:
- `research/` is the main folder (not `server/`)
- `experiments/` for reproducible configs
- `notebooks/` for analysis and visualization
- `paper/` for academic publication
- `demo/` is minimal (not `frontend/` with full React app)

---

## The core innovation (to emphasize in all docs)

### The novel claim
> "We demonstrate the first practical authentication system that derives non-invertible device fingerprints from quantum randomness, achieving >99% accuracy with provable resistance to replay and cloning attacks."

### Why it matters
- **Physics meets AI**: Combines quantum unpredictability with machine learning
- **No stored secrets**: Unlike passwords, keys, or biometric templates
- **Freshness by design**: Every auth uses new quantum measurements
- **Hardware-agnostic**: Works on commodity devices (phones, laptops, IoT)

### What we're NOT claiming
- ‚ùå "A better web authentication API" (that's OAuth2, FIDO2)
- ‚ùå "A faster biometric system" (that's Face ID, Touch ID)
- ‚ùå "A new encryption algorithm" (that's post-quantum crypto)

### What we ARE claiming
- ‚úÖ "A fundamentally new authentication primitive"
- ‚úÖ "Quantum randomness as a device fingerprint"
- ‚úÖ "ML-learned representation of physical entropy"
- ‚úÖ "Provably secure against replay and synthetic attacks"

---

## Conclusion

**QNA-Auth is a research project, not a product.**

The goal is to:
1. Prove that quantum noise can reliably fingerprint devices
2. Demonstrate security against realistic attacks
3. Publish findings in a top-tier venue
4. Provide open-source reference implementation

The web UI and API are **tools for evaluation**, not the end goal. Focus should be on the scientific contribution, not the software engineering.

**Next action**: Start the data collection campaign and statistical analysis. Prove the core hypothesis before optimizing the implementation.
</file>

<file path=".gitignore">
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
env/
ENV/
.venv

# IDE
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Jupyter Notebook
.ipynb_checkpoints

# PyTorch
*.pt
*.pth
*.onnx
checkpoints/

# Data and Models
dataset/samples/*.npy
dataset/samples/*.csv
dataset/samples/json/
dataset/training_data/
auth/device_embeddings/
auth/test_embeddings/
model/checkpoints/
model/test_checkpoints/
model/evaluation/

# Logs
*.log
logs/

# Environment variables
.env
.env.local

# OS
Thumbs.db
.DS_Store

# Frontend
frontend/node_modules/
frontend/build/
frontend/dist/
frontend/.next/
frontend/*.tsbuildinfo

# Testing
.pytest_cache/
.coverage
htmlcov/
.tox/

# Temporary files
*.tmp
temp/
tmp/

# Local config and temp files
config.py
server/models/
</file>

<file path="auth/authentication.py">
"""
Device Authentication Module
Authenticates devices using noise samples and embeddings
"""

import torch
import numpy as np
import json
from typing import Optional, Dict, Tuple, List
import logging
from datetime import datetime

from model.siamese_model import DeviceEmbedder
from preprocessing.features import NoisePreprocessor, FeatureVector
from noise_collection import QRNGClient, CameraNoiseCollector, MicrophoneNoiseCollector
from .enrollment import DeviceEnroller

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DeviceAuthenticator:
    """Handles device authentication"""
    
    def __init__(
        self,
        embedder: DeviceEmbedder,
        preprocessor: NoisePreprocessor,
        feature_converter: FeatureVector,
        enroller: DeviceEnroller,
        threshold: float = 0.85,
        metric: str = 'cosine'
    ):
        """
        Initialize device authenticator
        
        Args:
            embedder: Trained DeviceEmbedder
            preprocessor: Noise preprocessor
            feature_converter: Feature vector converter
            enroller: DeviceEnroller for loading stored embeddings
            threshold: Authentication threshold
            metric: Similarity metric ('cosine' or 'euclidean')
        """
        self.embedder = embedder
        self.preprocessor = preprocessor
        self.feature_converter = feature_converter
        self.enroller = enroller
        self.threshold = threshold
        self.metric = metric
        
        logger.info(f"DeviceAuthenticator initialized (threshold={threshold}, metric={metric})")
    
    def collect_authentication_sample(
        self,
        source: str = 'qrng',
        num_samples: int = 5
    ) -> Optional[np.ndarray]:
        """
        Collect fresh noise sample for authentication
        
        Args:
            source: Noise source to use
            num_samples: Number of samples to collect
            
        Returns:
            Noise array or None if collection failed
        """
        try:
            if source == 'qrng':
                qrng_client = QRNGClient()
                samples = qrng_client.fetch_multiple_samples(
                    num_samples=num_samples,
                    sample_size=1024
                )
                if samples:
                    # Concatenate or average samples
                    return np.mean(np.array(samples), axis=0)
            
            elif source == 'camera':
                camera_collector = CameraNoiseCollector(camera_index=0)
                if camera_collector.initialize_camera():
                    frame = camera_collector.capture_dark_frame(exposure_time=0.2)
                    camera_collector.release()
                    if frame is not None:
                        return camera_collector.extract_noise_features(frame)
            
            elif source == 'microphone':
                mic_collector = MicrophoneNoiseCollector(sample_rate=44100)
                audio = mic_collector.capture_ambient_noise(duration=1.0)
                return audio
            
        except Exception as e:
            logger.error(f"Failed to collect authentication sample from {source}: {e}")
        
        return None
    
    def generate_authentication_embedding(
        self,
        noise_samples: list
    ) -> Optional[torch.Tensor]:
        """
        Generate embedding from fresh noise samples
        
        Args:
            noise_samples: List of noise arrays
            
        Returns:
            Authentication embedding or None
        """
        try:
            feature_vectors = []
            
            for sample in noise_samples:
                # --- SILENCE DETECTION ---
                # Check for absolute silence (all zeros) or near silence
                amplitude = np.max(np.abs(sample))
                rms = np.sqrt(np.mean(sample**2))
                logger.info(f"DEBUG: Sample Stats - Max Amp: {amplitude:.6f}, RMS: {rms:.6f}")
                
                if rms < 0.0001:  # Threshold for digital silence/near silence
                    logger.error("CRITICAL: Detected SILENCE in audio sample. Rejecting.")
                    raise ValueError("Audio sample is silent (RMS < 0.0001). Check microphone inputs.")

                # Extract features
                features = self.preprocessor.extract_all_features(sample)
                logger.info(f"DEBUG: Features Snapshot: Mean={features['mean']:.4f}, Std={features['std']:.4f}, SpectralEntropy={features['spectral_entropy']:.4f}, ShannonEntropy={features.get('shannon_entropy', 0):.4f}")

                # Convert to vector
                feature_vector = self.feature_converter.to_vector(features)
                logger.info(f"DEBUG: Feature Vector (First 5): {feature_vector[:5]}")
                feature_vectors.append(feature_vector)
            
            # Use explicit self.feature_converter.feature_names to ensure we know what we are looking at
            if self.feature_converter.feature_names:
                 logger.info(f"DEBUG: Feature Names: {self.feature_converter.feature_names[:10]}...")

            # Generate embeddings
            embeddings = []
            for fv in feature_vectors:
                fv_tensor = torch.from_numpy(fv).float()
                embedding = self.embedder.embed(fv_tensor)
                
                # --- DEBUG: Print Raw Embedding ---
                logger.info(f"DEBUG: Raw Embedding Preview (First 10): {embedding.detach().numpy()[:10]}")
                # ----------------------------------

                embeddings.append(embedding)
            
            # Average embeddings
            auth_embedding = torch.mean(torch.stack(embeddings), dim=0)
            
            # Normalize
            auth_embedding = torch.nn.functional.normalize(auth_embedding, p=2, dim=0)
            
            return auth_embedding
            
        except Exception as e:
            logger.error(f"Failed to generate authentication embedding: {e}")
            return None
    
    def verify_device(
        self,
        device_id: str,
        auth_embedding: torch.Tensor
    ) -> Tuple[bool, float, Dict]:
        """
        Verify device by comparing embeddings
        
        Args:
            device_id: Device identifier to verify
            auth_embedding: Fresh authentication embedding
            
        Returns:
            Tuple of (is_authenticated, similarity_score, details)
        """
        # Load stored device embedding
        stored_embedding = self.enroller.load_device_embedding(device_id)
        
        if stored_embedding is None:
            return False, 0.0, {'error': 'Device not enrolled'}
        
        # --- DEBUG: Compare Embeddings ---
        logger.info(f"DEBUG: Auth Embedding (First 10): {auth_embedding.detach().cpu().numpy()[:10]}")
        logger.info(f"DEBUG: Stored Embedding (First 10): {stored_embedding.detach().cpu().numpy()[:10]}")
        # ---------------------------------

        # Compute similarity
        similarity = self.embedder.compute_similarity(
            auth_embedding,
            stored_embedding,
            metric=self.metric
        )
        
        # Make decision
        is_authenticated = similarity >= self.threshold
        
        details = {
            'device_id': device_id,
            'similarity': similarity,
            'threshold': self.threshold,
            'metric': self.metric,
            'timestamp': datetime.now().isoformat(),
            'authenticated': is_authenticated
        }
        
        logger.info(f"Verification: device={device_id}, similarity={similarity:.4f}, "
                   f"authenticated={is_authenticated}")
        
        return is_authenticated, similarity, details
    
    def authenticate(
        self,
        device_id: str,
        sources: list = ['qrng'],
        num_samples_per_source: int = 5,
        client_samples: Optional[Dict[str, List[List[float]]]] = None
    ) -> Tuple[bool, Dict]:
        """
        Complete authentication flow
        
        Args:
            device_id: Device identifier to authenticate
            sources: List of noise sources to use
            num_samples_per_source: Number of samples per source
            client_samples: Optional raw noise samples provided by client
            
        Returns:
            Tuple of (is_authenticated, details_dict)
        """
        logger.info(f"Starting authentication for device: {device_id}")
        
        # --- SOURCE MISMATCH CHECK ---
        try:
            metadata_path = self.enroller.storage_dir / f"{device_id}_metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                
                enrolled_sources = set(metadata.get('sources', []))
                requested_sources = set(sources)
                
                # Check if we are trying to use a source that wasn't enrolled
                # e.g. Trying to use Camera when we only enrolled Microphone
                invalid_sources = requested_sources - enrolled_sources
                
                # Special case: Ignore 'qrng' in mismatch check if it wasn't explicitly requested 
                # but might be default. But here 'sources' IS what is requested.
                
                if invalid_sources:
                    error_msg = f"Security Alert: Source Mismatch! Device enrolled with {list(enrolled_sources)}, but authentication requested {list(requested_sources)}."
                    logger.warning(error_msg)
                    return False, {'error': error_msg, 'details': 'Invalid Hardware Source'}
        except Exception as e:
            logger.error(f"Metadata verification failed: {e}")
            # We continue cautiously or fail? Let's fail secure.
            # return False, {'error': 'Metadata verification failed'}
            pass 

        # Collect noise samples
        noise_samples = []
        
        if client_samples:
            logger.info("============== DEBUG: CLIENT SAMPLES RECEIVED ==============")
            logger.info(f"Sources preset in client_samples: {list(client_samples.keys())}")
            for src, samples in client_samples.items():
                logger.info(f"Source: {src}, Count: {len(samples)}")
                if samples and len(samples) > 0:
                    arr = np.array(samples[0])
                    logger.info(f"Sample 0 Stats - Shape: {arr.shape}, Mean: {np.mean(arr):.4f}, Std: {np.std(arr):.4f}")
            logger.info("==========================================================")

            logger.info(f"Using client-provided samples for authentication. Sources: {list(client_samples.keys())}")
            for source, samples_list in client_samples.items():
                if source in sources:
                    for s in samples_list:
                        noise_samples.append(np.array(s))
        else:
            logger.warning("============== DEBUG: NO CLIENT SAMPLES ==============")
            logger.warning("FALLING BACK TO SERVER LOCAL HARDWARE")
            logger.warning("====================================================")
            for source in sources:
                logger.info(f"Collecting samples from {source}...")
                for _ in range(num_samples_per_source):
                    sample = self.collect_authentication_sample(source, num_samples=1)
                    if sample is not None:
                        noise_samples.append(sample)
        
        if not noise_samples:
            return False, {'error': 'Failed to collect noise samples'}
        
        logger.info(f"Collected {len(noise_samples)} noise samples")
        
        # Generate authentication embedding
        auth_embedding = self.generate_authentication_embedding(noise_samples)
        
        if auth_embedding is None:
            return False, {'error': 'Failed to generate authentication embedding'}

        # DEBUG: Print the first few values of the embedding to check for "Silence/Constant" issues
        log_embed = auth_embedding.detach().cpu().numpy().flatten()[:8]
        logger.info(f"DEBUG: Auth Embedding First 8 Values: {log_embed}")
        
        # Verify device
        is_authenticated, similarity, details = self.verify_device(
            device_id,
            auth_embedding
        )
        
        details['num_samples_collected'] = len(noise_samples)
        details['sources_used'] = sources
        
        return is_authenticated, details
    
    def identify_device(
        self,
        auth_embedding: torch.Tensor,
        top_k: int = 5
    ) -> list:
        """
        Identify device by finding closest matches (1:N matching)
        
        Args:
            auth_embedding: Authentication embedding
            top_k: Number of top matches to return
            
        Returns:
            List of (device_id, similarity_score) tuples
        """
        enrolled_devices = self.enroller.list_enrolled_devices()
        
        if not enrolled_devices:
            logger.warning("No enrolled devices found")
            return []
        
        # Compute similarities with all enrolled devices
        similarities = []
        
        for device_id in enrolled_devices:
            stored_embedding = self.enroller.load_device_embedding(device_id)
            if stored_embedding is not None:
                similarity = self.embedder.compute_similarity(
                    auth_embedding,
                    stored_embedding,
                    metric=self.metric
                )
                similarities.append((device_id, similarity))
        
        # Sort by similarity (descending)
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # Return top-k
        top_matches = similarities[:top_k]
        
        logger.info(f"Identification: top match = {top_matches[0][0]} "
                   f"(similarity={top_matches[0][1]:.4f})")
        
        return top_matches


class AuthenticationSession:
    """Manages an authentication session with retry logic"""
    
    def __init__(
        self,
        authenticator: DeviceAuthenticator,
        max_attempts: int = 3
    ):
        """
        Initialize authentication session
        
        Args:
            authenticator: DeviceAuthenticator instance
            max_attempts: Maximum authentication attempts
        """
        self.authenticator = authenticator
        self.max_attempts = max_attempts
        self.attempts = 0
        self.session_log = []
        
    def attempt_authentication(
        self,
        device_id: str,
        sources: list = ['qrng']
    ) -> Tuple[bool, Dict]:
        """
        Attempt authentication with retry logic
        
        Args:
            device_id: Device identifier
            sources: Noise sources to use
            
        Returns:
            Tuple of (is_authenticated, session_details)
        """
        self.attempts += 1
        
        if self.attempts > self.max_attempts:
            return False, {
                'error': 'Maximum authentication attempts exceeded',
                'attempts': self.attempts,
                'session_log': self.session_log
            }
        
        # Authenticate
        is_authenticated, details = self.authenticator.authenticate(
            device_id,
            sources=sources
        )
        
        # Log attempt
        attempt_log = {
            'attempt': self.attempts,
            'timestamp': datetime.now().isoformat(),
            'authenticated': is_authenticated,
            'details': details
        }
        self.session_log.append(attempt_log)
        
        if is_authenticated:
            details['session_log'] = self.session_log
            return True, details
        
        # Retry logic
        if self.attempts < self.max_attempts:
            logger.info(f"Authentication failed. Attempt {self.attempts}/{self.max_attempts}")
            return False, {'retry_available': True, 'attempts': self.attempts}
        else:
            logger.warning("Maximum authentication attempts reached")
            return False, {
                'error': 'Authentication failed after maximum attempts',
                'session_log': self.session_log
            }


def main():
    """Test authentication module"""
    print("\n=== Device Authentication Test ===")
    
    # Create mock components
    from model.siamese_model import SiameseNetwork
    
    input_dim = 50
    embedding_dim = 128
    
    # Create model and embedder
    model = SiameseNetwork(input_dim=input_dim, embedding_dim=embedding_dim)
    embedder = DeviceEmbedder(input_dim=input_dim, embedding_dim=embedding_dim)
    embedder.model = model
    
    # Create preprocessor and feature converter
    preprocessor = NoisePreprocessor(normalize=True)
    feature_converter = FeatureVector()
    
    # Create enroller
    enroller = DeviceEnroller(
        embedder=embedder,
        preprocessor=preprocessor,
        feature_converter=feature_converter,
        storage_dir="./auth/test_embeddings"
    )
    
    # Create authenticator
    authenticator = DeviceAuthenticator(
        embedder=embedder,
        preprocessor=preprocessor,
        feature_converter=feature_converter,
        enroller=enroller,
        threshold=0.8
    )
    
    print("\n=== Simulating Device Enrollment ===")
    # Simulate enrolling a device
    simulated_noise_enroll = {
        'qrng': [np.random.rand(1024) for _ in range(10)]
    }
    feature_vectors = enroller.process_noise_to_features(simulated_noise_enroll)
    device_embedding = enroller.create_device_embedding(feature_vectors)
    test_device_id = enroller.generate_device_id("TestDevice")
    enroller.save_device_embedding(test_device_id, device_embedding)
    print(f"Enrolled device: {test_device_id}")
    
    print("\n=== Simulating Authentication ===")
    # Simulate authentication with similar noise (should succeed)
    simulated_noise_auth = [np.random.rand(1024) for _ in range(5)]
    auth_embedding = authenticator.generate_authentication_embedding(simulated_noise_auth)
    
    if auth_embedding is not None:
        is_auth, similarity, details = authenticator.verify_device(test_device_id, auth_embedding)
        print(f"Authentication result: {is_auth}")
        print(f"Similarity: {similarity:.4f}")
        print(f"Details: {details}")
    
    print("\n=== Testing Authentication Session ===")
    session = AuthenticationSession(authenticator, max_attempts=3)
    # This will likely fail with random noise, but demonstrates the API
    is_auth, session_details = session.attempt_authentication(test_device_id, sources=['qrng'])
    print(f"Session authenticated: {is_auth}")
    print(f"Session attempts: {session.attempts}")


if __name__ == "__main__":
    main()
</file>

<file path="auth/enrollment.py">
"""
Device Enrollment Module
Collects noise samples and creates device embeddings
"""

import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional
import json
import hashlib
from datetime import datetime
import logging

from model.siamese_model import DeviceEmbedder
from preprocessing.features import NoisePreprocessor, FeatureVector
from noise_collection import QRNGClient, CameraNoiseCollector, MicrophoneNoiseCollector

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DeviceEnroller:
    """Handles device enrollment process"""
    
    def __init__(
        self,
        embedder: DeviceEmbedder,
        preprocessor: NoisePreprocessor,
        feature_converter: FeatureVector,
        storage_dir: str = "./auth/device_embeddings",
        dataset_builder: Optional[object] = None
    ):
        """
        Initialize device enroller
        
        Args:
            embedder: Trained DeviceEmbedder
            preprocessor: Noise preprocessor
            feature_converter: Feature vector converter
            storage_dir: Directory to store device embeddings
            dataset_builder: Optional dataset builder for saving raw training data
        """
        self.embedder = embedder
        self.preprocessor = preprocessor
        self.feature_converter = feature_converter
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        self.dataset_builder = dataset_builder
        
        logger.info("DeviceEnroller initialized")
    
    def generate_device_id(self, device_name: Optional[str] = None) -> str:
        """
        Generate unique device ID
        
        Args:
            device_name: Optional device name
            
        Returns:
            Unique device ID
        """
        timestamp = datetime.now().isoformat()
        
        if device_name:
            identifier = f"{device_name}_{timestamp}"
        else:
            identifier = timestamp
        
        # Generate hash-based ID
        device_id = hashlib.sha256(identifier.encode()).hexdigest()[:16]
        
        return device_id
    
    def collect_noise_samples(
        self,
        num_samples: int = 50,
        sources: List[str] = ['qrng', 'camera', 'microphone']
    ) -> Dict[str, List[np.ndarray]]:
        """
        Collect noise samples from multiple sources
        
        Args:
            num_samples: Number of samples to collect per source
            sources: List of noise sources to use
            
        Returns:
            Dictionary mapping source name to list of noise arrays
        """
        noise_samples = {}
        
        # QRNG samples (with API key for authentic quantum noise)
        if 'qrng' in sources:
            logger.info(f"Collecting {num_samples} QRNG samples...")
            try:
                # Use API key from environment or hardcoded
                import os
                api_key = os.getenv('QRNG_API_KEY', 'qnrk_e07be9c8e69b605ab33d13c241fc7f5b027de62cbd592c2c')
                qrng_client = QRNGClient(api_key=api_key)
                samples = qrng_client.fetch_multiple_samples(
                    num_samples=num_samples,
                    sample_size=1024
                )
                noise_samples['qrng'] = samples
                logger.info(f"Collected {len(samples)} authentic QRNG samples")
            except Exception as e:
                logger.error(f"Failed to collect QRNG samples: {e}")
        
        # Camera noise samples
        if 'camera' in sources:
            logger.info(f"Collecting {num_samples} camera noise samples...")
            try:
                camera_collector = CameraNoiseCollector(camera_index=0)
                if camera_collector.initialize_camera():
                    logger.info("Camera initialized successfully")
                    frames = camera_collector.capture_multiple_frames(
                        num_frames=num_samples,
                        exposure_time=0.1
                    )
                    logger.info(f"Captured {len(frames)} raw frames")
                    
                    # Extract noise features from frames
                    samples = [camera_collector.extract_noise_features(frame) 
                              for frame in frames if frame is not None]
                    noise_samples['camera'] = samples
                    camera_collector.release()
                    logger.info(f"Collected {len(samples)} camera samples with shapes: {[s.shape for s in samples[:3]]}")
                else:
                    logger.error("Failed to initialize camera")
            except Exception as e:
                logger.error(f"Failed to collect camera samples: {e}", exc_info=True)
        
        # Microphone noise samples
        if 'microphone' in sources:
            logger.info(f"Collecting {num_samples} microphone noise samples...")
            try:
                mic_collector = MicrophoneNoiseCollector(sample_rate=44100)
                logger.info("Microphone collector initialized")
                samples = mic_collector.capture_multiple_samples(
                    num_samples=num_samples,
                    duration=0.5
                )
                noise_samples['microphone'] = samples
                logger.info(f"Collected {len(samples)} microphone samples with shapes: {[s.shape for s in samples[:3]]}")
            except Exception as e:
                logger.error(f"Failed to collect microphone samples: {e}", exc_info=True)
        
        return noise_samples
    
    def process_noise_to_features(
        self,
        noise_samples: Dict[str, List[np.ndarray]]
    ) -> List[np.ndarray]:
        """
        Process noise samples into feature vectors
        
        Args:
            noise_samples: Dictionary of noise samples by source
            
        Returns:
            List of feature vectors
        """
        feature_vectors = []
        
        for source, samples in noise_samples.items():
            logger.info(f"Processing {len(samples)} samples from {source}...")
            
            for sample in samples:
                try:
                    # Extract features
                    features = self.preprocessor.extract_all_features(sample)
                    
                    # Convert to vector
                    feature_vector = self.feature_converter.to_vector(features)
                    feature_vectors.append(feature_vector)
                    
                except Exception as e:
                    logger.warning(f"Failed to process sample: {e}")
                    continue
        
        logger.info(f"Processed {len(feature_vectors)} feature vectors")
        return feature_vectors
    
    def create_device_embedding(
        self,
        feature_vectors: List[np.ndarray],
        method: str = 'mean'
    ) -> torch.Tensor:
        """
        Create device embedding from multiple feature vectors
        
        Args:
            feature_vectors: List of feature vectors
            method: Aggregation method ('mean', 'median', 'concat')
            
        Returns:
            Device embedding tensor
        """
        if not feature_vectors:
            raise ValueError("No feature vectors provided")
        
        # Generate embeddings for each feature vector
        embeddings = []
        for fv in feature_vectors:
            # Create tensor directly on the embedder's device to avoid CPU->CUDA transfer
            fv_tensor = torch.from_numpy(fv).float().to(self.embedder.device)
            embedding = self.embedder.embed(fv_tensor)
            embeddings.append(embedding)
        
        # Stack embeddings
        embedding_stack = torch.stack(embeddings)
        
        # Aggregate
        if method == 'mean':
            device_embedding = torch.mean(embedding_stack, dim=0)
        elif method == 'median':
            device_embedding = torch.median(embedding_stack, dim=0).values
        elif method == 'concat':
            # Take first N embeddings and concatenate (not recommended for large N)
            device_embedding = embedding_stack[:min(5, len(embeddings))].flatten()
        else:
            raise ValueError(f"Unknown aggregation method: {method}")
        
        # Normalize
        device_embedding = torch.nn.functional.normalize(device_embedding, p=2, dim=0)
        
        logger.info(f"Created device embedding with shape {device_embedding.shape}")
        return device_embedding
    
    def save_device_embedding(
        self,
        device_id: str,
        embedding: torch.Tensor,
        metadata: Optional[Dict] = None
    ):
        """
        Save device embedding to storage
        
        Args:
            device_id: Unique device identifier
            embedding: Device embedding tensor
            metadata: Optional metadata dictionary
        """
        # Save embedding
        embedding_path = self.storage_dir / f"{device_id}_embedding.pt"
        torch.save(embedding, embedding_path)
        
        # Save metadata
        if metadata is None:
            metadata = {}
        
        metadata['device_id'] = device_id
        metadata['enrollment_date'] = datetime.now().isoformat()
        metadata['embedding_shape'] = list(embedding.shape)
        
        metadata_path = self.storage_dir / f"{device_id}_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"Saved device embedding: {device_id}")
    
    def load_device_embedding(self, device_id: str) -> Optional[torch.Tensor]:
        """
        Load device embedding from storage
        
        Args:
            device_id: Device identifier
            
        Returns:
            Device embedding tensor or None if not found
        """
        embedding_path = self.storage_dir / f"{device_id}_embedding.pt"
        
        if not embedding_path.exists():
            logger.error(f"Device embedding not found: {device_id}")
            return None
        
        embedding = torch.load(embedding_path)
        logger.info(f"Loaded device embedding: {device_id}")
        
        return embedding
    
    def list_enrolled_devices(self) -> List[str]:
        """
        List all enrolled device IDs
        
        Returns:
            List of device IDs
        """
        device_ids = []
        
        for file in self.storage_dir.glob("*_embedding.pt"):
            device_id = file.stem.replace('_embedding', '')
            device_ids.append(device_id)
        
        return device_ids
    
    def enroll_device(
        self,
        device_name: Optional[str] = None,
        num_samples: int = 50,
        sources: List[str] = ['qrng', 'camera', 'microphone'],
        client_samples: Optional[Dict[str, List[List[float]]]] = None
    ) -> str:
        """
        Complete device enrollment process
        
        Args:
            device_name: Optional device name
            num_samples: Number of noise samples to collect
            sources: Noise sources to use
            client_samples: Optional raw noise samples provided by client
            
        Returns:
            Device ID
        """
        logger.info("Starting device enrollment...")
        
        # Generate device ID
        device_id = self.generate_device_id(device_name)
        logger.info(f"Generated device ID: {device_id}")
        
        # Collect noise samples
        if client_samples:
            logger.info("Using client-provided samples")
            noise_samples = {}
            for source, samples_list in client_samples.items():
                # Convert list of lists back to numpy arrays
                noise_samples[source] = [np.array(s) for s in samples_list]
        else:
            noise_samples = self.collect_noise_samples(
                num_samples=num_samples,
                sources=sources
            )
        
        # Validate samples were collected
        if not noise_samples:
            raise RuntimeError("No noise samples collected from any source")
        
        # Check if any source actually has samples
        total_samples = sum(len(samples) for samples in noise_samples.values())
        if total_samples == 0:
            sources_attempted = list(noise_samples.keys())
            raise RuntimeError(
                f"No samples collected from sources: {sources_attempted}. "
                "Check logs above for specific errors with camera/microphone/QRNG."
            )
        
        logger.info(f"Total samples collected: {total_samples} from {len(noise_samples)} sources")
        
        # Save raw data for training if builder is configured
        if self.dataset_builder:
            logger.info("Saving raw samples to dataset...")
            try:
                for source, samples in noise_samples.items():
                    self.dataset_builder.add_batch(
                        device_id=device_id,
                        noise_source=source,
                        samples=samples
                    )
                logger.info("Raw samples saved to dataset")
            except Exception as e:
                logger.error(f"Failed to save raw samples: {e}")

        # Process to feature vectors
        feature_vectors = self.process_noise_to_features(noise_samples)
        
        if not feature_vectors:
            raise RuntimeError(
                f"No feature vectors generated from {total_samples} samples. "
                "Feature extraction failed - check sample data format."
            )
        
        # Create device embedding
        device_embedding = self.create_device_embedding(
            feature_vectors,
            method='mean'
        )
        
        # Save embedding
        metadata = {
            'device_name': device_name,
            'num_samples': sum(len(v) for v in noise_samples.values()),
            'sources': list(noise_samples.keys()),
            'feature_dimension': len(feature_vectors[0])
        }
        
        self.save_device_embedding(
            device_id=device_id,
            embedding=device_embedding,
            metadata=metadata
        )
        
        logger.info(f"Device enrollment complete: {device_id}")
        return device_id


def main():
    """Test enrollment module"""
    print("\n=== Device Enrollment Test ===")
    
    # Create mock components
    from model.siamese_model import SiameseNetwork
    
    input_dim = 50  # Should match feature extractor output
    embedding_dim = 128
    
    # Create model and embedder
    model = SiameseNetwork(input_dim=input_dim, embedding_dim=embedding_dim)
    embedder = DeviceEmbedder(input_dim=input_dim, embedding_dim=embedding_dim)
    embedder.model = model
    
    # Create preprocessor and feature converter
    preprocessor = NoisePreprocessor(normalize=True)
    feature_converter = FeatureVector()
    
    # Create enroller
    enroller = DeviceEnroller(
        embedder=embedder,
        preprocessor=preprocessor,
        feature_converter=feature_converter,
        storage_dir="./auth/test_embeddings"
    )
    
    print("\n=== Testing with Simulated Noise ===")
    # Simulate noise samples (replace with real collection in production)
    simulated_noise = {
        'qrng': [np.random.rand(1024) for _ in range(10)],
        'camera': [np.random.rand(480*640) for _ in range(5)]
    }
    
    # Process to features
    feature_vectors = enroller.process_noise_to_features(simulated_noise)
    print(f"Generated {len(feature_vectors)} feature vectors")
    
    # Create embedding
    device_embedding = enroller.create_device_embedding(feature_vectors)
    print(f"Device embedding shape: {device_embedding.shape}")
    
    # Save embedding
    test_device_id = enroller.generate_device_id("TestDevice")
    enroller.save_device_embedding(test_device_id, device_embedding)
    
    # Load embedding
    loaded_embedding = enroller.load_device_embedding(test_device_id)
    print(f"Loaded embedding matches: {torch.allclose(device_embedding, loaded_embedding)}")
    
    # List enrolled devices
    devices = enroller.list_enrolled_devices()
    print(f"Enrolled devices: {devices}")


if __name__ == "__main__":
    main()
</file>

<file path="frontend/package.json">
{
  "name": "qna-auth-frontend",
  "version": "1.0.0",
  "private": true,
  "type": "module",
  "scripts": {
    "dev": "vite --host",
    "build": "tsc && vite build",
    "preview": "vite preview",
    "lint": "eslint . --ext ts,tsx --report-unused-disable-directives --max-warnings 0"
  },
  "dependencies": {
    "axios": "^1.6.2",
    "clsx": "^2.1.1",
    "framer-motion": "^12.23.26",
    "lucide-react": "^0.294.0",
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "react-router-dom": "^6.20.0",
    "tailwind-merge": "^3.4.0"
  },
  "devDependencies": {
    "@types/react": "^18.2.43",
    "@types/react-dom": "^18.2.17",
    "@typescript-eslint/eslint-plugin": "^6.14.0",
    "@typescript-eslint/parser": "^6.14.0",
    "@vitejs/plugin-react": "^4.2.1",
    "autoprefixer": "^10.4.16",
    "eslint": "^8.55.0",
    "eslint-plugin-react-hooks": "^4.6.0",
    "eslint-plugin-react-refresh": "^0.4.5",
    "postcss": "^8.4.32",
    "tailwindcss": "^3.3.6",
    "typescript": "^5.2.2",
    "vite": "^7.2.7"
  }
}
</file>

<file path="frontend/src/pages/AuthenticatePage.tsx">
import { useState, useEffect } from "react";
import { qnaAuthService, AuthenticationResponse } from "../services/api";
import {
  CameraNoiseCollector,
  MicNoiseCollector,
} from "../services/collectors";
import { Loader2, CheckCircle, XCircle } from "lucide-react";

export default function AuthenticatePage() {
  const [devices, setDevices] = useState<string[]>([]);
  const [selectedDevice, setSelectedDevice] = useState("");
  const [sources, setSources] = useState<string[]>(["qrng"]);
  // Default to TRUE if we are not on localhost (i.e. we are on a mobile/remote device)
  const isRemote =
    window.location.hostname !== "localhost" &&
    window.location.hostname !== "127.0.0.1";
  const [useClientSensors, setUseClientSensors] = useState(isRemote);
  const [loading, setLoading] = useState(false);
  const [status, setStatus] = useState<string>("");
  const [result, setResult] = useState<AuthenticationResponse | null>(null);
  const [error, setError] = useState<string | null>(null);

  useEffect(() => {
    loadDevices();
  }, []);

  const loadDevices = async () => {
    try {
      const response = await qnaAuthService.listDevices();
      setDevices(response.devices);
    } catch (err) {
      console.error("Failed to load devices:", err);
    }
  };

  const handleSourceToggle = (source: string) => {
    setSources((prev) =>
      prev.includes(source)
        ? prev.filter((s) => s !== source)
        : [...prev, source],
    );
  };

  const collectClientSamples = async (
    numSamples: number,
  ): Promise<Record<string, number[][]>> => {
    const clientSamples: Record<string, number[][]> = {};

    if (sources.includes("camera")) {
      setStatus("Initializing Camera...");
      const camCollector = new CameraNoiseCollector();
      if (await camCollector.initialize()) {
        setStatus("Collecting Camera Noise...");
        const samples: number[][] = [];
        for (let i = 0; i < numSamples; i++) {
          const sample = await camCollector.captureDarkFrame(50); // Fast capture
          if (sample) samples.push(sample);
        }
        clientSamples["camera"] = samples;
        camCollector.release();
      } else {
        console.error("Camera failed to init");
      }
    }

    if (sources.includes("microphone")) {
      setStatus("Initializing Microphone...");
      console.log("DEBUG: Frontend - Initializing Microphone Collector");
      const micCollector = new MicNoiseCollector();
      const initSuccess = await micCollector.initialize();
      console.log("DEBUG: Frontend - Mic Init Success:", initSuccess);

      if (initSuccess) {
        setStatus("Collecting Microphone Noise...");
        const samples: number[][] = [];
        for (let i = 0; i < numSamples; i++) {
          console.log(
            `DEBUG: Frontend - Capturing Mic Sample ${i + 1}/${numSamples}`,
          );
          // Short bursts for efficiency
          const sample = await micCollector.captureAmbientNoise(1.0);
          console.log(
            `DEBUG: Frontend - Sample ${i + 1} length:`,
            sample?.length,
          );
          if (sample && sample.length > 0) samples.push(sample);
        }
        console.log(
          "DEBUG: Frontend - Total Mic Samples collected:",
          samples.length,
        );

        if (samples.length === 0) {
          alert(
            "CRITICAL ERROR: Microphone initialized but captured 0 samples! The browser might be blocking audio capture despite permissions.",
          );
        }

        clientSamples["microphone"] = samples;
        micCollector.release();
      } else {
        console.error("Microphone failed to init");
        setStatus("Microphone Init Failed");
        alert(
          "Microphone initialization failed. Please check browser permissions.",
        );
      }
    }

    return clientSamples;
  };

  const handleAuthenticate = async () => {
    if (!selectedDevice) {
      setError("Please select a device");
      return;
    }

    if (sources.length === 0) {
      setError("Please select at least one noise source");
      return;
    }

    setLoading(true);
    setError(null);
    setResult(null);
    setStatus("Initializing...");

    try {
      // 1. Collect Client-Side Samples if needed
      let clientSamples: Record<string, number[][]> | undefined = undefined;

      // Check if we need to collect from client hardware
      // FORCE client collection if the user toggled it, regardless of source types initially
      const needsClientCollection = useClientSensors;

      if (needsClientCollection) {
        // Ensure sources list includes microphone if not already present
        if (!sources.includes("microphone")) {
          sources.push("microphone");
        }
        const samplesPerSource = 5; // Default for auth
        clientSamples = await collectClientSamples(samplesPerSource);

        // Verify we got what we needed
        if (sources.includes("camera") && !clientSamples["camera"]) {
          throw new Error("Failed to access Camera. Please allow permissions.");
        }
        if (sources.includes("microphone") && !clientSamples["microphone"]) {
          throw new Error(
            "Failed to access Microphone. Please allow permissions.",
          );
        }
      }

      setStatus("Authenticating...");

      console.log("DEBUG: Calling Authentication Endpoint with:", {
        deviceId: selectedDevice,
        sources: sources,
        clientSamplesKeys: clientSamples ? Object.keys(clientSamples) : "None",
      });

      const response = await qnaAuthService.authenticateDevice({
        device_id: selectedDevice,
        sources: sources,
        num_samples_per_source: 5,
        client_samples: clientSamples, // Ensure this property name matches the Python Pydantic model exactly
      });
      setResult(response);
    } catch (err) {
      console.log(err);
      const error = err as {
        response?: { data?: { detail?: string } };
        message?: string;
      };
      setError(
        error.response?.data?.detail ||
          error.message ||
          "Authentication failed",
      );
      setResult({
        authenticated: false,
        device_id: selectedDevice,
        message: "Authentication failed",
      });
    } finally {
      setLoading(false);
      setStatus("");
    }
  };

  return (
    <div className="flex items-center justify-center min-h-[calc(100vh-8rem)] p-4">
      <div className="max-w-2xl w-full bg-neutral-900/80 backdrop-blur-sm border border-neutral-800 p-8">
        <h1 className="text-3xl font-bold mb-8">Authenticate Device</h1>

        <div className="space-y-6">
          <div>
            <label className="block text-sm font-medium mb-2">
              Select Device
            </label>
            {devices.length === 0 ? (
              <p className="text-neutral-400 text-sm">
                No devices enrolled yet
              </p>
            ) : (
              <select
                value={selectedDevice}
                onChange={(e) => setSelectedDevice(e.target.value)}
                className="w-full px-4 py-2 bg-neutral-800 border border-neutral-700 focus:ring-2 focus:ring-blue-500 outline-none"
              >
                <option value="">Choose a device...</option>
                {devices.map((deviceId) => (
                  <option key={deviceId} value={deviceId}>
                    {deviceId}
                  </option>
                ))}
              </select>
            )}
          </div>

          <div>
            <label className="block text-sm font-medium mb-3">
              Noise Sources
            </label>
            <div className="space-y-4">
              <div className="space-y-2">
                <SourceCheckbox
                  label="Quantum RNG"
                  value="qrng"
                  checked={sources.includes("qrng")}
                  onChange={handleSourceToggle}
                />
                <SourceCheckbox
                  label="Camera"
                  value="camera"
                  checked={sources.includes("camera")}
                  onChange={handleSourceToggle}
                />
                <SourceCheckbox
                  label="Microphone"
                  value="microphone"
                  checked={sources.includes("microphone")}
                  onChange={handleSourceToggle}
                />
              </div>

              <div className="pt-2 border-t border-neutral-700">
                <label className="flex items-center space-x-2 cursor-pointer">
                  <input
                    type="checkbox"
                    checked={useClientSensors}
                    onChange={(e) => setUseClientSensors(e.target.checked)}
                    className="w-4 h-4 rounded border-gray-600 bg-neutral-800 text-blue-600 focus:ring-blue-500"
                  />
                  <span className="text-sm text-neutral-300">
                    Use Mobile/Local Hardware
                  </span>
                </label>
              </div>
            </div>
          </div>

          <button
            onClick={handleAuthenticate}
            disabled={loading || !selectedDevice || sources.length === 0}
            className="w-full bg-blue-600 hover:bg-blue-700 disabled:bg-neutral-700 disabled:cursor-not-allowed text-white py-3 font-semibold flex items-center justify-center gap-2 transition"
          >
            {loading ? (
              <>
                <Loader2 className="w-5 h-5 animate-spin" />
                <span>{status || "Authenticating..."}</span>
              </>
            ) : (
              <>
                <span>Authenticate</span>
              </>
            )}
          </button>
        </div>

        {result && result.authenticated && (
          <div className="mt-6 p-4 bg-green-900/30 border border-green-500">
            <div className="flex items-center gap-2 mb-2">
              <CheckCircle className="w-5 h-5 text-green-500" />
              <h3 className="font-semibold text-green-500">
                Authentication Successful
              </h3>
            </div>
            <div className="text-sm space-y-1">
              <p>
                <strong>Device ID:</strong> {result.device_id}
              </p>
              {result.details?.similarity && (
                <p>
                  <strong>Similarity:</strong>{" "}
                  {(result.details.similarity * 100).toFixed(2)}%
                </p>
              )}
            </div>
          </div>
        )}

        {result && !result.authenticated && (
          <div className="mt-6 p-4 bg-red-900/30 border border-red-500">
            <div className="flex items-center gap-2 mb-2">
              <XCircle className="w-5 h-5 text-red-500" />
              <h3 className="font-semibold text-red-500">
                Authentication Failed
              </h3>
            </div>
          </div>
        )}

        {error && !result && (
          <div className="mt-6 p-4 bg-red-900/30 border border-red-500">
            <div className="flex items-center gap-2">
              <XCircle className="w-5 h-5 text-red-500" />
              <p className="text-red-500">{error}</p>
            </div>
          </div>
        )}
      </div>
    </div>
  );
}

interface SourceCheckboxProps {
  label: string;
  value: string;
  checked: boolean;
  onChange: (value: string) => void;
}

function SourceCheckbox({
  label,
  value,
  checked,
  onChange,
}: SourceCheckboxProps) {
  return (
    <label className="flex items-center gap-3 p-3 bg-neutral-800 border border-neutral-700 cursor-pointer hover:bg-neutral-750 transition">
      <input
        type="checkbox"
        checked={checked}
        onChange={() => onChange(value)}
        className="w-4 h-4"
      />
      <span className="font-medium">{label}</span>
    </label>
  );
}
</file>

<file path="frontend/src/pages/EnrollPage.tsx">
import { useState } from "react";
import {
  qnaAuthService,
  EnrollmentRequest,
  EnrollmentResponse,
} from "../services/api";
import {
  CameraNoiseCollector,
  MicNoiseCollector,
} from "../services/collectors";
import { Loader2, CheckCircle, XCircle } from "lucide-react";

export default function EnrollPage() {
  const [deviceName, setDeviceName] = useState("");
  const [numSamples, setNumSamples] = useState(50);
  const [sources, setSources] = useState<string[]>(["qrng"]);
  // Default to TRUE if we are not on localhost (i.e. we are on a mobile/remote device)
  const isRemote =
    window.location.hostname !== "localhost" &&
    window.location.hostname !== "127.0.0.1";
  const [useClientSensors, setUseClientSensors] = useState(isRemote);
  const [loading, setLoading] = useState(false);
  const [status, setStatus] = useState<string>("");
  const [result, setResult] = useState<EnrollmentResponse | null>(null);
  const [error, setError] = useState<string | null>(null);

  const handleSourceToggle = (source: string) => {
    setSources((prev) =>
      prev.includes(source)
        ? prev.filter((s) => s !== source)
        : [...prev, source],
    );
  };

  const collectClientSamples = async (
    numSamples: number,
  ): Promise<Record<string, number[][]>> => {
    const clientSamples: Record<string, number[][]> = {};

    if (sources.includes("camera")) {
      setStatus("Initializing Camera...");
      const camCollector = new CameraNoiseCollector();
      if (await camCollector.initialize()) {
        setStatus("Collecting Camera Noise...");
        const samples: number[][] = [];
        for (let i = 0; i < numSamples; i++) {
          const sample = await camCollector.captureDarkFrame(50); // Fast capture
          if (sample) samples.push(sample);
        }
        clientSamples["camera"] = samples;
        camCollector.release();
      } else {
        console.error("Camera failed to init");
      }
    }

    if (sources.includes("microphone")) {
      setStatus("Initializing Microphone...");
      const micCollector = new MicNoiseCollector();
      if (await micCollector.initialize()) {
        setStatus("Collecting Microphone Noise...");
        const samples: number[][] = [];
        for (let i = 0; i < numSamples; i++) {
          // Short bursts for efficiency
          const sample = await micCollector.captureAmbientNoise(0.5);
          if (sample) samples.push(sample);
        }
        clientSamples["microphone"] = samples;
        micCollector.release();
      } else {
        console.error("Microphone failed to init");
      }
    }

    return clientSamples;
  };

  const handleEnroll = async () => {
    if (sources.length === 0) {
      setError("Please select at least one noise source");
      return;
    }

    setLoading(true);
    setError(null);
    setResult(null);
    setStatus("Initializing...");

    try {
      // 1. Collect Client-Side Samples if needed
      let clientSamples: Record<string, number[][]> | undefined = undefined;

      // Check if we need to collect from client hardware
      const needsClientCollection =
        (sources.includes("camera") || sources.includes("microphone")) &&
        useClientSensors;

      if (needsClientCollection) {
        clientSamples = await collectClientSamples(numSamples);

        // Verify we got what we needed
        if (sources.includes("camera") && !clientSamples["camera"]) {
          throw new Error("Failed to access Camera. Please allow permissions.");
        }
        if (sources.includes("microphone") && !clientSamples["microphone"]) {
          throw new Error(
            "Failed to access Microphone. Please allow permissions.",
          );
        }
      }

      setStatus("Sending to Server...");

      const request: EnrollmentRequest = {
        device_name: deviceName || undefined,
        num_samples: numSamples,
        sources: sources,
        client_samples: clientSamples,
      };

      const response = await qnaAuthService.enrollDevice(request);
      setResult(response);
    } catch (err) {
      const error = err as {
        response?: { data?: { detail?: string } };
        message?: string;
      };
      setError(
        error.response?.data?.detail || error.message || "Enrollment failed",
      );
    } finally {
      setLoading(false);
      setStatus("");
    }
  };

  return (
    <div className="flex items-center justify-center min-h-[calc(100vh-8rem)] p-4">
      <div className="max-w-2xl w-full bg-neutral-900/80 backdrop-blur-sm border border-neutral-800 p-8">
        <h1 className="text-3xl font-bold mb-8">Enroll Device</h1>

        <div className="space-y-6">
          <div>
            <label className="block text-sm font-medium mb-2">
              Device Name (Optional)
            </label>
            <input
              type="text"
              value={deviceName}
              onChange={(e) => setDeviceName(e.target.value)}
              placeholder="My Device"
              className="w-full px-4 py-2 bg-neutral-800 border border-neutral-700 focus:ring-2 focus:ring-blue-500 outline-none"
            />
          </div>

          <div>
            <label className="block text-sm font-medium mb-2">
              Number of Samples: {numSamples}
            </label>
            <input
              type="range"
              min="10"
              max="200"
              value={numSamples}
              onChange={(e) => setNumSamples(parseInt(e.target.value))}
              className="w-full"
            />
          </div>

          <div>
            <label className="block text-sm font-medium mb-3">
              Noise Sources
            </label>

            <div className="space-y-4">
              <div className="space-y-2">
                <SourceCheckbox
                  label="Quantum RNG"
                  value="qrng"
                  checked={sources.includes("qrng")}
                  onChange={handleSourceToggle}
                />
                <SourceCheckbox
                  label="Camera"
                  value="camera"
                  checked={sources.includes("camera")}
                  onChange={handleSourceToggle}
                />
                <SourceCheckbox
                  label="Microphone"
                  value="microphone"
                  checked={sources.includes("microphone")}
                  onChange={handleSourceToggle}
                />
              </div>

              <div className="pt-2 border-t border-neutral-700">
                {!window.isSecureContext &&
                window.location.hostname !== "localhost" ? (
                  <div className="p-3 bg-yellow-900/30 border border-yellow-600 rounded text-sm text-yellow-500 mb-2">
                    <strong>Warning:</strong> Remote Mic/Camera access is
                    blocked by your browser because this connection is not
                    secure (HTTP).
                    <br />
                    To fix this on Mobile/Laptop 2:
                    <ul className="list-disc ml-5 mt-1">
                      <li>
                        Use <code>chrome://flags</code> on the device
                      </li>
                      <li>
                        Enable <em>"Insecure origins treated as secure"</em>
                      </li>
                      <li>
                        Add <code>http://{window.location.host}</code> to the
                        list
                      </li>
                    </ul>
                  </div>
                ) : null}

                <label className="flex items-center space-x-2 cursor-pointer">
                  <input
                    type="checkbox"
                    checked={useClientSensors}
                    onChange={(e) => setUseClientSensors(e.target.checked)}
                    className="w-4 h-4 rounded border-gray-600 bg-neutral-800 text-blue-600 focus:ring-blue-500"
                  />
                  <span className="text-sm text-neutral-300">
                    Use *This* Device's Sensors (Remote Capture)
                  </span>
                </label>
                <p className="text-xs text-neutral-500 mt-1 ml-6">
                  Check this to capture audio/video from this device.
                  <br />
                  <strong>
                    Currently:{" "}
                    {useClientSensors
                      ? "ENABLED (Remote)"
                      : "DISABLED (Server-Side)"}
                  </strong>
                </p>
              </div>
            </div>
          </div>

          <button
            onClick={handleEnroll}
            disabled={loading || sources.length === 0}
            className="w-full bg-blue-600 hover:bg-blue-700 disabled:bg-neutral-700 disabled:cursor-not-allowed text-white py-3 font-semibold flex items-center justify-center gap-2 transition"
          >
            {loading ? (
              <>
                <Loader2 className="w-5 h-5 animate-spin" />
                <span>{status || "Enrolling..."}</span>
              </>
            ) : (
              <span>Enroll Device</span>
            )}
          </button>
        </div>

        {result && (
          <div className="mt-6 p-4 bg-green-900/30 border border-green-500">
            <div className="flex items-center gap-2 mb-2">
              <CheckCircle className="w-5 h-5 text-green-500" />
              <h3 className="font-semibold text-green-500">
                Enrollment Successful
              </h3>
            </div>
            <div className="text-sm space-y-1">
              <p>
                <strong>Device ID:</strong> {result.device_id}
              </p>
              <p>
                <strong>Samples Collected:</strong>{" "}
                {(result.metadata as { num_samples?: number })?.num_samples ||
                  numSamples}
              </p>
              <p>
                <strong>Sources Used:</strong>{" "}
                {(result.metadata as { sources?: string[] })?.sources?.join(
                  ", ",
                ) || sources.join(", ")}
              </p>
            </div>
          </div>
        )}

        {error && (
          <div className="mt-6 p-4 bg-red-900/30 border border-red-500">
            <div className="flex items-center gap-2">
              <XCircle className="w-5 h-5 text-red-500" />
              <p className="text-red-500">{error}</p>
            </div>
          </div>
        )}
      </div>
    </div>
  );
}

interface SourceCheckboxProps {
  label: string;
  value: string;
  checked: boolean;
  onChange: (value: string) => void;
}

function SourceCheckbox({
  label,
  value,
  checked,
  onChange,
}: SourceCheckboxProps) {
  return (
    <label className="flex items-center gap-3 p-3 bg-neutral-800 border border-neutral-700 cursor-pointer hover:bg-neutral-750 transition">
      <input
        type="checkbox"
        checked={checked}
        onChange={() => onChange(value)}
        className="w-4 h-4"
      />
      <div className="font-medium">{label}</div>
    </label>
  );
}
</file>

<file path="frontend/src/services/api.ts">
import axios from 'axios'

// Use the Vite proxy (configured in vite.config.ts)
// This routes requests like /api/endpoint -> http://localhost:8000/endpoint
// This solves CORS and firewall issues by tunneling through the frontend server port
const API_BASE_URL = '/api'

const api = axios.create({
  baseURL: API_BASE_URL,
  headers: {
    'Content-Type': 'application/json',
  },
})

export interface EnrollmentRequest {
  device_name?: string
  num_samples: number
  sources: string[]
  client_samples?: Record<string, number[][]>
}

export interface EnrollmentResponse {
  device_id: string
  status: string
  message: string
  metadata: Record<string, unknown>
}

export interface AuthenticationRequest {
  device_id: string
  sources: string[]
  num_samples_per_source: number
  client_samples?: Record<string, number[][]>
}

export interface AuthenticationResponse {
  authenticated: boolean
  device_id: string
  similarity?: number
  details?: {
    similarity: number
    threshold: number
  }
  message?: string
}

export interface Device {
  device_id: string
  device_name?: string
  enrollment_date: string
}

export const qnaAuthService = {
  // Health check
  async checkHealth() {
    const response = await api.get('health')
    return response.data
  },

  // Enrollment
  async enrollDevice(request: EnrollmentRequest): Promise<EnrollmentResponse> {
    const response = await api.post('enroll', request)
    return response.data
  },

  // Authentication
  async authenticateDevice(request: AuthenticationRequest): Promise<AuthenticationResponse> {
    const response = await api.post('authenticate', request)
    return response.data
  },

  // Challenge-Response
  async createChallenge(deviceId: string) {
    const response = await api.post('challenge', { device_id: deviceId })
    return response.data
  },

  async verifyChallenge(challengeId: string, response: string, deviceId: string, noiseSamples: number[][]) {
    const res = await api.post('verify', {
      challenge_id: challengeId,
      response: response,
      device_id: deviceId,
      noise_samples: noiseSamples,
    })
    return res.data
  },

  // Device Management
  async listDevices() {
    const response = await api.get('devices')
    return response.data
  },

  async getDevice(deviceId: string) {
    const response = await api.get(`devices/${deviceId}`)
    return response.data
  },

  async deleteDevice(deviceId: string) {
    const response = await api.delete(`devices/${deviceId}`)
    return response.data
  },
}

export default api
</file>

<file path="model/siamese_model.py">
"""
Siamese Neural Network for Device Authentication
Creates unique embeddings from noise samples using contrastive learning
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EmbeddingNetwork(nn.Module):
    """Base embedding network that processes input features"""
    
    def __init__(
        self,
        input_dim: int,
        embedding_dim: int = 128,
        hidden_dims: list = [256, 256, 128]
    ):
        """
        Initialize embedding network
        
        Args:
            input_dim: Dimension of input features
            embedding_dim: Dimension of output embedding
            hidden_dims: List of hidden layer dimensions
        """
        super(EmbeddingNetwork, self).__init__()
        
        self.input_dim = input_dim
        self.embedding_dim = embedding_dim
        
        # Build layers
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                # nn.BatchNorm1d(hidden_dim),  # Disabled due to CUDA device issues
                nn.Tanh(), # Switched from ReLU to Tanh to avoid dying ReLU collapse
                # nn.Dropout(0.3) # Remove dropout for untrained deterministic projection
            ])
            prev_dim = hidden_dim
        
        # Final embedding layer
        layers.append(nn.Linear(prev_dim, embedding_dim))
        
        self.network = nn.Sequential(*layers)
        
        # L2 normalization layer
        self.normalize = nn.functional.normalize
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass
        
        Args:
            x: Input features [batch_size, input_dim]
            
        Returns:
            Normalized embeddings [batch_size, embedding_dim]
        """
        embedding = self.network(x)
        # L2 normalize embeddings
        embedding = F.normalize(embedding, p=2, dim=1)
        return embedding


class SiameseNetwork(nn.Module):
    """Siamese network with shared embedding network"""
    
    def __init__(
        self,
        input_dim: int,
        embedding_dim: int = 128,
        hidden_dims: list = [256, 256, 128]
    ):
        """
        Initialize Siamese network
        
        Args:
            input_dim: Dimension of input features
            embedding_dim: Dimension of output embedding
            hidden_dims: List of hidden layer dimensions
        """
        super(SiameseNetwork, self).__init__()
        
        # Shared embedding network
        self.embedding_network = EmbeddingNetwork(
            input_dim=input_dim,
            embedding_dim=embedding_dim,
            hidden_dims=hidden_dims
        )
        
    def forward_one(self, x: torch.Tensor) -> torch.Tensor:
        """
        Process single input
        
        Args:
            x: Input features
            
        Returns:
            Embedding
        """
        return self.embedding_network(x)
    
    def forward(
        self,
        anchor: torch.Tensor,
        positive: torch.Tensor,
        negative: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass for triplet
        
        Args:
            anchor: Anchor samples
            positive: Positive samples (same device)
            negative: Negative samples (different device)
            
        Returns:
            Tuple of (anchor_embedding, positive_embedding, negative_embedding)
        """
        anchor_emb = self.forward_one(anchor)
        positive_emb = self.forward_one(positive)
        negative_emb = self.forward_one(negative)
        
        return anchor_emb, positive_emb, negative_emb


class TripletLoss(nn.Module):
    """Triplet loss for metric learning"""
    
    def __init__(self, margin: float = 1.0):
        """
        Initialize triplet loss
        
        Args:
            margin: Margin for triplet loss
        """
        super(TripletLoss, self).__init__()
        self.margin = margin
        
    def forward(
        self,
        anchor: torch.Tensor,
        positive: torch.Tensor,
        negative: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute triplet loss
        
        Args:
            anchor: Anchor embeddings
            positive: Positive embeddings
            negative: Negative embeddings
            
        Returns:
            Loss value
        """
        # Compute distances
        pos_dist = F.pairwise_distance(anchor, positive, p=2)
        neg_dist = F.pairwise_distance(anchor, negative, p=2)
        
        # Triplet loss
        loss = F.relu(pos_dist - neg_dist + self.margin)
        
        return loss.mean()


class ContrastiveLoss(nn.Module):
    """Contrastive loss for Siamese networks"""
    
    def __init__(self, margin: float = 1.0):
        """
        Initialize contrastive loss
        
        Args:
            margin: Margin for negative pairs
        """
        super(ContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(
        self,
        embedding1: torch.Tensor,
        embedding2: torch.Tensor,
        label: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute contrastive loss
        
        Args:
            embedding1: First embeddings
            embedding2: Second embeddings
            label: 1 for same device, 0 for different devices
            
        Returns:
            Loss value
        """
        # Euclidean distance
        distance = F.pairwise_distance(embedding1, embedding2, p=2)
        
        # Contrastive loss
        loss_positive = label * torch.pow(distance, 2)
        loss_negative = (1 - label) * torch.pow(F.relu(self.margin - distance), 2)
        
        loss = 0.5 * (loss_positive + loss_negative)
        
        return loss.mean()


class DeviceEmbedder:
    """High-level interface for device embedding"""
    
    def __init__(
        self,
        input_dim: int,
        embedding_dim: int = 128,
        device: str = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    ):
        """
        Initialize device embedder
        
        Args:
            input_dim: Input feature dimension
            embedding_dim: Embedding dimension
            device: Device to run model on
        """
        # Explicitly use cuda:0 to avoid device mismatch
        self.device = torch.device(device)
        self.model = SiameseNetwork(
            input_dim=input_dim,
            embedding_dim=embedding_dim
        )
        
        # Move model to device
        self.model = self.model.to(self.device)
        
        # Set to eval mode
        self.model.eval()
        
        self.embedding_dim = embedding_dim
        
        logger.info(f"Initialized DeviceEmbedder on {self.device}")
        logger.info(f"Input dim: {input_dim}, Embedding dim: {embedding_dim}")
    
    def embed(self, features: torch.Tensor) -> torch.Tensor:
        """
        Generate embedding for features
        
        Args:
            features: Input features [batch_size, input_dim] or [input_dim]
            
        Returns:
            Embeddings [batch_size, embedding_dim] or [embedding_dim]
        """
        self.model.eval()
        
        # Handle single sample
        if features.dim() == 1:
            features = features.unsqueeze(0)
            single_sample = True
        else:
            single_sample = False
        
        # Ensure features is float32 and on the correct device
        features = features.float().to(self.device)
        
        # Generate embedding
        with torch.no_grad():
            embedding = self.model.forward_one(features)
        
        # Return to CPU
        embedding = embedding.cpu()
        
        # Return single embedding if input was single sample
        if single_sample:
            embedding = embedding.squeeze(0)
        
        return embedding
    
    def compute_similarity(
        self,
        embedding1: torch.Tensor,
        embedding2: torch.Tensor,
        metric: str = 'cosine'
    ) -> float:
        """
        Compute similarity between two embeddings
        
        Args:
            embedding1: First embedding
            embedding2: Second embedding
            metric: Similarity metric ('cosine' or 'euclidean')
            
        Returns:
            Similarity score
        """
        if metric == 'cosine':
            # Cosine similarity (higher = more similar)
            similarity = F.cosine_similarity(
                embedding1.unsqueeze(0),
                embedding2.unsqueeze(0)
            ).item()
        elif metric == 'euclidean':
            # Euclidean distance (lower = more similar)
            similarity = -F.pairwise_distance(
                embedding1.unsqueeze(0),
                embedding2.unsqueeze(0),
                p=2
            ).item()
        else:
            raise ValueError(f"Unknown metric: {metric}")
        
        return similarity
    
    def save_model(self, path: str):
        """Save model to file"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'embedding_dim': self.embedding_dim,
            'input_dim': self.model.embedding_network.input_dim
        }, path)
        logger.info(f"Model saved to {path}")
    
    def load_model(self, path: str):
        """Load model from file"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        logger.info(f"Model loaded from {path}")


def main():
    """Test Siamese network"""
    print("\n=== Siamese Neural Network Test ===")
    
    # Parameters
    input_dim = 50
    embedding_dim = 128
    batch_size = 32
    
    # Create model
    model = SiameseNetwork(
        input_dim=input_dim,
        embedding_dim=embedding_dim
    )
    
    print(f"\nModel architecture:")
    print(model)
    
    # Test forward pass
    print("\n=== Testing Forward Pass ===")
    anchor = torch.randn(batch_size, input_dim)
    positive = torch.randn(batch_size, input_dim)
    negative = torch.randn(batch_size, input_dim)
    
    anchor_emb, pos_emb, neg_emb = model(anchor, positive, negative)
    
    print(f"Anchor embedding shape: {anchor_emb.shape}")
    print(f"Positive embedding shape: {pos_emb.shape}")
    print(f"Negative embedding shape: {neg_emb.shape}")
    
    # Test losses
    print("\n=== Testing Triplet Loss ===")
    triplet_loss = TripletLoss(margin=1.0)
    loss = triplet_loss(anchor_emb, pos_emb, neg_emb)
    print(f"Triplet loss: {loss.item():.4f}")
    
    print("\n=== Testing Contrastive Loss ===")
    contrastive_loss = ContrastiveLoss(margin=1.0)
    labels = torch.ones(batch_size)  # Same device
    loss = contrastive_loss(anchor_emb, pos_emb, labels)
    print(f"Contrastive loss (positive pairs): {loss.item():.4f}")
    
    labels = torch.zeros(batch_size)  # Different devices
    loss = contrastive_loss(anchor_emb, neg_emb, labels)
    print(f"Contrastive loss (negative pairs): {loss.item():.4f}")
    
    # Test embedder
    print("\n=== Testing DeviceEmbedder ===")
    embedder = DeviceEmbedder(input_dim=input_dim, embedding_dim=embedding_dim)
    
    # Single sample
    single_features = torch.randn(input_dim)
    embedding = embedder.embed(single_features)
    print(f"Single embedding shape: {embedding.shape}")
    
    # Batch
    batch_features = torch.randn(5, input_dim)
    embeddings = embedder.embed(batch_features)
    print(f"Batch embeddings shape: {embeddings.shape}")
    
    # Similarity
    emb1 = embedder.embed(torch.randn(input_dim))
    emb2 = embedder.embed(torch.randn(input_dim))
    sim = embedder.compute_similarity(emb1, emb2, metric='cosine')
    print(f"Cosine similarity: {sim:.4f}")


if __name__ == "__main__":
    main()
</file>

<file path="model/train.py">
"""
Training Script for Siamese Network
Trains the model using triplet or contrastive loss
"""

import torch
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from pathlib import Path
from typing import Tuple, List, Optional, Dict, Any
import json
import logging
from tqdm import tqdm

from .siamese_model import SiameseNetwork, TripletLoss, ContrastiveLoss
from preprocessing.features import NoisePreprocessor, FeatureVector

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TripletDataset(Dataset):
    """Dataset for triplet training"""
    
    def __init__(
        self,
        features_by_device: Dict[str, List[np.ndarray]],
        samples_per_epoch: int = 1000
    ):
        """
        Initialize triplet dataset
        
        Args:
            features_by_device: Dictionary mapping device_id to list of feature arrays
            samples_per_epoch: Number of triplets to generate per epoch
        """
        self.features_by_device = features_by_device
        self.device_ids = list(features_by_device.keys())
        self.samples_per_epoch = samples_per_epoch
        
        logger.info(f"TripletDataset: {len(self.device_ids)} devices, "
                   f"{samples_per_epoch} triplets/epoch")
    
    def __len__(self) -> int:
        return self.samples_per_epoch
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Generate a triplet (anchor, positive, negative)
        
        Returns:
            Tuple of (anchor, positive, negative) tensors
        """
        # Select anchor device
        anchor_device = np.random.choice(self.device_ids)
        anchor_features = self.features_by_device[anchor_device]
        
        # Select anchor and positive from same device
        anchor_idx, positive_idx = np.random.choice(
            len(anchor_features), size=2, replace=True
        )
        anchor = anchor_features[anchor_idx]
        positive = anchor_features[positive_idx]
        
        # Select negative from different device
        negative_device = np.random.choice(
            [d for d in self.device_ids if d != anchor_device]
        )
        negative_features = self.features_by_device[negative_device]
        negative_idx = np.random.choice(len(negative_features))
        negative = negative_features[negative_idx]
        
        # Convert to tensors
        anchor = torch.from_numpy(anchor).float()
        positive = torch.from_numpy(positive).float()
        negative = torch.from_numpy(negative).float()
        
        return anchor, positive, negative


class PairDataset(Dataset):
    """Dataset for contrastive learning with pairs"""
    
    def __init__(
        self,
        features_by_device: Dict[str, List[np.ndarray]],
        samples_per_epoch: int = 1000,
        positive_ratio: float = 0.5
    ):
        """
        Initialize pair dataset
        
        Args:
            features_by_device: Dictionary mapping device_id to list of feature arrays
            samples_per_epoch: Number of pairs to generate per epoch
            positive_ratio: Ratio of positive pairs (same device)
        """
        self.features_by_device = features_by_device
        self.device_ids = list(features_by_device.keys())
        self.samples_per_epoch = samples_per_epoch
        self.positive_ratio = positive_ratio
        
        logger.info(f"PairDataset: {len(self.device_ids)} devices, "
                   f"{samples_per_epoch} pairs/epoch")
    
    def __len__(self) -> int:
        return self.samples_per_epoch
    
    def __getitem__(
        self, 
        idx: int
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Generate a pair with label
        
        Returns:
            Tuple of (sample1, sample2, label) where label=1 for same device
        """
        # Decide if positive or negative pair
        is_positive = np.random.random() < self.positive_ratio
        
        if is_positive:
            # Same device
            device = np.random.choice(self.device_ids)
            features = self.features_by_device[device]
            idx1, idx2 = np.random.choice(len(features), size=2, replace=True)
            sample1 = features[idx1]
            sample2 = features[idx2]
            label = 1.0
        else:
            # Different devices
            device1, device2 = np.random.choice(self.device_ids, size=2, replace=False)
            features1 = self.features_by_device[device1]
            features2 = self.features_by_device[device2]
            sample1 = features1[np.random.choice(len(features1))]
            sample2 = features2[np.random.choice(len(features2))]
            label = 0.0
        
        # Convert to tensors
        sample1 = torch.from_numpy(sample1).float()
        sample2 = torch.from_numpy(sample2).float()
        label = torch.tensor(label).float()
        
        return sample1, sample2, label


class ModelTrainer:
    """Trains Siamese network for device authentication"""
    
    def __init__(
        self,
        model: SiameseNetwork,
        loss_type: str = 'triplet',
        learning_rate: float = 0.001,
        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
    ):
        """
        Initialize trainer
        
        Args:
            model: SiameseNetwork model
            loss_type: 'triplet' or 'contrastive'
            learning_rate: Learning rate
            device: Device to train on
        """
        self.model = model
        self.device = torch.device(device)
        self.model.to(self.device)
        
        # Loss function
        if loss_type == 'triplet':
            self.criterion = TripletLoss(margin=1.0)
        elif loss_type == 'contrastive':
            self.criterion = ContrastiveLoss(margin=1.0)
        else:
            raise ValueError(f"Unknown loss type: {loss_type}")
        
        self.loss_type = loss_type
        
        # Optimizer
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=1e-5
        )
        
        # Learning rate scheduler
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=5
        )
        
        # Training history
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'learning_rate': []
        }
        
        logger.info(f"ModelTrainer initialized on {self.device}")
        logger.info(f"Loss type: {loss_type}")
    
    def train_epoch(
        self,
        train_loader: DataLoader,
        epoch: int
    ) -> float:
        """
        Train for one epoch
        
        Args:
            train_loader: Training data loader
            epoch: Current epoch number
            
        Returns:
            Average training loss
        """
        self.model.train()
        total_loss = 0.0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}")
        
        for batch_idx, batch in enumerate(pbar):
            if self.loss_type == 'triplet':
                anchor, positive, negative = batch
                anchor = anchor.to(self.device)
                positive = positive.to(self.device)
                negative = negative.to(self.device)
                
                # Forward pass
                anchor_emb, pos_emb, neg_emb = self.model(anchor, positive, negative)
                
                # Compute loss
                loss = self.criterion(anchor_emb, pos_emb, neg_emb)
                
            elif self.loss_type == 'contrastive':
                sample1, sample2, label = batch
                sample1 = sample1.to(self.device)
                sample2 = sample2.to(self.device)
                label = label.to(self.device)
                
                # Forward pass
                emb1 = self.model.forward_one(sample1)
                emb2 = self.model.forward_one(sample2)
                
                # Compute loss
                loss = self.criterion(emb1, emb2, label)
            
            # Backward pass
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            
            # Update progress bar
            pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_loss = total_loss / len(train_loader)
        return avg_loss
    
    def validate(self, val_loader: DataLoader) -> float:
        """
        Validate model
        
        Args:
            val_loader: Validation data loader
            
        Returns:
            Average validation loss
        """
        self.model.eval()
        total_loss = 0.0
        
        with torch.no_grad():
            for batch in val_loader:
                if self.loss_type == 'triplet':
                    anchor, positive, negative = batch
                    anchor = anchor.to(self.device)
                    positive = positive.to(self.device)
                    negative = negative.to(self.device)
                    
                    anchor_emb, pos_emb, neg_emb = self.model(anchor, positive, negative)
                    loss = self.criterion(anchor_emb, pos_emb, neg_emb)
                    
                elif self.loss_type == 'contrastive':
                    sample1, sample2, label = batch
                    sample1 = sample1.to(self.device)
                    sample2 = sample2.to(self.device)
                    label = label.to(self.device)
                    
                    emb1 = self.model.forward_one(sample1)
                    emb2 = self.model.forward_one(sample2)
                    loss = self.criterion(emb1, emb2, label)
                
                total_loss += loss.item()
        
        avg_loss = total_loss / len(val_loader)
        return avg_loss
    
    def train(
        self,
        train_loader: DataLoader,
        val_loader: Optional[DataLoader] = None,
        epochs: int = 50,
        save_dir: str = "./model/checkpoints"
    ) -> Dict[str, List[float]]:
        """
        Train model for multiple epochs
        
        Args:
            train_loader: Training data loader
            val_loader: Optional validation data loader
            epochs: Number of epochs
            save_dir: Directory to save checkpoints
            
        Returns:
            Training history dictionary
        """
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)
        
        best_val_loss = float('inf')
        
        for epoch in range(1, epochs + 1):
            # Train
            train_loss = self.train_epoch(train_loader, epoch)
            self.history['train_loss'].append(train_loss)
            self.history['learning_rate'].append(
                self.optimizer.param_groups[0]['lr']
            )
            
            logger.info(f"Epoch {epoch}/{epochs} - Train Loss: {train_loss:.4f}")
            
            # Validate
            if val_loader is not None:
                val_loss = self.validate(val_loader)
                self.history['val_loss'].append(val_loss)
                logger.info(f"Epoch {epoch}/{epochs} - Val Loss: {val_loss:.4f}")
                
                # Update learning rate
                self.scheduler.step(val_loss)
                
                # Save best model
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    checkpoint_path = save_path / "best_model.pt"
                    self.save_checkpoint(checkpoint_path, epoch, val_loss)
                    logger.info(f"Saved best model (val_loss={val_loss:.4f})")
            
            # Save periodic checkpoint
            if epoch % 10 == 0:
                checkpoint_path = save_path / f"checkpoint_epoch_{epoch}.pt"
                self.save_checkpoint(
                    checkpoint_path,
                    epoch,
                    self.history['val_loss'][-1] if val_loader else train_loss
                )
        
        # Save final model
        final_path = save_path / "final_model.pt"
        self.save_checkpoint(final_path, epochs, train_loss)
        logger.info("Training completed!")
        
        return self.history
    
    def save_checkpoint(
        self,
        path: Path,
        epoch: int,
        loss: float
    ):
        """Save model checkpoint"""
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'loss': loss,
            'history': self.history
        }, path)
    
    def load_checkpoint(self, path: Path):
        """Load model checkpoint"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.history = checkpoint['history']
        logger.info(f"Loaded checkpoint from epoch {checkpoint['epoch']}")


def load_real_dataset(data_dir="./dataset/samples"):
    """Load and process real dataset from disk"""
    print(f"Loading dataset from {data_dir}...")
    json_dir = Path(data_dir) / "json"
    if not json_dir.exists():
        print("Dataset directory not found!")
        return {}, 0
    
    preprocessor = NoisePreprocessor(normalize=True)
    converter = FeatureVector()
    
    features_by_device = {}
    sample_count = 0
    
    json_files = list(json_dir.glob("*.json"))
    # Limit to 20 samples for speed during demo
    import random
    if len(json_files) > 20:
        print("Subsampling to 20 samples for speed...")
        random.shuffle(json_files)
        json_files = json_files[:20]
        
    print(f"Found {len(json_files)} samples.")
    
    if not json_files:
        return {}, 0
    
    for json_file in tqdm(json_files, desc="Processing samples"):
        try:
            with open(json_file, 'r') as f:
                meta = json.load(f)
            
            device_id = meta['device_id']
            # fix path (remove leading slash if present in metadata relative path)
            rel_path = meta['raw_data_path'].lstrip('/\\')
            raw_path = Path(data_dir) / rel_path
            
            if not raw_path.exists():
                # Fallback to absolute check or check if relative to root
                if (Path(data_dir).parent.parent / rel_path).exists():
                     raw_path = Path(data_dir).parent.parent / rel_path
                else:
                    # logger.warning(f"Raw file not found: {raw_path}")
                    continue
                
            raw_data = np.load(raw_path)
            
            # Process
            features = preprocessor.extract_all_features(raw_data)
            vector = converter.to_vector(features)
            
            if device_id not in features_by_device:
                features_by_device[device_id] = []
            
            features_by_device[device_id].append(vector)
            sample_count += 1
            
        except Exception as e:
            logger.error(f"Error loading {json_file}: {e}")
            continue
            
    input_dim = len(next(iter(features_by_device.values()))[0]) if sample_count > 0 else 0
    logger.info(f"Loaded {sample_count} samples from {len(features_by_device)} devices")
    return features_by_device, input_dim


def main():
    """Train pipeline with real data"""
    print("\n=== Siamese Network Training ===")
    
    # Load Real Dataset
    features_by_device, input_dim = load_real_dataset()
    
    if not features_by_device or len(features_by_device) < 2:
        print("!! INSUFFICIENT DATA !!")
        print("Need at least 2 devices with samples to train.")
        print("Run 'python auto_collect.py' to generate data.")
        return

    embedding_dim = 128
    
    print(f"Input Feature Dimension: {input_dim}")
    print(f"Devices: {len(features_by_device)}")
    
    # Create datasets
    # Adjust samples_per_epoch based on available data size
    total_samples = sum(len(v) for v in features_by_device.values())
    samples_per_epoch = max(100, total_samples * 2)
    
    train_dataset = TripletDataset(features_by_device, samples_per_epoch=samples_per_epoch)
    
    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=min(32, samples_per_epoch), shuffle=True)
    
    # Create model
    model = SiameseNetwork(input_dim=input_dim, embedding_dim=embedding_dim)
    
    # Create trainer
    trainer = ModelTrainer(
        model=model,
        loss_type='triplet',
        learning_rate=0.001
    )
    
    # Train
    print("\n=== Starting Training Loop ===")
    history = trainer.train(
        train_loader=train_loader,
        val_loader=None, # storing validation data is tricky with small datasets, skipping for now
        epochs=10,
        save_dir="./model/checkpoints"
    )
    
    print("\n=== Training Completed ===")
    print(f"Final Loss: {history['train_loss'][-1]:.4f}")
    
    # Save as best_model for server to pick up
    best_path = Path("model/checkpoints/best_model.pt")
    server_path = Path("server/models/best_model.pt")
    
    import shutil
    server_path.parent.mkdir(parents=True, exist_ok=True)
    shutil.copy(best_path, server_path)
    print(f"Deployed model to {server_path}")


if __name__ == "__main__":
    main()
</file>

<file path="noise_collection/mic_noise.py">
"""
Microphone Background Noise Collection
Captures ambient noise and microphone self-noise
"""

import numpy as np
from typing import Optional, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    import sounddevice as sd  # type: ignore
except ImportError:  # pragma: no cover
    sd = None


class MicrophoneNoiseCollector:
    """Collects ambient and self-noise from microphone"""
    
    def __init__(
        self, 
        sample_rate: int = 44100,
        device: Optional[int] = None
    ):
        """
        Initialize microphone noise collector
        
        Args:
            sample_rate: Audio sample rate in Hz
            device: Audio device index (None for default)
        """
        self.sample_rate = sample_rate
        self.device = device
        
    def list_devices(self):
        """List available audio devices"""
        if sd is None:
            raise ImportError(
                "Microphone features require the 'sounddevice' package. "
                "Install it with: pip install sounddevice"
            )
        print("\n=== Available Audio Devices ===")
        print(sd.query_devices())
    
    def capture_ambient_noise(
        self, 
        duration: float = 1.0,
        channels: int = 1
    ) -> Optional[np.ndarray]:
        """
        Capture ambient noise from microphone
        
        Args:
            duration: Recording duration in seconds
            channels: Number of audio channels (1 for mono, 2 for stereo)
            
        Returns:
            Numpy array of audio samples or None if failed
        """
        try:
            if sd is None:
                raise ImportError(
                    "Microphone features require the 'sounddevice' package. "
                    "Install it with: pip install sounddevice"
                )
            logger.info(f"Recording {duration}s of ambient noise...")
            
            # Record audio
            recording = sd.rec(
                int(duration * self.sample_rate),
                samplerate=self.sample_rate,
                channels=channels,
                device=self.device,
                dtype='float32'
            )
            
            # Wait for recording to complete
            sd.wait()
            
            # Flatten if mono or convert stereo to mono
            if channels > 1:
                recording = np.mean(recording, axis=1)
            else:
                recording = recording.flatten()
            
            logger.info(f"Captured {len(recording)} audio samples")
            return recording
            
        except Exception as e:
            logger.error(f"Failed to capture audio: {e}")
            return None
    
    def capture_multiple_samples(
        self, 
        num_samples: int = 5,
        duration: float = 0.5
    ) -> list:
        """
        Capture multiple noise samples
        
        Args:
            num_samples: Number of samples to capture
            duration: Duration of each sample
            
        Returns:
            List of audio sample arrays
        """
        samples = []
        
        for i in range(num_samples):
            sample = self.capture_ambient_noise(duration=duration)
            
            if sample is not None:
                samples.append(sample)
                logger.info(f"Captured sample {i+1}/{num_samples}")
            else:
                logger.warning(f"Failed to capture sample {i+1}")
        
        return samples
    
    def extract_noise_features(self, audio: np.ndarray) -> dict:
        """
        Extract statistical features from audio noise
        
        Args:
            audio: Audio sample array
            
        Returns:
            Dictionary of noise features
        """
        features = {
            'mean': float(np.mean(audio)),
            'std': float(np.std(audio)),
            'variance': float(np.var(audio)),
            'min': float(np.min(audio)),
            'max': float(np.max(audio)),
            'rms': float(np.sqrt(np.mean(audio**2))),
            'zero_crossings': int(np.sum(np.diff(np.sign(audio)) != 0)),
            'peak_to_peak': float(np.ptp(audio))
        }
        
        return features
    
    def get_frequency_spectrum(self, audio: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Compute frequency spectrum of audio noise
        
        Args:
            audio: Audio sample array
            
        Returns:
            Tuple of (frequencies, magnitudes)
        """
        # Apply FFT
        fft = np.fft.rfft(audio)
        magnitude = np.abs(fft)
        
        # Frequency bins
        freqs = np.fft.rfftfreq(len(audio), 1/self.sample_rate)
        
        return freqs, magnitude
    
    def compute_spectral_entropy(self, audio: np.ndarray) -> float:
        """
        Compute spectral entropy of audio signal
        
        Args:
            audio: Audio sample array
            
        Returns:
            Spectral entropy value
        """
        # Get magnitude spectrum
        _, magnitude = self.get_frequency_spectrum(audio)
        
        # Normalize to probability distribution
        power = magnitude ** 2
        power_norm = power / np.sum(power)
        
        # Remove zeros to avoid log(0)
        power_norm = power_norm[power_norm > 0]
        
        # Calculate entropy
        entropy = -np.sum(power_norm * np.log2(power_norm))
        
        return float(entropy)
    
    def get_high_frequency_noise(
        self, 
        audio: np.ndarray,
        cutoff_freq: float = 10000
    ) -> np.ndarray:
        """
        Extract high-frequency noise component
        
        Args:
            audio: Audio sample array
            cutoff_freq: High-pass filter cutoff frequency
            
        Returns:
            High-frequency noise array
        """
        # Apply FFT
        fft = np.fft.rfft(audio)
        freqs = np.fft.rfftfreq(len(audio), 1/self.sample_rate)
        
        # High-pass filter
        fft[freqs < cutoff_freq] = 0
        
        # Inverse FFT
        filtered = np.fft.irfft(fft, len(audio))
        
        return filtered


def main():
    """Test microphone noise collection"""
    collector = MicrophoneNoiseCollector(sample_rate=44100)
    
    print("\n=== Microphone Noise Collection Test ===")
    
    # List available devices
    collector.list_devices()
    
    # Capture single sample
    print("\n=== Capturing Ambient Noise ===")
    print("Keep quiet for best noise isolation...")
    
    audio = collector.capture_ambient_noise(duration=1.0)
    
    if audio is not None:
        print(f"Audio samples: {len(audio)}")
        print(f"Duration: {len(audio)/collector.sample_rate:.2f}s")
        print(f"Sample rate: {collector.sample_rate} Hz")
        
        # Extract features
        print("\n=== Noise Features ===")
        features = collector.extract_noise_features(audio)
        for key, value in features.items():
            print(f"{key}: {value:.6f}")
        
        # Spectral analysis
        print("\n=== Spectral Analysis ===")
        freqs, magnitude = collector.get_frequency_spectrum(audio)
        print(f"Frequency bins: {len(freqs)}")
        print(f"Max magnitude: {magnitude.max():.2f}")
        print(f"Max frequency: {freqs[np.argmax(magnitude)]:.2f} Hz")
        
        entropy = collector.compute_spectral_entropy(audio)
        print(f"Spectral entropy: {entropy:.4f} bits")
        
        # High-frequency noise
        print("\n=== High-Frequency Noise ===")
        hf_noise = collector.get_high_frequency_noise(audio, cutoff_freq=8000)
        print(f"HF noise samples: {len(hf_noise)}")
        print(f"HF noise RMS: {np.sqrt(np.mean(hf_noise**2)):.6f}")
    
    # Capture multiple samples
    print("\n=== Capturing Multiple Samples ===")
    samples = collector.capture_multiple_samples(num_samples=3, duration=0.5)
    print(f"Collected {len(samples)} samples")
    
    # Compare entropy across samples
    print("\n=== Entropy Comparison ===")
    for i, sample in enumerate(samples):
        ent = collector.compute_spectral_entropy(sample)
        print(f"Sample {i+1} entropy: {ent:.4f} bits")


if __name__ == "__main__":
    main()
</file>

<file path="start.bat">
@echo off
echo Starting QNA-Auth System
echo.

echo Starting Backend Server...
start "QNA-Auth Backend" cmd /k "cd /d %~dp0 && call .venv\Scripts\activate.bat && python server/app.py"

timeout /t 3 /nobreak >nul

echo Starting Frontend Server...
start "QNA-Auth Frontend" cmd /k "cd /d %~dp0frontend && npm run dev"

echo.
echo Both servers started!
echo Backend: http://localhost:8000
echo Frontend: http://localhost:3000
echo.
pause
</file>

<file path="README.md">
# QNA-Auth (Quantum Noise Assisted Authentication)

QNA-Auth is an experimental **device authentication** system that derives a **device "fingerprint" embedding** from high-entropy noise sources (QRNG / camera sensor noise / microphone noise / system jitter) and uses a Siamese-style embedding model + similarity thresholding to authenticate a device.

This repository includes:
- A **FastAPI backend** that exposes enrollment/authentication APIs and persists enrolled device embeddings.
- A **React + TypeScript frontend** that calls the backend and provides an interactive UI.
- **Noise collection modules** for QRNG + hardware entropy sources.
- A **feature extraction pipeline** that converts raw noise arrays into fixed-length feature vectors.
- A **PyTorch Siamese embedding model** plus training/evaluation utilities.

> Note: This is a capstone-style research project. The security properties are **educational/prototypical** and should be reviewed carefully before any real-world use.

---

## What the system does (high-level)

### Enrollment (creates the "stored reference")
1. Collect \(N\) raw noise samples from one or more sources (e.g., `qrng`, `camera`, `microphone`).
2. For each sample: extract a **feature dictionary** (stats/entropy/FFT/autocorr/complexity).
3. Convert features to a **fixed feature vector** (sorted feature names) and run it through the embedder.
4. Aggregate embeddings across samples (mean by default) to produce a **single device embedding**.
5. Save:
   - `auth/device_embeddings/<device_id>_embedding.pt` (PyTorch tensor)
   - `auth/device_embeddings/<device_id>_metadata.json` (metadata for UI + debugging)

### Authentication (verifies a new "fresh" reading)
1. Collect fresh noise samples from the selected sources.
2. Generate a fresh authentication embedding (average across samples).
3. Compare fresh embedding to stored embedding using a similarity metric (default **cosine**).
4. Authenticate if similarity >= threshold (default **0.85** in the server).

### Challenge/Response (anti-replay "nonce" protocol)
The backend also exposes `/challenge` and `/verify` endpoints that implement:
- A server-generated **nonce** (short-lived)
- A response derived from the stored embedding + nonce (HMAC-like)
- A second factor check: **embedding similarity** must still meet threshold

---

## Repository structure (map)

```text
QNA-Auth/
  auth/                  # Enrollment + authentication + challenge/response logic
  noise_collection/      # QRNG + camera + microphone + system jitter collectors
  preprocessing/         # Feature extraction + preprocessing utilities
  model/                 # Siamese model, training, evaluation
  dataset/               # Dataset builder for storing raw samples/metadata
  server/                # FastAPI app exposing REST endpoints
  frontend/              # React/Vite UI
  *.py / *.bat / *.sh    # Scripts for setup, testing, and demos
```

---

## Quick start (recommended)

You can follow the dedicated quickstart at `QUICKSTART.md`, but here is the consolidated version.

### Prerequisites
- **Python**: 3.8+ (3.10+ recommended)
- **Node.js**: 18+ (frontend)
- Optional hardware:
  - Webcam (for `camera` source)
  - Microphone (for `microphone` source)

### Backend setup

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# macOS/Linux
# source venv/bin/activate

pip install -r requirements.txt
python server/app.py
```

Backend runs on `http://localhost:8000`
- OpenAPI/Swagger UI: `http://localhost:8000/docs`

### Frontend setup

```bash
cd frontend
npm install
npm run dev
```

Frontend runs on `http://localhost:3000`

---

## Configuration

### `config.example.py` (optional template)
There is a `config.example.py` you can copy to `config.py`. **Important:** the current backend (`server/app.py`) mostly uses **hard-coded defaults** (e.g., threshold=0.85) and does **not** fully wire `config.py` everywhere. Treat `config.example.py` primarily as a ?future config? template.

### QRNG API key
`auth/enrollment.py` attempts to read `QRNG_API_KEY` from the environment, otherwise it uses a fallback string.

Set an environment variable to override:

```bash
# PowerShell
$env:QRNG_API_KEY="your_key_here"

# bash/zsh
export QRNG_API_KEY="your_key_here"
```

Security note: **do not commit real API keys**. Prefer `.env` + environment variables.

---

## API reference (backend)

Base URL: `http://localhost:8000`

### Health
- `GET /health`
  - Returns whether the model and core services initialized successfully.

### Enrollment
- `POST /enroll`
  - Body:
    - `device_name` (optional)
    - `num_samples` (10?200)
    - `sources` (list; e.g. `["qrng"]`, `["camera","microphone"]`)
  - Returns: `device_id`, status/message, and stored metadata.

### Authentication (simple)
- `POST /authenticate`
  - Body:
    - `device_id`
    - `sources`
    - `num_samples_per_source` (1?20)
  - Returns: authenticated boolean + details, or 401 on failure.

### Challenge/response
- `POST /challenge`
  - Body: `device_id`
  - Returns: `challenge_id`, `nonce`, `expires_at`

- `POST /verify`
  - Body:
    - `challenge_id`
    - `device_id`
    - `response` (signature string)
    - `noise_samples` (list of float arrays; fresh raw noise)
  - Returns: authenticated boolean + similarity + protocol details, or 401 on failure.

### Device management
- `GET /devices` ? list device IDs
- `GET /devices/{device_id}` ? read stored metadata JSON
- `DELETE /devices/{device_id}` ? delete `*_embedding.pt` + `*_metadata.json`

---

## Frontend behavior

The frontend calls backend endpoints via `axios`:
- Default base URL is `http://localhost:8000` (or `VITE_API_URL` if set).

Pages:
- `/` Home
- `/enroll` Enroll a device
- `/authenticate` Authenticate a device
- `/devices` List/view/delete enrolled devices

---

## How data flows through the code (end-to-end)

### Enrollment flow (important call chain)
1. `server/app.py` ? `POST /enroll`
2. `auth/enrollment.py` ? `DeviceEnroller.enroll_device(...)`
3. `noise_collection/*` ? collects raw samples (`np.ndarray`)
4. `preprocessing/features.py` ? `NoisePreprocessor.extract_all_features(...)`
5. `preprocessing/features.py` ? `FeatureVector.to_vector(...)`
6. `model/siamese_model.py` ? `DeviceEmbedder.embed(...)`
7. `auth/enrollment.py` ? aggregate embeddings + save files

### Authentication flow (simple endpoint)
1. `server/app.py` ? `POST /authenticate`
2. `auth/authentication.py` ? `DeviceAuthenticator.authenticate(...)`
3. Collect fresh noise per source (same noise collectors)
4. Generate fresh embedding ? compute similarity vs stored embedding

### Challenge/response flow
1. `server/app.py` ? `POST /challenge` ? `auth/challenge_response.py` creates nonce and stores it temporarily in-memory
2. `server/app.py` ? `POST /verify`:
   - loads stored embedding
   - generates fresh embedding from `noise_samples`
   - verifies nonce response signature and embedding similarity

---

## Scripts and common tasks

### Verify CUDA
- `python verify_cuda.py`

### Test hardware access
- `python test_hardware.py`
  - Uses OpenCV and `sounddevice` directly to confirm camera/mic permissions.

### Test collection + feature extraction
- `python test_collection.py`

### Test enrollment/auth via HTTP
- `python test_enrollment.py`
  - Note: this script sends `noise_sources` in JSON, but the server expects `sources`.
  - If it fails, update the payload keys or use the frontend/UI.

### Collect data via API for training
- `python collect_training_data.py`
  - Enrolls multiple ?devices? via repeated enrollment calls to build up stored embeddings.

### Start scripts
- `setup.bat` / `setup.sh`: create venv, install deps, create folders, copy `config.py`
- `start.bat`: starts backend + frontend, but **contains a hard-coded Python path** (`D:/QNA-Auth/venv/...`). You will likely need to edit it to point to your local `venv`.

---

## Detailed file-by-file guide (what each file does)

This section explains the ?meaningful? files (source code + scripts). Generated artifacts (e.g. `frontend/package-lock.json`) are not described in depth.

### Root
- `README.md`: This documentation (project overview + guide).
- `QUICKSTART.md`: Shorter quickstart instructions.
- `requirements.txt`: Python dependencies (FastAPI, PyTorch, OpenCV, sounddevice, etc.).
- `config.example.py`: Example configuration template (copy to `config.py`).
- `setup.bat`: Windows bootstrap (venv + pip install + folder creation).
- `setup.sh`: macOS/Linux bootstrap (same intent as above).
- `start.bat`: Convenience starter for backend + frontend (hard-coded venv path to fix).
- `verify_cuda.py`: Prints CUDA availability and GPU details.
- `test_hardware.py`: Tests whether camera/mic can be opened/recorded.
- `test_collection.py`: Smoke test: camera/mic noise ? feature extraction ? vectorization.
- `test_enrollment.py`: HTTP smoke test for backend endpoints (payload keys may not match).
- `collect_training_data.py`: Uses the running backend to enroll multiple devices for training/demo.

### `server/` (FastAPI backend)
- `server/app.py`:
  - Creates the FastAPI app, configures CORS, initializes global services on startup.
  - Builds the preprocessing pipeline and detects the feature vector dimension dynamically.
  - Creates a `DeviceEmbedder` (loads `./model/checkpoints/best_model.pt` if it exists).
  - Exposes endpoints: `/health`, `/enroll`, `/authenticate`, `/challenge`, `/verify`, `/devices`, `/devices/{id}`, deletion.
- `server/routes.py`:
  - Placeholder router for future endpoints like `/stats` and `/metrics` (not currently used by `app.py`).

### `auth/` (authentication domain logic)
- `auth/__init__.py`: Re-exports the main classes for convenience imports.
- `auth/enrollment.py`:
  - `DeviceEnroller`: collect samples, extract features, embed, aggregate, and persist device embeddings + metadata.
  - Knows where embeddings are stored (`./auth/device_embeddings` by default).
- `auth/authentication.py`:
  - `DeviceAuthenticator`: collects fresh samples, produces an auth embedding, compares vs stored embedding (cosine by default).
  - `AuthenticationSession`: optional retry/session wrapper.
- `auth/challenge_response.py`:
  - `ChallengeResponseProtocol`: creates nonce challenges and verifies responses (in-memory active challenge store).
  - `SecureAuthenticationFlow`: combines challenge-response validity + embedding similarity check.
  - `AntiReplayProtection`: additional replay protection utility (not wired into the server endpoints by default).

### `noise_collection/` (entropy sources)
- `noise_collection/__init__.py`: Exposes collector classes: `QRNGClient`, `CameraNoiseCollector`, `MicrophoneNoiseCollector`, `SensorNoiseCollector`.
- `noise_collection/qrng_api.py`:
  - `QRNGClient`: fetches quantum random bytes (ANU by default; QRNG.org supported if key provided).
  - `fetch_multiple_samples` intentionally **does not** fallback to pseudo-random values.
- `noise_collection/camera_noise.py`:
  - `CameraNoiseCollector`: captures dark frames, extracts noise by subtracting a blurred image, returns flattened noise arrays.
- `noise_collection/mic_noise.py`:
  - `MicrophoneNoiseCollector`: records audio noise via `sounddevice` and returns raw sample arrays (or lists).
- `noise_collection/sensor_noise.py`:
  - `SensorNoiseCollector`: collects timing jitter + CPU usage + memory access timing + disk/network stats; can concatenate into a composite signature.
- `noise_collection/camera_noise.py`, `mic_noise.py`, `sensor_noise.py` each have a `main()` for manual testing.

### `preprocessing/` (feature extraction)
- `preprocessing/features.py`:
  - `NoisePreprocessor`: statistical features + Shannon entropy + FFT features + autocorrelation + complexity (approx entropy + Hurst exponent).
  - `FeatureVector`: converts feature dictionaries to stable vectors using sorted keys (auto-initializes feature order on first call).
- `preprocessing/utils.py`:
  - Utility helpers: sliding window, augmentation, pad/truncate, batch processing, SNR, merging multiple sources, etc.

### `model/` (ML model + training/evaluation)
- `model/siamese_model.py`:
  - `EmbeddingNetwork` and `SiameseNetwork`
  - `TripletLoss` + `ContrastiveLoss`
  - `DeviceEmbedder`: wraps the model and provides `embed()` + `compute_similarity()`.
- `model/train.py`:
  - `TripletDataset` and `PairDataset` to generate training tuples/pairs from per-device feature vectors.
  - `ModelTrainer`: training loop, checkpointing, LR scheduler.
- `model/evaluate.py`:
  - `ModelEvaluator`: generates embeddings, computes similarity distributions, finds a threshold, plots ROC/PR curves, and produces a report.

### `dataset/` (dataset builder / storage)
- `dataset/builder.py`:
  - `DatasetBuilder`: stores raw noise arrays as `.npy`, metadata as JSON, and a summary row in CSV.
  - Useful for building training corpora beyond the ?store one embedding per device? approach.

### `frontend/` (React UI)
- `frontend/package.json`: frontend scripts (`dev`, `build`, `preview`, `lint`) and dependencies.
- `frontend/vite.config.ts`: Vite dev server config; runs on port 3000 and defines a proxy for `/api` (not required by current UI).
- `frontend/src/services/api.ts`: axios client + typed API wrappers (`enrollDevice`, `authenticateDevice`, device management).
- `frontend/src/App.tsx`: router + navigation.
- `frontend/src/pages/HomePage.tsx`: landing page.
- `frontend/src/pages/EnrollPage.tsx`: enrollment form + status display.
- `frontend/src/pages/AuthenticatePage.tsx`: device dropdown + authenticate flow.
- `frontend/src/pages/DevicesPage.tsx`: list devices, view metadata, delete device.
- `frontend/src/components/ui/background-beams.tsx`: background visual component used by layout.

---

## Troubleshooting

### Camera / Microphone not working
- Run `python test_hardware.py` to diagnose device availability and permissions.
- For camera: ensure no other app is using the webcam.
- For mic: ensure Windows microphone permissions allow Python to record input.

### QRNG failures
- Ensure you have internet connectivity.
- Expect timeouts/rate limits; retry later.
- If you have an API key, set `QRNG_API_KEY` and `noise_collection/qrng_api.py` will use authenticated requests.

### Low authentication accuracy
- Increase samples during enrollment (`num_samples`).
- Use multiple sources (camera + microphone typically adds more device-specific entropy than QRNG alone).
- Consider training and loading a better checkpoint in `model/checkpoints/best_model.pt`.
- Tune similarity threshold in `server/app.py` (currently hard-coded).

---

## Notes on limitations (important)

- The backend stores embeddings and challenges **in-process** (no database). Restarting the server clears in-memory active challenges.
- `server/app.py` loads a model from `./model/checkpoints/best_model.pt` if it exists; otherwise it runs with random weights.
- Some scripts are demos and may drift from the current API shape (e.g., `test_enrollment.py` payload field names).

# QNA-Auth - Quantum Noise Assisted Authentication

A novel authentication system that uses quantum noise samples and machine learning to authenticate devices in a secure, non-reproducible way.

## ?? Features

- **Quantum Random Number Generation**: Fetches true quantum noise from ANU QRNG service
- **Multi-Source Noise Collection**: Camera dark frames, microphone ambient noise, and system sensors
- **Siamese Neural Network**: Creates unique, non-invertible device embeddings
- **Challenge-Response Protocol**: Secure authentication with nonce-based verification
- **FastAPI Backend**: RESTful API for enrollment and authentication
- **React TypeScript Frontend**: Modern UI for device management

## ??? Project Structure

```
qna-auth/
??? noise_collection/          # Noise sampling modules
?   ??? qrng_api.py           # Quantum RNG client
?   ??? camera_noise.py       # Camera dark frame capture
?   ??? mic_noise.py          # Microphone noise capture
?   ??? sensor_noise.py       # System sensor jitter
??? dataset/                   # Dataset management
?   ??? builder.py            # Dataset builder
?   ??? samples/              # Stored samples
??? preprocessing/             # Feature extraction
?   ??? features.py           # Statistical & FFT features
?   ??? utils.py              # Preprocessing utilities
??? model/                     # ML models
?   ??? siamese_model.py      # Siamese network architecture
?   ??? train.py              # Training script
?   ??? evaluate.py           # Model evaluation
??? auth/                      # Authentication modules
?   ??? enrollment.py         # Device enrollment
?   ??? authentication.py     # Authentication logic
?   ??? challenge_response.py # Challenge-response protocol
??? server/                    # FastAPI backend
?   ??? app.py               # Main application
?   ??? routes.py            # Additional routes
??? frontend/                  # React TypeScript UI
    ??? src/
    ?   ??? pages/           # Page components
    ?   ??? services/        # API service
    ?   ??? App.tsx          # Main app
    ??? package.json

```

## ?? Getting Started

### Prerequisites

- Python 3.8+
- Node.js 18+ (for frontend)
- Webcam (optional, for camera noise)
- Microphone (optional, for audio noise)

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd QNA-Auth
```

2. **Install Python dependencies**
```bash
pip install -r requirements.txt
```

3. **Install frontend dependencies**
```bash
cd frontend
npm install
```

### Running the Application

1. **Start the backend server**
```bash
python server/app.py
```
The API will be available at `http://localhost:8000`

2. **Start the frontend (in a new terminal)**
```bash
cd frontend
npm run dev
```
The UI will be available at `http://localhost:3000`

## ?? Usage

### 1. Enroll a Device

Navigate to the Enroll page and:
- Choose noise sources (QRNG, camera, microphone)
- Set number of samples (50-100 recommended)
- Click "Enroll Device"
- Save the generated device ID

### 2. Authenticate a Device

Navigate to the Authenticate page and:
- Select an enrolled device
- Choose the same noise sources used during enrollment
- Click "Authenticate"
- View authentication result and similarity score

### 3. Manage Devices

Navigate to the Devices page to:
- View all enrolled devices
- See device metadata
- Delete devices

## ?? API Endpoints

- `GET /health` - Health check
- `POST /enroll` - Enroll a new device
- `POST /authenticate` - Authenticate a device
- `POST /challenge` - Create authentication challenge
- `POST /verify` - Verify challenge response
- `GET /devices` - List enrolled devices
- `GET /devices/{id}` - Get device details
- `DELETE /devices/{id}` - Delete device

## ?? Testing Individual Modules

Each module can be tested independently:

```bash
# Test quantum noise collection
python -m noise_collection.qrng_api

# Test camera noise collection
python -m noise_collection.camera_noise

# Test microphone noise collection
python -m noise_collection.mic_noise

# Test preprocessing
python -m preprocessing.features

# Test model
python -m model.siamese_model

# Test enrollment
python -m auth.enrollment

# Test authentication
python -m auth.authentication
```

## ?? Training the Model

To train the Siamese network with your own data:

1. Collect data from multiple devices:
```python
from dataset.builder import DatasetBuilder
from noise_collection import QRNGClient

builder = DatasetBuilder()
qrng = QRNGClient()

# Collect samples for each device
for device_id in device_ids:
    samples = qrng.fetch_multiple_samples(num_samples=50)
    builder.add_batch(device_id, 'qrng', samples)
```

2. Train the model:
```python
from model.train import ModelTrainer, TripletDataset
from model.siamese_model import SiameseNetwork
import torch

# Create model
model = SiameseNetwork(input_dim=50, embedding_dim=128)

# Create dataset
dataset = TripletDataset(features_by_device, samples_per_epoch=1000)
loader = DataLoader(dataset, batch_size=32)

# Train
trainer = ModelTrainer(model, loss_type='triplet')
trainer.train(loader, epochs=50)
```

## ?? Security Considerations

- **Quantum Randomness**: Uses ANU QRNG for true quantum random numbers
- **Non-Invertible Embeddings**: Embeddings cannot be reversed to recover original noise
- **Challenge-Response**: Prevents replay attacks with time-limited nonces
- **Threshold-Based**: Configurable similarity threshold for authentication
- **Multi-Factor**: Combines multiple noise sources for robustness

## ?? Configuration

Copy `config.example.py` to `config.py` and adjust settings:

```python
# Authentication threshold
AUTH_CONFIG = {
    "similarity_threshold": 0.85,  # Adjust based on requirements
    "similarity_metric": "cosine",
}

# Model architecture
MODEL_CONFIG = {
    "input_dim": 50,
    "embedding_dim": 128,
    "hidden_dims": [256, 256, 128]
}
```

## ?? Architecture

### Noise Collection
- **QRNG**: Fetches quantum random bits from ANU quantum service
- **Camera**: Captures sensor noise from dark frames
- **Microphone**: Records ambient noise and self-noise
- **Sensors**: Collects timing jitter and system noise

### Feature Extraction
- Statistical features (mean, std, skewness, kurtosis)
- FFT-based frequency features
- Entropy calculations
- Autocorrelation analysis

### Machine Learning
- Siamese network with shared weights
- Triplet loss or contrastive loss
- L2-normalized embeddings
- Cosine similarity for verification

### Authentication
- Multi-sample collection for robustness
- Embedding aggregation (mean/median)
- Challenge-response protocol
- Configurable threshold

## ?? Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ?? License

This project is licensed under the MIT License.

## ?? Acknowledgments

- ANU Quantum Random Numbers: https://qrng.anu.edu.au/
- PyTorch team for the deep learning framework
- FastAPI for the modern web framework

## ?? Contact

For questions or support, please open an issue on GitHub.

---

**Built with quantum randomness and machine learning** ???
</file>

<file path="server/app.py">
"""
FastAPI Backend Server for QNA-Auth
Implements REST API for device enrollment and authentication
"""

from __future__ import annotations

import sys
from pathlib import Path

# Ensure project root is on sys.path when running `python server/app.py`
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from fastapi import FastAPI, HTTPException, BackgroundTasks, status
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict
import torch
import numpy as np
import logging

# Import QNA-Auth modules
from model.siamese_model import SiameseNetwork, DeviceEmbedder
from dataset.builder import DatasetBuilder
from preprocessing.features import NoisePreprocessor, FeatureVector
from auth import (
    DeviceEnroller,
    DeviceAuthenticator,
    ChallengeResponseProtocol,
    SecureAuthenticationFlow,
    AuthenticationSession
)
import config  # Import configuration

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create FastAPI app
app = FastAPI(
    title="QNA-Auth API",
    description="Quantum Noise Assisted Authentication System",
    version="1.0.0"
)

@app.get("/")
async def root():
    return {"status": "online", "message": "QNA-Auth Server is running"}

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global state (in production, use proper dependency injection)
class AppState:
    embedder: Optional[DeviceEmbedder] = None
    preprocessor: Optional[NoisePreprocessor] = None
    feature_converter: Optional[FeatureVector] = None
    enroller: Optional[DeviceEnroller] = None
    authenticator: Optional[DeviceAuthenticator] = None
    challenge_protocol: Optional[ChallengeResponseProtocol] = None
    auth_flow: Optional[SecureAuthenticationFlow] = None

state = AppState()


# Pydantic models
class EnrollmentRequest(BaseModel):
    device_name: Optional[str] = Field(None, description="Optional device name")
    num_samples: int = Field(50, description="Number of noise samples to collect", ge=10, le=200)
    sources: List[str] = Field(['qrng'], description="Noise sources to use")
    client_samples: Optional[Dict[str, List[List[float]]]] = Field(None, description="Optional raw noise samples provided by client")


class EnrollmentResponse(BaseModel):
    device_id: str
    status: str
    message: str
    metadata: Dict


class AuthenticationRequest(BaseModel):
    device_id: str = Field(..., description="Device identifier to authenticate")
    sources: List[str] = Field(['qrng'], description="Noise sources to use")
    num_samples_per_source: int = Field(5, description="Samples per source", ge=1, le=20)
    client_samples: Optional[Dict[str, List[List[float]]]] = Field(None, description="Optional raw noise samples provided by client")


class ChallengeRequest(BaseModel):
    device_id: str = Field(..., description="Device identifier")


class ChallengeResponse(BaseModel):
    challenge_id: str
    nonce: str
    expires_at: str


class VerifyRequest(BaseModel):
    challenge_id: str = Field(..., description="Challenge identifier")
    response: str = Field(..., description="Challenge response signature")
    device_id: str = Field(..., description="Device identifier")
    noise_samples: List[List[float]] = Field(..., description="Fresh noise samples for embedding")


class VerifyResponse(BaseModel):
    authenticated: bool
    similarity: float
    details: Dict


class DeviceListResponse(BaseModel):
    devices: List[str]
    count: int


class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    components_initialized: bool


# Startup event
@app.on_event("startup")
async def startup_event():
    """Initialize system components"""
    logger.info("Initializing QNA-Auth system...")
    
    try:
        # Initialize preprocessor first to determine feature dimension
        # DISABLE normalization to preserve amplitude/loudness differences between sensors
        state.preprocessor = NoisePreprocessor(normalize=False)
        state.feature_converter = FeatureVector()
        
        # Extract features from a dummy sample to get the actual feature dimension
        dummy_sample = np.random.randn(1024)
        dummy_features = state.preprocessor.extract_all_features(dummy_sample)
        dummy_vector = state.feature_converter.to_vector(dummy_features)
        input_dim = len(dummy_vector)
        
        logger.info(f"Detected feature dimension: {input_dim}")
        
        embedding_dim = 128
        
        # Create embedder (this creates and initializes the model on device)
        state.embedder = DeviceEmbedder(input_dim=input_dim, embedding_dim=embedding_dim, device=config.DEVICE)
        
        # Load model if checkpoint exists
        # Use path from config, or fallback to the one in server/models
        model_path = Path(config.MODEL_PATH)
        
        if model_path.exists():
            state.embedder.load_model(str(model_path))
            logger.info(f"Loaded trained model from {model_path}")
        else:
            logger.warning(f"No trained model found at {model_path}, using random initialization")
        
        # Initialize dataset builder for training data collection
        dataset_builder = DatasetBuilder()

        # Initialize enroller
        state.enroller = DeviceEnroller(
            embedder=state.embedder,
            preprocessor=state.preprocessor,
            feature_converter=state.feature_converter,
            storage_dir="./auth/device_embeddings",
            dataset_builder=dataset_builder
        )
        
        # Initialize authenticator
        state.authenticator = DeviceAuthenticator(
            embedder=state.embedder,
            preprocessor=state.preprocessor,
            feature_converter=state.feature_converter,
            enroller=state.enroller,
            threshold=0.85
        )
        
        # Initialize challenge-response protocol
        state.challenge_protocol = ChallengeResponseProtocol(
            nonce_length=32,
            challenge_expiry_seconds=60
        )
        
        state.auth_flow = SecureAuthenticationFlow(
            protocol=state.challenge_protocol,
            similarity_threshold=0.85
        )
        
        logger.info("QNA-Auth system initialized successfully")
        
    except Exception as e:
        logger.error(f"Failed to initialize system: {e}")
        raise


# Health check endpoint
@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Check system health"""
    return {
        "status": "healthy",
        "model_loaded": state.embedder is not None,
        "components_initialized": all([
            state.embedder,
            state.preprocessor,
            state.enroller,
            state.authenticator,
            state.challenge_protocol
        ])
    }


# Enrollment endpoint
@app.post("/enroll", response_model=EnrollmentResponse, status_code=status.HTTP_201_CREATED)
async def enroll_device(request: EnrollmentRequest, background_tasks: BackgroundTasks):
    """
    Enroll a new device
    
    This endpoint collects noise samples and creates a device embedding
    """
    if not state.enroller:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Enrollment service not initialized"
        )
    
    try:
        logger.info(f"Enrollment request: device_name={request.device_name}, "
                   f"num_samples={request.num_samples}, sources={request.sources}")
        
        # Enroll device
        device_id = state.enroller.enroll_device(
            device_name=request.device_name,
            num_samples=request.num_samples,
            sources=request.sources,
            client_samples=request.client_samples
        )
        
        # Load metadata
        metadata_path = state.enroller.storage_dir / f"{device_id}_metadata.json"
        import json
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        return {
            "device_id": device_id,
            "status": "success",
            "message": f"Device enrolled successfully",
            "metadata": metadata
        }
        
    except Exception as e:
        logger.error(f"Enrollment failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Enrollment failed: {str(e)}"
        )


# Authentication endpoint (simple version)
@app.post("/authenticate")
async def authenticate_device(request: AuthenticationRequest):
    """
    Authenticate a device using noise samples
    
    This is a simplified authentication flow
    """
    if not state.authenticator:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Authentication service not initialized"
        )
    
    try:
        logger.info(f"Authentication request: device_id={request.device_id}")
        
        # DEBUG: Check if client_samples arrived
        if request.client_samples:
            logger.info("API: received client_samples in request")
            logger.info(f"API: keys={list(request.client_samples.keys())}")
            for k, v in request.client_samples.items():
                logger.info(f"API: source {k} has {len(v)} samples")
        else:
            logger.warning("API: request.client_samples is None or Empty")

        # Authenticate
        is_authenticated, details = state.authenticator.authenticate(
            device_id=request.device_id,
            sources=request.sources,
            num_samples_per_source=request.num_samples_per_source,
            client_samples=request.client_samples
        )
        
        if is_authenticated:
            return {
                "authenticated": True,
                "device_id": request.device_id,
                "details": details
            }
        else:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Authentication failed",
                headers={"WWW-Authenticate": "Bearer"}
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Authentication error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Authentication error: {str(e)}"
        )


# Challenge-response endpoints
@app.post("/challenge", response_model=ChallengeResponse)
async def create_challenge(request: ChallengeRequest):
    """
    Create authentication challenge
    
    Step 1 of challenge-response protocol
    """
    if not state.challenge_protocol:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Challenge protocol not initialized"
        )
    
    try:
        challenge = state.challenge_protocol.create_challenge(request.device_id)
        return challenge
        
    except Exception as e:
        logger.error(f"Challenge creation failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Challenge creation failed: {str(e)}"
        )


@app.post("/verify", response_model=VerifyResponse)
async def verify_challenge_response(request: VerifyRequest):
    """
    Verify challenge response
    
    Step 2 of challenge-response protocol
    """
    if not state.auth_flow or not state.enroller:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Verification service not initialized"
        )
    
    try:
        # Load stored embedding
        stored_embedding = state.enroller.load_device_embedding(request.device_id)
        
        if stored_embedding is None:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Device not enrolled"
            )
        
        # Convert noise samples to embedding
        noise_arrays = [np.array(sample, dtype=np.float32) for sample in request.noise_samples]
        auth_embedding = state.authenticator.generate_authentication_embedding(noise_arrays)
        
        if auth_embedding is None:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Failed to generate authentication embedding"
            )
        
        # Complete authentication
        is_authenticated, details = state.auth_flow.complete_authentication(
            challenge_id=request.challenge_id,
            response=request.response,
            auth_embedding=auth_embedding,
            stored_embedding=stored_embedding
        )
        
        if is_authenticated:
            return {
                "authenticated": True,
                "similarity": details['embedding_similarity'],
                "details": details
            }
        else:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Verification failed",
                headers={"WWW-Authenticate": "Bearer"}
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Verification error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Verification error: {str(e)}"
        )


# Device management endpoints
@app.get("/devices", response_model=DeviceListResponse)
async def list_devices():
    """List all enrolled devices"""
    if not state.enroller:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Enrollment service not initialized"
        )
    
    try:
        devices = state.enroller.list_enrolled_devices()
        return {
            "devices": devices,
            "count": len(devices)
        }
        
    except Exception as e:
        logger.error(f"Failed to list devices: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to list devices: {str(e)}"
        )


@app.get("/devices/{device_id}")
async def get_device_info(device_id: str):
    """Get device metadata"""
    if not state.enroller:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Enrollment service not initialized"
        )
    
    try:
        metadata_path = state.enroller.storage_dir / f"{device_id}_metadata.json"
        
        if not metadata_path.exists():
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Device not found"
            )
        
        import json
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        return metadata
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get device info: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get device info: {str(e)}"
        )


@app.delete("/devices/{device_id}")
async def delete_device(device_id: str):
    """Delete enrolled device"""
    if not state.enroller:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Enrollment service not initialized"
        )
    
    try:
        # Delete embedding and metadata files
        embedding_path = state.enroller.storage_dir / f"{device_id}_embedding.pt"
        metadata_path = state.enroller.storage_dir / f"{device_id}_metadata.json"
        
        if not embedding_path.exists():
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Device not found"
            )
        
        embedding_path.unlink()
        metadata_path.unlink()
        
        logger.info(f"Deleted device: {device_id}")
        
        return {
            "status": "success",
            "message": f"Device {device_id} deleted successfully"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete device: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete device: {str(e)}"
        )


# Root endpoint
@app.get("/")
async def root():
    """API root"""
    return {
        "service": "QNA-Auth API",
        "version": "1.0.0",
        "description": "Quantum Noise Assisted Authentication System",
        "endpoints": {
            "health": "/health",
            "enroll": "/enroll",
            "authenticate": "/authenticate",
            "challenge": "/challenge",
            "verify": "/verify",
            "devices": "/devices"
        }
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
</file>

</files>
