# QNA-Auth: What This Project Is and What It Does

## What the project is

**QNA-Auth** is an experimental **device authentication** system. The idea: use **noise** (from different sources on or around the device) to build a **fingerprint** for that device, then later check “is this the same device?” by comparing a new fingerprint to the one stored at enrollment.

So in one sentence: **same device → similar noise over time → similar fingerprint → authenticate; different device → different noise → different fingerprint → reject.**

---

## The big picture (flow)

1. **Enrollment**  
   A device is “enrolled”: the server collects noise samples from one or more sources (QRNG, camera, microphone), turns them into a **feature vector**, runs that through a **neural embedder**, and saves a single **device embedding** (the “stored fingerprint”).

2. **Authentication**  
   Later, someone claims “I am device X.” The server collects **fresh** noise from the same sources, builds a new embedding, and compares it to the stored one (e.g. cosine similarity). If similarity is above a threshold (e.g. 0.85), authentication succeeds.

3. **Challenge/response**  
   To reduce replay attacks, the server can issue a short-lived **challenge (nonce)**; the client must respond with something derived from the stored embedding + nonce, and the server still checks that the **live** embedding from fresh noise is similar enough.

So the core loop is: **collect noise → features → embedder → embedding → compare**.

---

## What is QRNG and how is it used?

### What QRNG is

- **QRNG** = **Quantum Random Number Generator**.  
  It’s a service that gives you random numbers generated by a **quantum** process (e.g. photon detection, vacuum fluctuations), not by a software algorithm.

- In this project we use the **ANU QRNG** (Australian National University):  
  `https://qrng.anu.edu.au/API/jsonI.php`  
  You send a request (e.g. “give me 1024 random bytes”); they return JSON with an array of numbers. No API key needed for the basic endpoint, but the API is **rate-limited to 1 request per minute**.

- So “QRNG” here = **that online API** that returns quantum random numbers. It’s not a piece of hardware on the user’s device; it’s a remote service.

### Are we currently using QRNG?

**Yes.** The backend and scripts use it as **one of the noise sources**:

- **Enrollment** (`auth/enrollment.py`): If `sources` includes `'qrng'`, the server calls the QRNG client and collects N samples (each sample = one API call, e.g. 1024 bytes).
- **Authentication** (`auth/authentication.py`): Same idea; fresh QRNG samples can be used to build the “live” embedding.
- **API default**: The enrollment and auth request models default to `sources=['qrng']`, so if the client doesn’t specify sources, QRNG is used.
- **Data collection script** (`scripts/collect_data_for_training.py`): Can collect from `qrng`, `camera`, and `microphone`; QRNG uses the same ANU API (with a 65 s delay between calls to respect the 1/min limit).

So **whenever the app or script is configured to use the `qrng` source, we are using the ANU QRNG API.**

### Important caveat: QRNG is not device-specific

- **Camera** and **microphone** noise are **device- (and environment-) specific**: sensor noise, mic self-noise, room acoustics, etc. So “same device” tends to give **similar** noise over time → good for a **stable** fingerprint.

- **QRNG** is **the same for everyone**: it’s a single global stream of random numbers from the internet. Each request gives new random data; there’s nothing tied to *this* laptop or *that* phone. So:
  - **Enrollment with only QRNG**: You store an embedding of “whatever random numbers we got that day.”
  - **Auth later with only QRNG**: You get *different* random numbers → different features → different embedding → **same device will often fail** (low similarity).

So for **device fingerprinting**, QRNG by itself does **not** provide a stable, device-specific signal. It’s useful as:
- A **high-entropy** source (e.g. for challenge/response or mixing with other signals), or  
- A **fallback** when camera/mic aren’t available, with the understanding that matching will be weak unless combined with device-specific sources.

In practice, **camera and/or microphone** are what make the system actually recognize “this device” across sessions; QRNG is an optional extra source the code supports.

---

## Why we need “models” (the neural network and training)

### What “the model” is

- **Raw noise** is high-dimensional (e.g. 1024 floats per sample, or 100k+ for a camera frame). We don’t compare raw noise directly.
- We first reduce each sample to a **fixed-size feature vector** (e.g. ~33 numbers: stats, entropy, FFT, autocorrelation, etc.) via `preprocessing/features.py`.
- Then a **Siamese-style neural network** (in `model/siamese_model.py`) maps each feature vector to a **compact embedding** (e.g. 128-D). Same network is used for every sample.

So: **raw noise → features (hand‑designed) → neural embedder → embedding.**

### Why we need the embedder (and training)

- We want: **same device** → embeddings **close** in vector space; **different device** → embeddings **far**.
- Hand-designed features alone don’t guarantee that: two different devices can have similar stats/entropy. So we **train** the embedder so that:
  - Pairs of samples from the **same** device get **similar** embeddings (small distance).
  - Pairs from **different** devices get **dissimilar** embeddings (large distance).

Training uses **triplet (or contrastive) loss** on a dataset of “device X” vs “device Y” noise: the model learns a mapping that pulls same-device embeddings together and pushes different-device ones apart. That’s why we have:

- **Dataset / data collection**: Many devices, each with many samples (from camera, mic, and/or QRNG).
- **Training script**: Builds feature vectors from that data, trains the Siamese network, saves a checkpoint (e.g. `best_model.pt`).
- **Server**: Loads that checkpoint and uses it as the **embedder** during enrollment and authentication.

So **we need the model** to turn “noise features” into a space where **similarity = same device**. Without training (or with a random model), same-device and different-device similarities would be unpredictable and authentication would be useless.

---

## How the pieces fit together

| Piece | Role |
|-------|------|
| **Noise sources** | QRNG (ANU API), camera (sensor noise), microphone (ambient/self-noise). Provide raw signals. |
| **Preprocessing** | Converts raw noise → fixed feature vector (same pipeline in train and server). |
| **Feature pipeline** | Canonical feature list + version; saved with the model so server and training use the same features. |
| **Siamese / embedder** | Trained on multi-device data; maps feature vector → embedding. |
| **Enrollment** | Collect noise → features → embedder → one embedding per device → save it. |
| **Authentication** | Collect fresh noise → features → embedder → compare to stored embedding (e.g. cosine ≥ 0.85). |
| **Challenge/response** | Nonce + signature derived from embedding; still requires live embedding to be similar. |

**QRNG** is one of the supported noise sources and is used whenever `sources` includes `'qrng'`. For **device-specific** behavior, camera and/or microphone are the sources that actually tie the fingerprint to the device; QRNG adds entropy but not device identity.

If you want, we can next add a short “Quick answers” section (e.g. “Do we use QRNG?” → “Yes, when qrng is in sources”) or tighten any part of this overview.
